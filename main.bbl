% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{Alber1998}{article}{}
      \name{author}{3}{}{%
        {{hash=5167c16f4295c84a6d43e7b4b41f1299}{%
           family={Alber},
           familyi={A\bibinitperiod},
           given={Yakov},
           giveni={Y\bibinitperiod}}}%
        {{hash=f85d3128c38fe9d35a07f07fb570d1b3}{%
           family={Iusem},
           familyi={I\bibinitperiod},
           given={Alfred\bibnamedelima Noel},
           giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=72d6d86ca9877e74426877a7097c6b7a}{%
           family={Solodov},
           familyi={S\bibinitperiod},
           given={Mikhail\bibnamedelima Vladimir},
           giveni={M\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{ed1360077bbc1282e532da60dc5e4a22}
      \strng{fullhash}{ed1360077bbc1282e532da60dc5e4a22}
      \strng{bibnamehash}{ed1360077bbc1282e532da60dc5e4a22}
      \strng{authorbibnamehash}{ed1360077bbc1282e532da60dc5e4a22}
      \strng{authornamehash}{ed1360077bbc1282e532da60dc5e4a22}
      \strng{authorfullhash}{ed1360077bbc1282e532da60dc5e4a22}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider the method for constrained convex optimization in a Hilbert space, consisting of a step in the direction opposite to an$\epsilon$ k -subgradient of the objective at a current iterate, followed by an orthogonal projection onto the feasible set. The normalized stepsizes$\epsilon$ k are exogenously given, satisfying$\Sigma$ k=0 ∞ $\alpha$k = ∞, $\Sigma$ k=0 ∞ $\alpha$ k 2 < ∞, and$\epsilon$ k is chosen so that$\epsilon$ k ⩽ $\mu$$\alpha$k for some$\mu$ > 0. We prove that the sequence generated in this way is weakly convergent to a minimizer if the problem has solutions, and is unbounded otherwise. Among the features of our convergence analysis, we mention that it covers the nonsmooth case, in the sense that we make no assumption of differentiability off, and much less of Lipschitz continuity of its gradient. Also, we prove weak convergence of the whole sequence, rather than just boundedness of the sequence and optimality of its weak accumulation points, thus improving over all previously known convergence results. We present also convergence rate results. {{$\copyright$}} 1998 The Mathematical Programming Society, Inc. Published by Elsevier Science B.V.}
      \field{issn}{1436-4646}
      \field{journaltitle}{Mathematical Programming}
      \field{month}{3}
      \field{number}{1}
      \field{title}{{On the projected subgradient method for nonsmooth convex optimization in a Hilbert space}}
      \field{volume}{81}
      \field{year}{1998}
      \field{pages}{23\bibrangedash 35}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1007/BF01584842
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Alber, Iusem, Solodov - 1998 - On the projected subgradient method for nonsmooth convex optimization in a Hilbert space.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/BF01584842
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/BF01584842
      \endverb
      \keyw{Calculus of Variations and Optimal Control,Combinatorics,Mathematical Methods in Physics,Mathematical and Computational Physics,Mathematics of Computing,Numerical Analysis,Optimization,Theoretical}
    \endentry
    \entry{Alber2001}{article}{}
      \name{author}{2}{}{%
        {{hash=5167c16f4295c84a6d43e7b4b41f1299}{%
           family={Alber},
           familyi={A\bibinitperiod},
           given={Yakov},
           giveni={Y\bibinitperiod}}}%
        {{hash=0eba3dfea4ed61f5a6bdfb2e6016fb1d}{%
           family={Iusem},
           familyi={I\bibinitperiod},
           given={Alfredo\bibnamedelima Noel},
           giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{680bfdce834b4950d0e9da8a94412cfe}
      \strng{fullhash}{680bfdce834b4950d0e9da8a94412cfe}
      \strng{bibnamehash}{680bfdce834b4950d0e9da8a94412cfe}
      \strng{authorbibnamehash}{680bfdce834b4950d0e9da8a94412cfe}
      \strng{authornamehash}{680bfdce834b4950d0e9da8a94412cfe}
      \strng{authorfullhash}{680bfdce834b4950d0e9da8a94412cfe}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper we continue the study of the subgradient method for nonsmooth convex constrained minimization problems in a uniformly convex and uniformly smooth Banach space. We consider the case when the stepsizes satisfy ∑ k=1 ∞$\alpha$ k =∞, lim k→∞$\alpha$ k =0.}
      \field{issn}{1572-932X}
      \field{journaltitle}{Set-Valued Analysis}
      \field{number}{4}
      \field{title}{{Extension of subgradient techniques for nonsmooth optimization in Banach spaces}}
      \field{volume}{9}
      \field{year}{2001}
      \field{pages}{315\bibrangedash 335}
      \range{pages}{21}
      \verb{doi}
      \verb 10.1023/A:1012665832688
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Alber, Iusem - 2001 - Extension of subgradient techniques for nonsmooth optimization in Banach spaces.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1023/A:1012665832688
      \endverb
      \verb{url}
      \verb https://doi.org/10.1023/A:1012665832688
      \endverb
      \keyw{Analysis,Generalized projection,J-Lipschitz continuous functional,Nonsmooth convex functional,Optimization,Subgradient,Subgradient method,Weak convergence}
    \endentry
    \entry{Alizadeh2003}{article}{}
      \name{author}{2}{}{%
        {{hash=748cb8b9dde61cba8b646392da14d6f0}{%
           family={Alizadeh},
           familyi={A\bibinitperiod},
           given={F.},
           giveni={F\bibinitperiod}}}%
        {{hash=3cd25081e68d87e5330591956135cb89}{%
           family={Goldfarb},
           familyi={G\bibinitperiod},
           given={D.},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{6997db335b67d7005108ff444e0c19ea}
      \strng{fullhash}{6997db335b67d7005108ff444e0c19ea}
      \strng{bibnamehash}{6997db335b67d7005108ff444e0c19ea}
      \strng{authorbibnamehash}{6997db335b67d7005108ff444e0c19ea}
      \strng{authornamehash}{6997db335b67d7005108ff444e0c19ea}
      \strng{authorfullhash}{6997db335b67d7005108ff444e0c19ea}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{1436-4646}
      \field{journaltitle}{Mathematical Programming}
      \field{month}{1}
      \field{number}{1}
      \field{title}{{Second-order cone programming}}
      \field{volume}{95}
      \field{year}{2003}
      \field{pages}{3\bibrangedash 51}
      \range{pages}{49}
      \verb{doi}
      \verb 10.1007/S10107-002-0339-5
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Alizadeh, Goldfarb - 2003 - Second-order cone programming.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10107-002-0339-5
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10107-002-0339-5
      \endverb
      \keyw{Calculus of Variations and Optimal Control,Combinatorics,Mathematical Methods in Physics,Mathematical and Computational Physics,Mathematics of Computing,Numerical Analysis,Optimization,Theoretical}
    \endentry
    \entry{Attouch2018}{article}{}
      \name{author}{4}{}{%
        {{hash=462b91257a7d44e2720906b8d28d9577}{%
           family={Attouch},
           familyi={A\bibinitperiod},
           given={Hedy},
           giveni={H\bibinitperiod}}}%
        {{hash=15b47b922301d35c68149c8eb3688b41}{%
           family={Chbani},
           familyi={C\bibinitperiod},
           given={Zaki},
           giveni={Z\bibinitperiod}}}%
        {{hash=8b3d068d1c2149eb08790b97b2d01eb3}{%
           family={Peypouquet},
           familyi={P\bibinitperiod},
           given={Juan},
           giveni={J\bibinitperiod}}}%
        {{hash=da01a8b1b96741a77c3c2614fb9c55a2}{%
           family={Redont},
           familyi={R\bibinitperiod},
           given={Patrick},
           giveni={P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{cb6f32bd2c018990376296569a305fb8}
      \strng{fullhash}{cb6f32bd2c018990376296569a305fb8}
      \strng{bibnamehash}{cb6f32bd2c018990376296569a305fb8}
      \strng{authorbibnamehash}{cb6f32bd2c018990376296569a305fb8}
      \strng{authornamehash}{cb6f32bd2c018990376296569a305fb8}
      \strng{authorfullhash}{cb6f32bd2c018990376296569a305fb8}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In a Hilbert space setting $$\mathcal{H}$$, we study the fast convergence properties as $$t \rightarrow + \infty $$ of the trajectories of the second-order differential equation $$\begin{aligned} \ddot{x}(t) + \frac{\alpha}{t} \dot{x}(t) + \nabla \Phi(x(t)) = g(t), \end{aligned}$$ where $$\nabla \Phi $$ is the gradient of a convex continuously differentiable function $$\Phi : \mathcal{H} \rightarrow \mathbb{R}, \alpha $$ is a positive parameter, and $$g: [t_0, + \infty [ \rightarrow \mathcal{H}$$ is a small perturbation term. In this inertial system, the viscous damping coefficient $$\frac{\alpha}{t}$$ vanishes asymptotically, but not too rapidly. For $$\alpha \ge 3$$, and $$\int_{t_0}^{+\infty } t \Vert g(t)\Vert dt < + \infty $$ , just assuming that $$\mathrm{argmin} \Phi \ne \emptyset $$ , we show that any trajectory of the above system satisfies the fast convergence property $$\begin{aligned} \Phi(x(t))- \min_{\mathcal{H}}\Phi \le \frac{C}{t^2}. \end{aligned}$$ Moreover, for $$\alpha > 3$$, any trajectory converges weakly to a minimizer of $$\Phi $$. The strong convergence is established in various practical situations. These results complement the $$\mathcal{O}(t^{-2})$$ rate of convergence for the values obtained by Su, Boyd and Cand\`{e}s in the unperturbed case $$g=0$$ . Time discretization of this system, and some of its variants, provides new fast converging algorithms, expanding the field of rapid methods for structured convex minimization introduced by Nesterov, and further developed by Beck and Teboulle with FISTA. This study also complements recent advances due to Chambolle and Dossal.}
      \field{issn}{1436-4646}
      \field{journaltitle}{Mathematical Programming}
      \field{month}{3}
      \field{number}{1}
      \field{title}{{Fast convergence of inertial dynamics and algorithms with asymptotic vanishing viscosity}}
      \field{volume}{168}
      \field{year}{2018}
      \field{pages}{123\bibrangedash 175}
      \range{pages}{53}
      \verb{doi}
      \verb 10.1007/S10107-016-0992-8
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Attouch et al. - 2016 - Fast convergence of inertial dynamics and algorithms with asymptotic vanishing viscosity.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10107-016-0992-8
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10107-016-0992-8
      \endverb
      \keyw{Calculus of Variations and Optimal Control,Combinatorics,Mathematical Methods in Physics,Mathematical and Computational Physics,Mathematics of Computing,Numerical Analysis,Optimization,Theoretical}
    \endentry
    \entry{Attouch2016}{article}{}
      \name{author}{2}{}{%
        {{hash=462b91257a7d44e2720906b8d28d9577}{%
           family={Attouch},
           familyi={A\bibinitperiod},
           given={Hedy},
           giveni={H\bibinitperiod}}}%
        {{hash=8b3d068d1c2149eb08790b97b2d01eb3}{%
           family={Peypouquet},
           familyi={P\bibinitperiod},
           given={Juan},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{2}{%
        {Society for Industrial}%
        {Applied Mathematics}%
      }
      \strng{namehash}{777f020bf4751c12b8fbf0cd9977c766}
      \strng{fullhash}{777f020bf4751c12b8fbf0cd9977c766}
      \strng{bibnamehash}{777f020bf4751c12b8fbf0cd9977c766}
      \strng{authorbibnamehash}{777f020bf4751c12b8fbf0cd9977c766}
      \strng{authornamehash}{777f020bf4751c12b8fbf0cd9977c766}
      \strng{authorfullhash}{777f020bf4751c12b8fbf0cd9977c766}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The forward-backward algorithm is a powerful tool for solving optimization problems with an additively separable and smooth plus nonsmooth structure. In the convex setting, a simple but ingenious a...}
      \field{eprinttype}{arXiv}
      \field{issn}{10526234}
      \field{journaltitle}{SIAM Journal on Optimization}
      \field{month}{9}
      \field{number}{3}
      \field{title}{{The rate of convergence of {N}esterov's accelerated forward-backward method is actually faster than $1/k^2$}}
      \field{volume}{26}
      \field{year}{2016}
      \field{pages}{1824\bibrangedash 1834}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1137/15M1046095
      \endverb
      \verb{eprint}
      \verb 1510.08740
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Attouch, Peypouquet - 2016 - The Rate of Convergence of Nesterov's Accelerated Forward-Backward Method is Actually Faster Than $1k2$.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1137/15M1046095
      \endverb
      \verb{url}
      \verb https://doi.org/10.1137/15M1046095
      \endverb
      \keyw{49M37,65K05,90C25,Nesterov method,convex optimization,fast convergent methods}
    \endentry
    \entry{Auchmuty1989}{article}{}
      \name{author}{1}{}{%
        {{hash=3506b7c30de70f3163d3742dc7b6350c}{%
           family={Auchmuty},
           familyi={A\bibinitperiod},
           given={Giles},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Marcel Dekker, Inc.}%
      }
      \strng{namehash}{3506b7c30de70f3163d3742dc7b6350c}
      \strng{fullhash}{3506b7c30de70f3163d3742dc7b6350c}
      \strng{bibnamehash}{3506b7c30de70f3163d3742dc7b6350c}
      \strng{authorbibnamehash}{3506b7c30de70f3163d3742dc7b6350c}
      \strng{authornamehash}{3506b7c30de70f3163d3742dc7b6350c}
      \strng{authorfullhash}{3506b7c30de70f3163d3742dc7b6350c}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper describes, and analyzes, variational principles for the solutions of finite dimensional variational inequalities on closed convex sets. No symmetry conditions are required for the functi...}
      \field{issn}{15322467}
      \field{journaltitle}{Numerical Functional Analysis and Optimization}
      \field{number}{9-10}
      \field{title}{{Variational principles for variational inequalities}}
      \field{volume}{10}
      \field{year}{1989}
      \field{pages}{863\bibrangedash 874}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1080/01630568908816335
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Auchrnuty - 2010 - Variational principles for variational inequalities(2).pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1080/01630568908816335
      \endverb
      \verb{url}
      \verb https://doi.org/10.1080/01630568908816335
      \endverb
    \endentry
    \entry{Aujol2021}{misc}{}
      \name{author}{3}{}{%
        {{hash=9f89761b51d3031a996e818afff055dc}{%
           family={Aujol},
           familyi={A\bibinitperiod},
           given={Jean-Fran\c{c}ois},
           giveni={J\bibinithyphendelim F\bibinitperiod}}}%
        {{hash=bd33384632289703381b32e50974752d}{%
           family={Dossal},
           familyi={D\bibinitperiod},
           given={Charles},
           giveni={C\bibinitperiod}}}%
        {{hash=3ad6214614597483ac102f234c0cbbf1}{%
           family={Rondepierre},
           familyi={R\bibinitperiod},
           given={Aude},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{68eab38a5f3393c411234ef7706d33fc}
      \strng{fullhash}{68eab38a5f3393c411234ef7706d33fc}
      \strng{bibnamehash}{68eab38a5f3393c411234ef7706d33fc}
      \strng{authorbibnamehash}{68eab38a5f3393c411234ef7706d33fc}
      \strng{authornamehash}{68eab38a5f3393c411234ef7706d33fc}
      \strng{authorfullhash}{68eab38a5f3393c411234ef7706d33fc}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{FISTA is an automatic geometrically optimized algorithm for strongly convex functions}}
      \field{year}{2021}
      \verb{urlraw}
      \verb https://hal.archives-ouvertes.fr/hal-03491527/
      \endverb
      \verb{url}
      \verb https://hal.archives-ouvertes.fr/hal-03491527/
      \endverb
    \endentry
    \entry{Auslender1976}{book}{}
      \name{author}{1}{}{%
        {{hash=6c36b3069cd0cdbe6c80d1286b46f709}{%
           family={Auslender},
           familyi={A\bibinitperiod},
           given={Algred},
           giveni={A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Paris}%
      }
      \list{publisher}{1}{%
        {Masson}%
      }
      \strng{namehash}{6c36b3069cd0cdbe6c80d1286b46f709}
      \strng{fullhash}{6c36b3069cd0cdbe6c80d1286b46f709}
      \strng{bibnamehash}{6c36b3069cd0cdbe6c80d1286b46f709}
      \strng{authorbibnamehash}{6c36b3069cd0cdbe6c80d1286b46f709}
      \strng{authornamehash}{6c36b3069cd0cdbe6c80d1286b46f709}
      \strng{authorfullhash}{6c36b3069cd0cdbe6c80d1286b46f709}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{2-225-42900-6}
      \field{month}{1}
      \field{title}{{Optimisation : m\'{e}thodes num\'{e}riques}}
      \field{volume}{1}
      \field{year}{1976}
    \endentry
    \entry{Beck2017}{book}{}
      \name{author}{1}{}{%
        {{hash=7e285058a4af1ce645d910e3951d7b82}{%
           family={Beck},
           familyi={B\bibinitperiod},
           given={Amir},
           giveni={A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Philadelphia, Pennsylvania, USA}%
      }
      \list{publisher}{2}{%
        {Society for Industrial}%
        {Applied Mathematics}%
      }
      \strng{namehash}{7e285058a4af1ce645d910e3951d7b82}
      \strng{fullhash}{7e285058a4af1ce645d910e3951d7b82}
      \strng{bibnamehash}{7e285058a4af1ce645d910e3951d7b82}
      \strng{authorbibnamehash}{7e285058a4af1ce645d910e3951d7b82}
      \strng{authornamehash}{7e285058a4af1ce645d910e3951d7b82}
      \strng{authorfullhash}{7e285058a4af1ce645d910e3951d7b82}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Vector spaces -- Extended real-value functions -- Subgradients -- Conjugate functions -- Smoothness and strong convexity -- The proximal operator -- Spectral functions -- Primal and dual projected subgradient methods -- Mirror descent -- The proximal gradient method -- The block proximal gradient method -- Dual-based proximal gradient methods -- The generalized Conditional gradient method -- Alternating minimization -- ADMM.}
      \field{booktitle}{First-Order Methods in Optimization}
      \field{isbn}{978-1-61197-498-0}
      \field{month}{10}
      \field{title}{{First-Order Methods in Optimization}}
      \field{year}{2017}
      \verb{doi}
      \verb 10.1137/1.9781611974997
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Beck - 2017 - First-Order Methods in Optimization.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.appa.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.appb.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.appc.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.appd.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.bm.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.ch1.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.ch2.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.ch3.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.ch4.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.ch5.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.ch6.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.ch7.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.ch8.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.ch9.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.ch10.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.ch11.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.ch12.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.ch13.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.ch14.pdf:pdf;:Users/zalgo/Downloads/1.9781611974997.ch15.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1137/1.9781611974997
      \endverb
      \verb{url}
      \verb https://doi.org/10.1137/1.9781611974997
      \endverb
      \keyw{Alexander,Algorithms,Amir,Andrew R,Andrzej,Applications of Stochastic Programming Gr\"{o}tschel,Arc Routing: Problems,Buttazzo,Daniele,Darinka,Dentcheva,Evaluating Gas Network Capacities Corber\'{a}n,Fabio,Gilbert,Giuseppe,Global Optimization: Theory,G\'{e}rard,Hedy,Introduction to Derivative-Free Optimization Ferri,Introduction to Nonlinear Optimization: Theory,Introduction to Optimization and Semidifferential,Katya,Lars,Lectures on Stochastic Programming: Modeling and T,Linear Programming with MATLAB Attouch,Lorenz T,Luis N,M C,Mangasarian,Marco and Schoen,Martin,Methods,Michael,Michael C,Nonlinear Programming: Concepts,Olvi L,Paolo,Scheinberg,Second Edition Beck,Second Edition Locatelli,Second Edition Shapiro,Semidefinite Optimization and Convex Algebraic Geo,Semismooth Newton Methods for Variational Inequali,Stein W and Ziemba,Stephen J,The,Variational Analysis in Sobolev and BV Spaces: App,Vehicle Routing: Problems,William T,and Applications,and Applications Toth,and Applications editors,and Applications to Chemical Processes Shapiro,and Applications with MATLAB Attouch,and Laporte,and Michaille,and Ruszczy\'{n}ski,and Vicente,and Vigo,and Wright,editor,editors,\'{A}ngel}
    \endentry
    \entry{Beck2006}{article}{}
      \name{author}{2}{}{%
        {{hash=7e285058a4af1ce645d910e3951d7b82}{%
           family={Beck},
           familyi={B\bibinitperiod},
           given={Amir},
           giveni={A\bibinitperiod}}}%
        {{hash=d147b0cce00b122576f734fc8dc462a7}{%
           family={Eldar},
           familyi={E\bibinitperiod},
           given={Yonina\bibnamedelima C.},
           giveni={Y\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \list{publisher}{2}{%
        {Society for Industrial}%
        {Applied Mathematics}%
      }
      \strng{namehash}{b5b210b2fb1521d358632a6ee539702b}
      \strng{fullhash}{b5b210b2fb1521d358632a6ee539702b}
      \strng{bibnamehash}{b5b210b2fb1521d358632a6ee539702b}
      \strng{authorbibnamehash}{b5b210b2fb1521d358632a6ee539702b}
      \strng{authornamehash}{b5b210b2fb1521d358632a6ee539702b}
      \strng{authorfullhash}{b5b210b2fb1521d358632a6ee539702b}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider the problem of minimizing an indefinite quadratic function subject to two quadratic inequality constraints. When the problem is defined over the complex plane we show that strong duality holds and obtain necessary and sufficient optimality conditions. We then develop a connection between the image of the real and complex spaces under a quadratic mapping, which together with the results in the complex case lead to a condition that ensures strong duality in the real setting. Preliminary numerical simulations suggest that for random instances of the extended trust region subproblem, the sufficient condition is satisfied with a high probability. Furthermore, we show that the sufficient condition is always satisfied in two classes of nonconvex quadratic problems. Finally, we discuss an application of our results to robust least squares problems. {{$\copyright$}} 2006 Society for Industrial and Applied Mathematics.}
      \field{issn}{10526234}
      \field{journaltitle}{SIAM Journal on Optimization}
      \field{month}{1}
      \field{number}{3}
      \field{title}{{Strong duality in nonconvex quadratic optimization with two quadratic constraints}}
      \field{volume}{17}
      \field{year}{2006}
      \field{pages}{844\bibrangedash 860}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1137/050644471
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Beck, Eldar - 2006 - Strong duality in nonconvex quadratic optimization with two quadratic constraints.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://epubs.siam.org/doi/10.1137/050644471
      \endverb
      \verb{url}
      \verb http://epubs.siam.org/doi/10.1137/050644471
      \endverb
      \keyw{Nonconvex optimization,Quadratic mappings,Quadratic programming,Strong duality}
    \endentry
    \entry{Beck2009}{article}{}
      \name{author}{2}{}{%
        {{hash=7e285058a4af1ce645d910e3951d7b82}{%
           family={Beck},
           familyi={B\bibinitperiod},
           given={Amir},
           giveni={A\bibinitperiod}}}%
        {{hash=009141d6de34d70316721834af7f1e0b}{%
           family={Teboulle},
           familyi={T\bibinitperiod},
           given={Marc},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{2}{%
        {Society for Industrial}%
        {Applied Mathematics}%
      }
      \strng{namehash}{c91e0d857797424d21c636688a53c132}
      \strng{fullhash}{c91e0d857797424d21c636688a53c132}
      \strng{bibnamehash}{c91e0d857797424d21c636688a53c132}
      \strng{authorbibnamehash}{c91e0d857797424d21c636688a53c132}
      \strng{authornamehash}{c91e0d857797424d21c636688a53c132}
      \strng{authorfullhash}{c91e0d857797424d21c636688a53c132}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.}
      \field{issn}{19364954}
      \field{journaltitle}{SIAM Journal on Imaging Sciences}
      \field{month}{1}
      \field{number}{1}
      \field{title}{{A fast iterative shrinkage-thresholding algorithm for linear inverse problems}}
      \field{volume}{2}
      \field{year}{2009}
      \field{pages}{183\bibrangedash 202}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1137/080716542
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Beck, Teboulle - 2009 - A fast iterative shrinkage-thresholding algorithm for linear inverse problems.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1137/080716542
      \endverb
      \verb{url}
      \verb https://doi.org/10.1137/080716542
      \endverb
      \keyw{Deconvolution,Global rate of convergence,Image deblurring,Iterative shrinkage-thresholding algorithm,Least squares and l1 regularization problems,Linear inverse problem,Optimal gradient method,Two-step iterative algorithms}
    \endentry
    \entry{Bello-Cruz2013}{article}{}
      \name{author}{1}{}{%
        {{hash=e9db38b5e2bd67ab3073e66abd88e854}{%
           family={{Bello Cruz}},
           familyi={B\bibinitperiod},
           given={Jose\bibnamedelima Yunier},
           giveni={J\bibinitperiod\bibinitdelim Y\bibinitperiod}}}%
      }
      \strng{namehash}{e9db38b5e2bd67ab3073e66abd88e854}
      \strng{fullhash}{e9db38b5e2bd67ab3073e66abd88e854}
      \strng{bibnamehash}{e9db38b5e2bd67ab3073e66abd88e854}
      \strng{authorbibnamehash}{e9db38b5e2bd67ab3073e66abd88e854}
      \strng{authornamehash}{e9db38b5e2bd67ab3073e66abd88e854}
      \strng{authorfullhash}{e9db38b5e2bd67ab3073e66abd88e854}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Vector optimization problems are a significant extension of scalar optimization and have many real life applications. We consider an extension of the projected subgradient method to convex vector optimization, which works directly with vector-valued functions, without using scalarvalued objectives. We eliminate the scalarization approach, a popular strategy for solving vector optimization problems, exploring strongly the structure of these kinds of problems. Under suitable assumptions, we show that the sequence generated by the algorithm converges to a weakly efficient optimum point. {{$\copyright$}} 2013 Society for Industrial and Applied Mathematics.}
      \field{issn}{10526234}
      \field{journaltitle}{SIAM Journal on Optimization}
      \field{number}{4}
      \field{title}{{A subgradient method for vector optimization problems}}
      \field{volume}{23}
      \field{year}{2013}
      \field{pages}{2169\bibrangedash 2182}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1137/120866415
      \endverb
      \verb{file}
      \verb :Users/zalgo/Downloads/Bello Cruz - Unknown - A SUBGRADIENT METHOD FOR VECTOR OPTIMIZATION PROBLEMS.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1137/120866415
      \endverb
      \verb{url}
      \verb https://doi.org/10.1137/120866415
      \endverb
      \keyw{Nonsmooth optimization,Projected subgradient method,Vector optimization,Weakly efficient points}
    \endentry
    \entry{Ben-tal1998}{article}{}
      \name{author}{2}{}{%
        {{hash=b4f10717c86d56d33bf7c4dadf066084}{%
           family={Ben-Tal},
           familyi={B\bibinithyphendelim T\bibinitperiod},
           given={Aharon},
           giveni={A\bibinitperiod}}}%
        {{hash=b7bb695716d8b9e139e5d51d5dfd3567}{%
           family={Nemirovski},
           familyi={N\bibinitperiod},
           given={Arkadi},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{e2b5a991c81d42dde6d2fed62a5c6a5c}
      \strng{fullhash}{e2b5a991c81d42dde6d2fed62a5c6a5c}
      \strng{bibnamehash}{e2b5a991c81d42dde6d2fed62a5c6a5c}
      \strng{authorbibnamehash}{e2b5a991c81d42dde6d2fed62a5c6a5c}
      \strng{authornamehash}{e2b5a991c81d42dde6d2fed62a5c6a5c}
      \strng{authorfullhash}{e2b5a991c81d42dde6d2fed62a5c6a5c}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study convex optimization problems for which the data is not specified exactly and it is only known to belong to a given uncertainty set, yet the constraints must hold for all possible values of the data from. The ensuing optimization problem is called robust optimization. In this paper we lay the foundation of robust convex optimization. In the main part of the paper we show that if is an ellipsoidal uncertainty set, then for some of the most important generic convex optimization problems (linear programming, quadratically constrained programming, semidefinite programming and others) the corresponding robust convex program is either exactly, or approximately, a tractable problem which lends itself to efficient algorithms such as polynomial time interior point methods.}
      \field{issn}{0364765X}
      \field{journaltitle}{Mathematics of Operations Research}
      \field{number}{4}
      \field{title}{{Robust convex optimization}}
      \field{volume}{23}
      \field{year}{1998}
      \field{pages}{769\bibrangedash 805}
      \range{pages}{37}
      \verb{doi}
      \verb 10.1287/moor.23.4.769
      \endverb
      \keyw{Convex optimization,Data uncertainty,Geometric programming,Linear programming,Quadratic programming,Robustness,Semidefinite programming}
    \endentry
    \entry{Bertsekas1999}{book}{}
      \name{author}{1}{}{%
        {{hash=c79eaa39c7ca7091f241b9404b19d954}{%
           family={Bertsekas},
           familyi={B\bibinitperiod},
           given={Dimitri\bibnamedelima Panteli},
           giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Belmont, Massachusetts}%
      }
      \list{publisher}{1}{%
        {Athena Scientific}%
      }
      \strng{namehash}{c79eaa39c7ca7091f241b9404b19d954}
      \strng{fullhash}{c79eaa39c7ca7091f241b9404b19d954}
      \strng{bibnamehash}{c79eaa39c7ca7091f241b9404b19d954}
      \strng{authorbibnamehash}{c79eaa39c7ca7091f241b9404b19d954}
      \strng{authornamehash}{c79eaa39c7ca7091f241b9404b19d954}
      \strng{authorfullhash}{c79eaa39c7ca7091f241b9404b19d954}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{These proceedings are based on lectures delivered at the Symposium on Nonlinear Programming held March 23 and 24, 1975, as part of the American Mathematical Society's annual New York meeting. The purpose of the symposium was to help bring to the attention of a larger mathematical audience some of the history, theory, applications and vigorous research activity of the Nonlinear Programming field. The program was largely tutorial, although excellent talks on new results were purposely included and the speakers were encouraged to identify promising regions for further exploration. The eight papers presented deal with such aspects as optimality conditions, algorithms and their convergence, approximate solutions, and optimization.}
      \field{edition}{Second}
      \field{issn}{01605682}
      \field{title}{{Nonlinear Programming}}
      \field{year}{1999}
      \verb{file}
      \verb :Users/zalgo/Downloads/Bertsekas1997_Article_NonlinearProgramming (1).pdf:pdf
      \endverb
    \endentry
    \entry{Bertsekas2003}{book}{}
      \name{author}{3}{}{%
        {{hash=c79eaa39c7ca7091f241b9404b19d954}{%
           family={Bertsekas},
           familyi={B\bibinitperiod},
           given={Dimitri\bibnamedelima Panteli},
           giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=3415c06ffaf92636fe33210160b958a5}{%
           family={Nedic},
           familyi={N\bibinitperiod},
           given={Angelia},
           giveni={A\bibinitperiod}}}%
        {{hash=e2394ecac54af3732259ff62712c7b36}{%
           family={Ozdaglar},
           familyi={O\bibinitperiod},
           given={Asuman\bibnamedelima E.},
           giveni={A\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Belmont, Mass.}%
      }
      \list{publisher}{1}{%
        {Athena Scientific}%
      }
      \strng{namehash}{c12b9c7dbea129c037063ed71b330988}
      \strng{fullhash}{c12b9c7dbea129c037063ed71b330988}
      \strng{bibnamehash}{c12b9c7dbea129c037063ed71b330988}
      \strng{authorbibnamehash}{c12b9c7dbea129c037063ed71b330988}
      \strng{authornamehash}{c12b9c7dbea129c037063ed71b330988}
      \strng{authorfullhash}{c12b9c7dbea129c037063ed71b330988}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{1-886529-45-0}
      \field{title}{{Convex analysis and optimization}}
      \field{year}{2003}
    \endentry
    \entry{Bonnans2000}{book}{}
      \name{author}{2}{}{%
        {{hash=a859c415cea61a5f83cd91c5e01d3b7b}{%
           family={Bonnans},
           familyi={B\bibinitperiod},
           given={J.\bibnamedelimi Fr\'{e}d\'{e}ric},
           giveni={J\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
        {{hash=79b4f8b2eec1f0cce98adc8a524fc774}{%
           family={Shapiro},
           familyi={S\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New York, NY, USA}%
      }
      \list{publisher}{1}{%
        {Springer New York}%
      }
      \strng{namehash}{ca5a89f615b2ee35dbe43e8250003d25}
      \strng{fullhash}{ca5a89f615b2ee35dbe43e8250003d25}
      \strng{bibnamehash}{ca5a89f615b2ee35dbe43e8250003d25}
      \strng{authorbibnamehash}{ca5a89f615b2ee35dbe43e8250003d25}
      \strng{authornamehash}{ca5a89f615b2ee35dbe43e8250003d25}
      \strng{authorfullhash}{ca5a89f615b2ee35dbe43e8250003d25}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A presentation of general results for discussing local optimality and computation of the expansion of value function and approximate solution of optimization problems, followed by their application to various fields, from physics to economics. The book is thus an opportunity for popularizing these techniques among researchers involved in other sciences, including users of optimization in a wide sense, in mechanics, physics, statistics, finance and economics. Of use to research professionals, including graduate students at an advanced level.}
      \field{booktitle}{Perturbation Analysis of Optimization Problems}
      \field{title}{{Perturbation Analysis of Optimization Problems}}
      \field{year}{2000}
      \verb{doi}
      \verb 10.1007/978-1-4612-1394-9
      \endverb
      \verb{file}
      \verb :Users/zalgo/Downloads/2000_Book_Pertubation.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-1-4612-1394-9
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-1-4612-1394-9
      \endverb
    \endentry
    \entry{Bonnel2005}{article}{}
      \name{author}{3}{}{%
        {{hash=74db7c6ee1fa4f61b3768e15a226dd0b}{%
           family={Bonnel},
           familyi={B\bibinitperiod},
           given={Henri},
           giveni={H\bibinitperiod}}}%
        {{hash=0eba3dfea4ed61f5a6bdfb2e6016fb1d}{%
           family={Iusem},
           familyi={I\bibinitperiod},
           given={Alfredo\bibnamedelima Noel},
           giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=975a8e0ef61c5aca1bf497aac32c8c43}{%
           family={Svaiter},
           familyi={S\bibinitperiod},
           given={Benar\bibnamedelima Fux},
           giveni={B\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
      }
      \strng{namehash}{1755bf9f709ab7764dcb82c5073584a5}
      \strng{fullhash}{1755bf9f709ab7764dcb82c5073584a5}
      \strng{bibnamehash}{1755bf9f709ab7764dcb82c5073584a5}
      \strng{authorbibnamehash}{1755bf9f709ab7764dcb82c5073584a5}
      \strng{authornamehash}{1755bf9f709ab7764dcb82c5073584a5}
      \strng{authorfullhash}{1755bf9f709ab7764dcb82c5073584a5}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider the vector optimization problem of finding weakly efficient points for maps from a Hilbert space X to a Banach space Y, with respect to the partial order induced by a closed, convex, and pointed cone C ⊂ Y with a nonempty interior. We develop for this problem an extension of the proximal point method for scalar-valued convex optimization. In this extension, the subproblems consist of finding weakly efficient points for suitable regularizations of the original map. We present both an exact and an inexact version, in which the subproblems are solved only approximately, within a constant relative tolerance. In both cases, we prove weak convergence of the generated sequence to a weakly efficient point, assuming convexity of the map with respect to C and C-completeness of the initial section. In cases where this last assumption fails, we still establish that the generating sequence is, in a suitable sense, a minimizing one. We also exhibit a particular instance of the algorithm for which, under a mild hypothesis on C, the weak limit of the generated sequence is an efficient, rather than a weakly efficient, point. {{$\copyright$}} 2005 Society for Industrial and Applied Mathematics.}
      \field{issn}{1052-6234}
      \field{journaltitle}{SIAM Journal on Optimization}
      \field{month}{1}
      \field{number}{4}
      \field{title}{{Proximal methods in vector optimization}}
      \field{volume}{15}
      \field{year}{2005}
      \field{pages}{953\bibrangedash 970}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1137/S1052623403429093
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Bonnel, Iusem, Svaiter - 2005 - Proximal methods in vector optimization.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1137/S1052623403429093
      \endverb
      \verb{url}
      \verb https://doi.org/10.1137/S1052623403429093
      \endverb
      \keyw{Inexact algorithm,Proximal point,Vector optimization}
    \endentry
    \entry{Burachik1995}{article}{}
      \name{author}{4}{}{%
        {{hash=ff6ffa5ab73884ae4d01c25b27875ce2}{%
           family={Burachik},
           familyi={B\bibinitperiod},
           given={Regina\bibnamedelima Sandra},
           giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=79b9d0386837e14eb2455f224b15bec4}{%
           family={{Gra\~{n}a Drummond}},
           familyi={G\bibinitperiod},
           given={Luis\bibnamedelima Mauricio},
           giveni={L\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=0eba3dfea4ed61f5a6bdfb2e6016fb1d}{%
           family={Iusem},
           familyi={I\bibinitperiod},
           given={Alfredo\bibnamedelima Noel},
           giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=975a8e0ef61c5aca1bf497aac32c8c43}{%
           family={Svaiter},
           familyi={S\bibinitperiod},
           given={Benar\bibnamedelima Fux},
           giveni={B\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
      }
      \list{publisher}{2}{%
        {Gordon}%
        {Breach Science Publishers}%
      }
      \strng{namehash}{36cc6a98fc9d1b62f19970feb2fb8df2}
      \strng{fullhash}{36cc6a98fc9d1b62f19970feb2fb8df2}
      \strng{bibnamehash}{36cc6a98fc9d1b62f19970feb2fb8df2}
      \strng{authorbibnamehash}{36cc6a98fc9d1b62f19970feb2fb8df2}
      \strng{authornamehash}{36cc6a98fc9d1b62f19970feb2fb8df2}
      \strng{authorfullhash}{36cc6a98fc9d1b62f19970feb2fb8df2}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Several finite procedures for determining the step size of the steepest descent method for unconstrained optimization, without performing exact one-dimensional minimizations, have been considered i...}
      \field{issn}{10294945}
      \field{journaltitle}{Optimization}
      \field{number}{2}
      \field{title}{{Full convergence of the steepest descent method with inexact line searches}}
      \field{volume}{32}
      \field{year}{1995}
      \field{pages}{137\bibrangedash 146}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1080/02331939508844042
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Burachik et al. - 1995 - Full convergence of the steepest descent method with inexact line searches.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1080/02331939508844042
      \endverb
      \verb{url}
      \verb https://doi.org/10.1080/02331939508844042
      \endverb
      \keyw{Convex Programming,Inexact Line Searches,Steepest Descent Method}
    \endentry
    \entry{Byrd1999}{article}{}
      \name{author}{3}{}{%
        {{hash=5a28551b79116d5aa47933899f1dfee0}{%
           family={Byrd},
           familyi={B\bibinitperiod},
           given={Richard\bibnamedelima H.},
           giveni={R\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=2eff6b044389c20bbaa8a0b6c93ccf3f}{%
           family={Hribar},
           familyi={H\bibinitperiod},
           given={Mary\bibnamedelima E.},
           giveni={M\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=88a342d927bf795b0d92af8a5613da31}{%
           family={Nocedal},
           familyi={N\bibinitperiod},
           given={Jorge},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{2}{%
        {Society for Industrial}%
        {Applied Mathematics Publications}%
      }
      \strng{namehash}{bc71902f71dc08dee6ae55c1c7dd9067}
      \strng{fullhash}{bc71902f71dc08dee6ae55c1c7dd9067}
      \strng{bibnamehash}{bc71902f71dc08dee6ae55c1c7dd9067}
      \strng{authorbibnamehash}{bc71902f71dc08dee6ae55c1c7dd9067}
      \strng{authornamehash}{bc71902f71dc08dee6ae55c1c7dd9067}
      \strng{authorfullhash}{bc71902f71dc08dee6ae55c1c7dd9067}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The design and implementation of a new algorithm for solving large nonlinear programming problems is described. It follows a barrier approach that employs sequential quadratic programming and trust regions to solve the subproblems occurring in the iteration. Both primal and primal-dual versions of the algorithm are developed, and their performance is illustrated in a set of numerical tests.}
      \field{issn}{1052-6234}
      \field{journaltitle}{SIAM Journal on Optimization}
      \field{month}{1}
      \field{number}{4}
      \field{title}{{An interior point algorithm for large-scale nonlinear programming}}
      \field{volume}{9}
      \field{year}{1999}
      \field{pages}{877\bibrangedash 900}
      \range{pages}{24}
      \verb{doi}
      \verb 10.1137/S1052623497325107
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Byrd, Hribar, Nocedal - 1999 - An interior point algorithm for large-scale nonlinear programming.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1137/S1052623497325107
      \endverb
      \verb{url}
      \verb https://doi.org/10.1137/S1052623497325107
      \endverb
      \keyw{Barrier method,Constrained optimization,Interior point method,Large-scale optimization,Nonlinear programming,Primal method,Primal-dual method,Sequential quadratic programming,Trust region method}
    \endentry
    \entry{Calderon2020}{article}{}
      \name{author}{3}{}{%
        {{hash=3e6a9d2564820f6ad19ceb018c8c1350}{%
           family={Calder\'{o}n},
           familyi={C\bibinitperiod},
           given={L.},
           giveni={L\bibinitperiod}}}%
        {{hash=ce08cbf76e460603e7ace76b2f0daaa3}{%
           family={Diniz-Ehrhardt},
           familyi={D\bibinithyphendelim E\bibinitperiod},
           given={M.\bibnamedelimi A.},
           giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=66a47c548889cb360069fd38711533b4}{%
           family={Mart\'{\i}nez},
           familyi={M\bibinitperiod},
           given={J.\bibnamedelimi M.},
           giveni={J\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Taylor & Francis}%
      }
      \strng{namehash}{bd19b939e651a0a1d2b268a6c989a679}
      \strng{fullhash}{bd19b939e651a0a1d2b268a6c989a679}
      \strng{bibnamehash}{bd19b939e651a0a1d2b268a6c989a679}
      \strng{authorbibnamehash}{bd19b939e651a0a1d2b268a6c989a679}
      \strng{authornamehash}{bd19b939e651a0a1d2b268a6c989a679}
      \strng{authorfullhash}{bd19b939e651a0a1d2b268a6c989a679}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A p-order regularization method for finding weak stationary points of multiobjective optimization problems with constraints is introduced. Under H\"{o}lder conditions on the derivatives of the objectiv...}
      \field{issn}{10294937}
      \field{journaltitle}{Optimization Methods and Software}
      \field{title}{{On high-order model regularization for multiobjective optimization}}
      \field{year}{2020}
      \verb{doi}
      \verb 10.1080/10556788.2020.1719408
      \endverb
      \verb{file}
      \verb :Users/zalgo/Downloads/6997.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://www.tandfonline.com/doi/abs/10.1080/10556788.2020.1719408
      \endverb
      \verb{url}
      \verb https://www.tandfonline.com/doi/abs/10.1080/10556788.2020.1719408
      \endverb
      \keyw{Constrained minimization,complexity,multiobjective}
    \endentry
    \entry{Carrizosa1998}{article}{}
      \name{author}{2}{}{%
        {{hash=c19ee81e25a09561cea8769b19e3c001}{%
           family={Carrizosa},
           familyi={C\bibinitperiod},
           given={Emilio},
           giveni={E\bibinitperiod}}}%
        {{hash=ac661d7365c3c2b2c88f942a6024d03e}{%
           family={Frenk},
           familyi={F\bibinitperiod},
           given={J.\bibnamedelimi B.\bibnamedelimi G.},
           giveni={J\bibinitperiod\bibinitdelim B\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
      }
      \strng{namehash}{27dea85b8e410245a3ab5051c7e13c59}
      \strng{fullhash}{27dea85b8e410245a3ab5051c7e13c59}
      \strng{bibnamehash}{27dea85b8e410245a3ab5051c7e13c59}
      \strng{authorbibnamehash}{27dea85b8e410245a3ab5051c7e13c59}
      \strng{authornamehash}{27dea85b8e410245a3ab5051c7e13c59}
      \strng{authorfullhash}{27dea85b8e410245a3ab5051c7e13c59}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A number of optimization methods require as a first step the construction of a dominating set (a set containing an optimal solution) enjoying properties such as compactness or convexity. In this paper, we address the problem of constructing dominating sets for problems whose objective is a componentwise nondecreasing function of (possibly an infinite number of) convex functions, and we show how to obtain a convex dominating set in terms of dominating sets of simpler problems. The applicability of the results obtained is illustrated with the statement of new localization results in the fields of linear regression and location.}
      \field{issn}{0022-3239}
      \field{journaltitle}{Journal of Optimization Theory and Applications}
      \field{month}{2}
      \field{number}{2}
      \field{title}{{Dominating sets for convex functions with some applications}}
      \field{volume}{96}
      \field{year}{1998}
      \field{pages}{281\bibrangedash 295}
      \range{pages}{15}
      \verb{doi}
      \verb 10.1023/A:1022614029984
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Carrizosa, Frenk - 1998 - Dominating sets for convex functions with some applications.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1023/A:1022614029984
      \endverb
      \verb{url}
      \verb https://doi.org/10.1023/A:1022614029984
      \endverb
      \keyw{Dominating sets,convexity,location,regression}
    \endentry
    \entry{Cauchy1847}{article}{}
      \name{author}{1}{}{%
        {{hash=16b1d6105e8c665c6ea031132da22f8b}{%
           family={Cauchy},
           familyi={C\bibinitperiod},
           given={Augustin\bibnamedelima Louis},
           giveni={A\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {French}%
      }
      \strng{namehash}{16b1d6105e8c665c6ea031132da22f8b}
      \strng{fullhash}{16b1d6105e8c665c6ea031132da22f8b}
      \strng{bibnamehash}{16b1d6105e8c665c6ea031132da22f8b}
      \strng{authorbibnamehash}{16b1d6105e8c665c6ea031132da22f8b}
      \strng{authornamehash}{16b1d6105e8c665c6ea031132da22f8b}
      \strng{authorfullhash}{16b1d6105e8c665c6ea031132da22f8b}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Comptes rendus de l'Acad\'{e}mie des Sciences de Paris}
      \field{title}{{M\'{e}thodes g\'{e}n\'{e}rales pour la r\'{e}solution des syst\`{e}mes d'\'{e}quations simultan\'{e}es}}
      \field{volume}{25}
      \field{year}{1847}
      \field{pages}{536\bibrangedash 538}
      \range{pages}{3}
    \endentry
    \entry{Chambolle2015}{article}{}
      \name{author}{2}{}{%
        {{hash=680af58d38f3c3c0dd43394b2f898dca}{%
           family={Chambolle},
           familyi={C\bibinitperiod},
           given={Antonin},
           giveni={A\bibinitperiod}}}%
        {{hash=bd33384632289703381b32e50974752d}{%
           family={Dossal},
           familyi={D\bibinitperiod},
           given={Charles},
           giveni={C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{0a6b77c07c999c80bf6c81bd1ed7d4ca}
      \strng{fullhash}{0a6b77c07c999c80bf6c81bd1ed7d4ca}
      \strng{bibnamehash}{0a6b77c07c999c80bf6c81bd1ed7d4ca}
      \strng{authorbibnamehash}{0a6b77c07c999c80bf6c81bd1ed7d4ca}
      \strng{authornamehash}{0a6b77c07c999c80bf6c81bd1ed7d4ca}
      \strng{authorfullhash}{0a6b77c07c999c80bf6c81bd1ed7d4ca}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We discuss here the convergence of the iterates of the \textquotedblleft{}Fast Iterative Shrinkage/Thresholding Algorithm,\textquotedblright{} which is an algorithm proposed by Beck and Teboulle for minimizing the sum of two convex, lower-semicontinuous, and proper functions (defined in a Euclidean or Hilbert space), such that one is differentiable with Lipschitz gradient, and the proximity operator of the second is easy to compute. It builds a sequence of iterates for which the objective is controlled, up to a (nearly optimal) constant, by the inverse of the square of the iteration number. However, the convergence of the iterates themselves is not known. We show here that with a small modification, we can ensure the same upper bound for the decay of the energy, as well as the convergence of the iterates to a minimizer.}
      \field{issn}{1573-2878}
      \field{journaltitle}{Journal of Optimization Theory and Applications}
      \field{month}{5}
      \field{number}{3}
      \field{title}{{On the convergence of the iterates of the \textquotedblleft{}{F}ast {I}terative {S}hrinkage/{T}hresholding {A}lgorithm\textquotedblright{}}}
      \field{volume}{166}
      \field{year}{2015}
      \field{pages}{968\bibrangedash 982}
      \range{pages}{15}
      \verb{doi}
      \verb 10.1007/S10957-015-0746-4
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Chambolle, Dossal - 2015 - On the Convergence of the Iterates of the \textquotedblleft{}Fast Iterative ShrinkageThresholding Algorithm\textquotedblright{}.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10957-015-0746-4
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10957-015-0746-4
      \endverb
      \keyw{Applications of Mathematics,Calculus of Variations and Optimal Control,Engineering,Operations Research/Decision Theory,Optimization,Theory of Computation,general}
    \endentry
    \entry{Charitha2010}{article}{}
      \name{author}{2}{}{%
        {{hash=43f033cc98486fe736602916f9343ec3}{%
           family={Charitha},
           familyi={C\bibinitperiod},
           given={Cherugondi},
           giveni={C\bibinitperiod}}}%
        {{hash=978a78a0d314cd5afc919390ce551ff1}{%
           family={Dutta},
           familyi={D\bibinitperiod},
           given={Joydeep},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{9f2e202c5f6b8ce67d8ba44ba50ce2f2}
      \strng{fullhash}{9f2e202c5f6b8ce67d8ba44ba50ce2f2}
      \strng{bibnamehash}{9f2e202c5f6b8ce67d8ba44ba50ce2f2}
      \strng{authorbibnamehash}{9f2e202c5f6b8ce67d8ba44ba50ce2f2}
      \strng{authornamehash}{9f2e202c5f6b8ce67d8ba44ba50ce2f2}
      \strng{authorfullhash}{9f2e202c5f6b8ce67d8ba44ba50ce2f2}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Pacific Journal of Optimization}
      \field{number}{3}
      \field{title}{{Regularized gap functions and error bounds for vector variational inequalities}}
      \field{volume}{6}
      \field{year}{2010}
      \field{pages}{497\bibrangedash 510}
      \range{pages}{14}
      \verb{file}
      \verb :Users/zalgo/Downloads/pjov6n3p497.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://www.ybook.co.jp/online2/oppjo/vol6/p497.html
      \endverb
      \verb{url}
      \verb http://www.ybook.co.jp/online2/oppjo/vol6/p497.html
      \endverb
    \endentry
    \entry{Chen2000}{incollection}{}
      \name{author}{3}{}{%
        {{hash=c20de537c9f162ce95f9282edc5f918a}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Guang-ya},
           giveni={G\bibinithyphendelim y\bibinitperiod}}}%
        {{hash=4d599eeeb4da1cd14594ab9fa715cdd0}{%
           family={Goh},
           familyi={G\bibinitperiod},
           given={Chuen-Jin},
           giveni={C\bibinithyphendelim J\bibinitperiod}}}%
        {{hash=ed6f1cd802fbd72f9d2ba807fe42c15c}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Xiao\bibnamedelima Qi},
           giveni={X\bibinitperiod\bibinitdelim Q\bibinitperiod}}}%
      }
      \name{editor}{1}{}{%
        {{hash=d647ebb07d0fb48ac0b5286a4c013a4b}{%
           family={Giannessi},
           familyi={G\bibinitperiod},
           given={Franco},
           giveni={F\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Boston, MA}%
      }
      \list{publisher}{1}{%
        {Springer US}%
      }
      \strng{namehash}{e437183beb7990ee56a90ef64c97b6bb}
      \strng{fullhash}{e437183beb7990ee56a90ef64c97b6bb}
      \strng{bibnamehash}{e437183beb7990ee56a90ef64c97b6bb}
      \strng{authorbibnamehash}{e437183beb7990ee56a90ef64c97b6bb}
      \strng{authornamehash}{e437183beb7990ee56a90ef64c97b6bb}
      \strng{authorfullhash}{e437183beb7990ee56a90ef64c97b6bb}
      \strng{editorbibnamehash}{d647ebb07d0fb48ac0b5286a4c013a4b}
      \strng{editornamehash}{d647ebb07d0fb48ac0b5286a4c013a4b}
      \strng{editorfullhash}{d647ebb07d0fb48ac0b5286a4c013a4b}
      \field{extraname}{1}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We extend the theory of gap functions for scalar variational inequality problems (see [1,8)) to the case of vector variational inequality. The gap functions for vector variational inequality are defined as set-valued mappings. The significance of the gap function is interpreted in terms of the inverse vector variational inequality. Convexity properties of these set-valued mappings are studied under different assumptions.}
      \field{booktitle}{Vector Variational Inequalities and Vector Equilibria: Mathematical Theories}
      \field{title}{{On Gap Functions for Vector Variational Inequalities}}
      \field{year}{2000}
      \field{pages}{55\bibrangedash 72}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1007/978-1-4613-0299-5_4
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Goh, Yang - 2000 - On Gap Functions for Vector Variational Inequalities.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-1-4613-0299-5_4
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-1-4613-0299-5_4
      \endverb
      \keyw{Fenchel conjugate,Vector variational inequality,duality,gap functions}
    \endentry
    \entry{Chen1998}{article}{}
      \name{author}{3}{}{%
        {{hash=c20de537c9f162ce95f9282edc5f918a}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Guang-ya},
           giveni={G\bibinithyphendelim y\bibinitperiod}}}%
        {{hash=4d599eeeb4da1cd14594ab9fa715cdd0}{%
           family={Goh},
           familyi={G\bibinitperiod},
           given={Chuen-Jin},
           giveni={C\bibinithyphendelim J\bibinitperiod}}}%
        {{hash=ed6f1cd802fbd72f9d2ba807fe42c15c}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Xiao\bibnamedelima Qi},
           giveni={X\bibinitperiod\bibinitdelim Q\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {North-Holland}%
      }
      \strng{namehash}{e437183beb7990ee56a90ef64c97b6bb}
      \strng{fullhash}{e437183beb7990ee56a90ef64c97b6bb}
      \strng{bibnamehash}{e437183beb7990ee56a90ef64c97b6bb}
      \strng{authorbibnamehash}{e437183beb7990ee56a90ef64c97b6bb}
      \strng{authornamehash}{e437183beb7990ee56a90ef64c97b6bb}
      \strng{authorfullhash}{e437183beb7990ee56a90ef64c97b6bb}
      \field{extraname}{2}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We generalize the concept of a gap function previously defined for a convex (scalar) optimization problem to a convex multicriteria optimization problem and study its various properties. {{$\copyright$}} 1998 Elsevier Science B.V. All rights reserved.}
      \field{issn}{0377-2217}
      \field{journaltitle}{European Journal of Operational Research}
      \field{month}{11}
      \field{number}{1}
      \field{title}{{The gap function of a convex multicriteria optimization problem}}
      \field{volume}{111}
      \field{year}{1998}
      \field{pages}{142\bibrangedash 151}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1016/S0377-2217(97)00300-7
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1016/S0377-2217(97)00300-7
      \endverb
      \verb{url}
      \verb https://doi.org/10.1016/S0377-2217(97)00300-7
      \endverb
      \keyw{Cone-convexity of set-valued functions,Gap function,Multicriteria optimization,Wolfe dual}
    \endentry
    \entry{Deb2014}{article}{}
      \name{author}{2}{}{%
        {{hash=b11d1da0cc2920eef235d484e6b858ef}{%
           family={Deb},
           familyi={D\bibinitperiod},
           given={Kalyanmoy},
           giveni={K\bibinitperiod}}}%
        {{hash=a7ee3f759bf5ccf1b94bebde9d2af758}{%
           family={Jain},
           familyi={J\bibinitperiod},
           given={Himanshu},
           giveni={H\bibinitperiod}}}%
      }
      \list{publisher}{2}{%
        {Institute of Electrical}%
        {Electronics Engineers Inc.}%
      }
      \strng{namehash}{9282fa3947bfd0e4b3ea6b8ef5c86934}
      \strng{fullhash}{9282fa3947bfd0e4b3ea6b8ef5c86934}
      \strng{bibnamehash}{9282fa3947bfd0e4b3ea6b8ef5c86934}
      \strng{authorbibnamehash}{9282fa3947bfd0e4b3ea6b8ef5c86934}
      \strng{authornamehash}{9282fa3947bfd0e4b3ea6b8ef5c86934}
      \strng{authorfullhash}{9282fa3947bfd0e4b3ea6b8ef5c86934}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Having developed multiobjective optimization algorithms using evolutionary optimization methods and demonstrated their niche on various practical problems involving mostly two and three objectives, there is now a growing need for developing evolutionary multiobjective optimization (EMO) algorithms for handling many-objective (having four or more objectives) optimization problems. In this paper, we recognize a few recent efforts and discuss a number of viable directions for developing a potential EMO algorithm for solving many-objective optimization problems. Thereafter, we suggest a reference-point-based many-objective evolutionary algorithm following NSGA-II framework (we call it NSGA-III) that emphasizes population members that are nondominated, yet close to a set of supplied reference points. The proposed NSGA-III is applied to a number of many-objective test problems with three to 15 objectives and compared with two versions of a recently suggested EMO algorithm (MOEA/D). While each of the two MOEA/D methods works well on different classes of problems, the proposed NSGA-III is found to produce satisfactory results on all problems considered in this paper. This paper presents results on unconstrained problems, and the sequel paper considers constrained and other specialties in handling many-objective optimization problems. {{$\copyright$}} 1997-2012 IEEE.}
      \field{issn}{1089778X}
      \field{journaltitle}{IEEE Transactions on Evolutionary Computation}
      \field{number}{4}
      \field{title}{{An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, Part I: Solving problems with box constraints}}
      \field{volume}{18}
      \field{year}{2014}
      \field{pages}{577\bibrangedash 601}
      \range{pages}{25}
      \verb{doi}
      \verb 10.1109/TEVC.2013.2281535
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1109/TEVC.2013.2281535
      \endverb
      \verb{url}
      \verb https://doi.org/10.1109/TEVC.2013.2281535
      \endverb
      \keyw{Many-objective optimization,NSGA-III,evolutionary computation,large dimension,multi-criterion optimization,non-dominated sorting}
    \endentry
    \entry{Deb2002}{article}{}
      \name{author}{4}{}{%
        {{hash=b11d1da0cc2920eef235d484e6b858ef}{%
           family={Deb},
           familyi={D\bibinitperiod},
           given={Kalyanmoy},
           giveni={K\bibinitperiod}}}%
        {{hash=9fbc80a7299653739376ff004f77fab1}{%
           family={Pratap},
           familyi={P\bibinitperiod},
           given={Amrit},
           giveni={A\bibinitperiod}}}%
        {{hash=4e180088c2d397bb169dfb45679eba22}{%
           family={Agarwal},
           familyi={A\bibinitperiod},
           given={Sameer},
           giveni={S\bibinitperiod}}}%
        {{hash=9661b0fa73eb1cbd0885c442a9870907}{%
           family={Meyarivan},
           familyi={M\bibinitperiod},
           given={T.},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{b08e5063396137513578f8d3ebd59c5d}
      \strng{fullhash}{b08e5063396137513578f8d3ebd59c5d}
      \strng{bibnamehash}{b08e5063396137513578f8d3ebd59c5d}
      \strng{authorbibnamehash}{b08e5063396137513578f8d3ebd59c5d}
      \strng{authornamehash}{b08e5063396137513578f8d3ebd59c5d}
      \strng{authorfullhash}{b08e5063396137513578f8d3ebd59c5d}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multiobjective evolutionary algorithms (EAs) that use nondominated sorting and sharing have been criticized mainly for their: 1) O(MN3) computational complexity (where M is the number of objectives and N is the population size); 2) nonelitism approach; and 3) the need for specifying a sharing parameter. In this paper, we suggest a nondominated sorting-based multiobjective EA (MOEA), called nondominated sorting genetic algorithm II (NSGA-II), which alleviates all the above three difficulties. Specifically, a fast nondominated sorting approach with O(MN2) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best (with respect to fitness and spread) N solutions. Simulation results on difficult test problems show that the proposed NSGA-II, in most problems, is able to find much better spread of solutions and better convergence near the true Pareto-optimal front compared to Pareto-archived evolution strategy and strength-Pareto EA - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multiobjective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective seven-constraint nonlinear problem, are compared with another constrained multiobjective optimizer and much better performance of NSGA-II is observed.}
      \field{issn}{1089778X}
      \field{journaltitle}{IEEE Transactions on Evolutionary Computation}
      \field{month}{4}
      \field{number}{2}
      \field{title}{{A fast and elitist multiobjective genetic algorithm: NSGA-II}}
      \field{volume}{6}
      \field{year}{2002}
      \field{pages}{182\bibrangedash 197}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1109/4235.996017
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Deb et al. - 2002 - A fast and elitist multiobjective genetic algorithm NSGA-II.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1109/4235.996017
      \endverb
      \verb{url}
      \verb https://doi.org/10.1109/4235.996017
      \endverb
      \keyw{Constraint handling,Elitism,Genetic algorithms,Multicriterion decision making,Multiobjective optimization,Pareto-optimal solutions}
    \endentry
    \entry{Dong2018}{incollection}{}
      \name{author}{5}{}{%
        {{hash=77ff0f1e56994188df11b7a511b352ff}{%
           family={Dong},
           familyi={D\bibinitperiod},
           given={Jin-Dong},
           giveni={J\bibinithyphendelim D\bibinitperiod}}}%
        {{hash=5462a1dabd6203dfddbe20c2d47bdd4d}{%
           family={Cheng},
           familyi={C\bibinitperiod},
           given={An-Chieh},
           giveni={A\bibinithyphendelim C\bibinitperiod}}}%
        {{hash=2ac3b8d02009088e8cd462a1821c0d32}{%
           family={Juan},
           familyi={J\bibinitperiod},
           given={Da-Cheng},
           giveni={D\bibinithyphendelim C\bibinitperiod}}}%
        {{hash=c1d89b1112687aab0def500c9ed1fd46}{%
           family={Wei},
           familyi={W\bibinitperiod},
           given={Wei},
           giveni={W\bibinitperiod}}}%
        {{hash=7e4290bad319dc9b96a098c8eee04fc8}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Min},
           giveni={M\bibinitperiod}}}%
      }
      \name{editor}{4}{}{%
        {{hash=4b784283b1a13714245b9089bc0ebd7f}{%
           family={Ferrari},
           familyi={F\bibinitperiod},
           given={Vittori},
           giveni={V\bibinitperiod}}}%
        {{hash=11ae05bcfa9fe4b27159ba542f20264e}{%
           family={Martial},
           familyi={M\bibinitperiod},
           given={Hebert},
           giveni={H\bibinitperiod}}}%
        {{hash=bd7af1e31d4938e27dbc867b4bb71e9d}{%
           family={Sminchisescu},
           familyi={S\bibinitperiod},
           given={Cristian},
           giveni={C\bibinitperiod}}}%
        {{hash=214a71ab353c5570ed4acb95cf84f539}{%
           family={Weiss},
           familyi={W\bibinitperiod},
           given={Yair},
           giveni={Y\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Munich}%
      }
      \list{publisher}{1}{%
        {Springer Cham}%
      }
      \strng{namehash}{5a1a752c6587547567aed7b5aebc5c2a}
      \strng{fullhash}{5a1a752c6587547567aed7b5aebc5c2a}
      \strng{bibnamehash}{5a1a752c6587547567aed7b5aebc5c2a}
      \strng{authorbibnamehash}{5a1a752c6587547567aed7b5aebc5c2a}
      \strng{authornamehash}{5a1a752c6587547567aed7b5aebc5c2a}
      \strng{authorfullhash}{5a1a752c6587547567aed7b5aebc5c2a}
      \strng{editorbibnamehash}{d6a6b6b62ac4aef6094b8fbd830e6cf6}
      \strng{editornamehash}{d6a6b6b62ac4aef6094b8fbd830e6cf6}
      \strng{editorfullhash}{d6a6b6b62ac4aef6094b8fbd830e6cf6}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent breakthroughs in Neural Architectural Search (NAS) have achieved state-of-the-art performances in applications such as image classification and language modeling. However, these techniques typically ignore device-related objectives such as inference time, memory usage, and power consumption. Optimizing neural architecture for device-related objectives is immensely crucial for deploying deep networks on portable devices with limited computing resources. We propose DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architectures, optimizing for both device-related (e.g., inference time and memory usage) and device-agnostic (e.g., accuracy and model size) objectives. DPP-Net employs a compact search space inspired by current state-of-the-art mobile CNNs, and further improves search efficiency by adopting progressive search (Liu et al. 2017). Experimental results on CIFAR-10 are poised to demonstrate the effectiveness of Pareto-optimal networks found by DPP-Net, for three different devices: (1) a workstation with Titan X GPU, (2) NVIDIA Jetson TX1 embedded system, and (3) mobile phone with ARM Cortex-A53. Compared to CondenseNet and NASNet (Mobile), DPP-Net achieves better performances: higher accuracy & shorter inference time on various devices. Additional experimental results show that models found by DPP-Net also achieve considerably-good performance on ImageNet as well.}
      \field{booktitle}{Computer Vision -- ECCV 2018}
      \field{edition}{first}
      \field{eprinttype}{arXiv}
      \field{isbn}{9783030012519}
      \field{issn}{16113349}
      \field{title}{{DPP-Net: Device-aware progressive search for Pareto-optimal neural architectures}}
      \field{year}{2018}
      \field{pages}{540\bibrangedash 555}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1007/978-3-030-01252-6_32
      \endverb
      \verb{eprint}
      \verb 1806.08198
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Dong et al. - 2018 - DPP-Net Device-aware progressive search for Pareto-optimal neural architectures.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-3-030-01252-6_32
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-3-030-01252-6_32
      \endverb
      \keyw{Architecture search,Multi-objective optimization}
    \endentry
    \entry{Dutta2017}{article}{}
      \name{author}{3}{}{%
        {{hash=978a78a0d314cd5afc919390ce551ff1}{%
           family={Dutta},
           familyi={D\bibinitperiod},
           given={Joydeep},
           giveni={J\bibinitperiod}}}%
        {{hash=7856346a18deee2d8e3f4534950a5ce4}{%
           family={Kesarwani},
           familyi={K\bibinitperiod},
           given={Poonam},
           giveni={P\bibinitperiod}}}%
        {{hash=7ce4bc660ef4d4a113a844fc032764cf}{%
           family={Gupta},
           familyi={G\bibinitperiod},
           given={Sanjeev},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Taylor & Francis}%
      }
      \strng{namehash}{58d584a793814c328059497d106572cb}
      \strng{fullhash}{58d584a793814c328059497d106572cb}
      \strng{bibnamehash}{58d584a793814c328059497d106572cb}
      \strng{authorbibnamehash}{58d584a793814c328059497d106572cb}
      \strng{authornamehash}{58d584a793814c328059497d106572cb}
      \strng{authorfullhash}{58d584a793814c328059497d106572cb}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this article, our main aim is to develop gap functions and error bounds for a (non-smooth) convex vector optimization problem. We show that by focusing on convexity we are able to quite efficiently compute the gap functions and try to gain insight about the structure of set of weak Pareto minimizers by viewing its graph. We will discuss several properties of gap functions and develop error bounds when the data are strongly convex. We also compare our results with some recent results on weak vector variational inequalities with set-valued maps, and also argue as to why we focus on the convex case.}
      \field{issn}{10294945}
      \field{journaltitle}{Optimization}
      \field{number}{11}
      \field{title}{{Gap functions and error bounds for nonsmooth convex vector optimization problem}}
      \field{volume}{66}
      \field{year}{2017}
      \field{pages}{1807\bibrangedash 1836}
      \range{pages}{30}
      \verb{doi}
      \verb 10.1080/02331934.2017.1332622
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Dutta, Kesarwani, Gupta - 2017 - Gap functions and error bounds for nonsmooth convex vector optimization problem.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1080/02331934.2017.1332622
      \endverb
      \verb{url}
      \verb https://doi.org/10.1080/02331934.2017.1332622
      \endverb
      \keyw{Convex functions,convex optimization,error bounds,gap function,subdifferential,vector optimization}
    \endentry
    \entry{Ehrgott2014}{article}{}
      \name{author}{3}{}{%
        {{hash=dd52f015c9f959c6a328469399e09e4c}{%
           family={Ehrgott},
           familyi={E\bibinitperiod},
           given={Matthias},
           giveni={M\bibinitperiod}}}%
        {{hash=5492088c49c95ad0ccab2ae0982eb9f1}{%
           family={Ide},
           familyi={I\bibinitperiod},
           given={Jonas},
           giveni={J\bibinitperiod}}}%
        {{hash=b1d7cd1029a20eb60a2bf3baba727869}{%
           family={Sch\"{o}bel},
           familyi={S\bibinitperiod},
           given={Anita},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{c01ceff71434e469ef21f74c74268a71}
      \strng{fullhash}{c01ceff71434e469ef21f74c74268a71}
      \strng{bibnamehash}{c01ceff71434e469ef21f74c74268a71}
      \strng{authorbibnamehash}{c01ceff71434e469ef21f74c74268a71}
      \strng{authornamehash}{c01ceff71434e469ef21f74c74268a71}
      \strng{authorfullhash}{c01ceff71434e469ef21f74c74268a71}
      \field{sortinit}{E}
      \field{sortinithash}{8da8a182d344d5b9047633dfc0cc9131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In real-world applications of optimization, optimal solutions are often of limited value, because disturbances of or changes to input data may diminish the quality of an optimal solution or even render it infeasible. One way to deal with uncertain input data is robust optimization, the aim of which is to find solutions which remain feasible and of good quality for all possible scenarios, i.e.; realizations of the uncertain data. For single objective optimization, several definitions of robustness have been thoroughly analyzed and robust optimization methods have been developed. In this paper, we extend the concept of minmax robustness (Ben-Tal, Ghaoui, & Nemirovski, 2009) to multi-objective optimization and call this extension robust efficiency for uncertain multi-objective optimization problems. We use ingredients from robust (single objective) and (deterministic) multi-objective optimization to gain insight into the new area of robust multi-objective optimization. We analyze the new concept and discuss how robust solutions of multi-objective optimization problems may be computed. To this end, we use techniques from both robust (single objective) and (deterministic) multi-objective optimization. The new concepts are illustrated with some linear and quadratic programming instances. {{$\copyright$}} 2014 Elsevier B.V. All rights reserved.}
      \field{issn}{03772217}
      \field{journaltitle}{European Journal of Operational Research}
      \field{number}{1}
      \field{title}{{Minmax robustness for multi-objective optimization problems}}
      \field{volume}{239}
      \field{year}{2014}
      \field{pages}{17\bibrangedash 31}
      \range{pages}{15}
      \verb{doi}
      \verb 10.1016/j.ejor.2014.03.013
      \endverb
      \keyw{Multi-objective optimization,Robustness and sensitivity analysis,Scenarios,Uncertainty modelling}
    \endentry
    \entry{ElMoudden2020}{article}{}
      \name{author}{2}{}{%
        {{hash=c6e9e4ed72b98225fcd6557364b73062}{%
           family={{El Moudden}},
           familyi={E\bibinitperiod},
           given={Mustapha},
           giveni={M\bibinitperiod}}}%
        {{hash=3f327875973994ce4bf232ee985b9d69}{%
           family={{El Mouatasim}},
           familyi={E\bibinitperiod},
           given={Abdelkrim},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{2a94c98fc93bb8be0f09947cf483e855}
      \strng{fullhash}{2a94c98fc93bb8be0f09947cf483e855}
      \strng{bibnamehash}{2a94c98fc93bb8be0f09947cf483e855}
      \strng{authorbibnamehash}{2a94c98fc93bb8be0f09947cf483e855}
      \strng{authornamehash}{2a94c98fc93bb8be0f09947cf483e855}
      \strng{authorfullhash}{2a94c98fc93bb8be0f09947cf483e855}
      \field{sortinit}{E}
      \field{sortinithash}{8da8a182d344d5b9047633dfc0cc9131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, we propose two methods for solving unconstrained multiobjective optimization problems. First, we present a diagonal steepest descent method, in which, at each iteration, a common diagonal matrix is used to approximate the Hessian of every objective function. This method works directly with the objective functions, without using any kind of a priori chosen parameters. It is proved that accumulation points of the sequence generated by the method are Pareto-critical points under standard assumptions. Based on this approach and on the Nesterov step strategy, an improved version of the method is proposed and its convergence rate is analyzed. Finally, computational experiments are presented in order to analyze the performance of the proposed methods.}
      \field{issn}{15732878}
      \field{journaltitle}{Journal of Optimization Theory and Applications}
      \field{month}{11}
      \field{title}{{Accelerated diagonal steepest descent method for unconstrained multiobjective optimization}}
      \field{volume}{188}
      \field{year}{2021}
      \field{pages}{220\bibrangedash 242}
      \range{pages}{23}
      \verb{doi}
      \verb 10.1007/s10957-020-01785-9
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/El Moudden, El Mouatasim - 2020 - Accelerated Diagonal Steepest Descent Method for Unconstrained Multiobjective Optimization.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10957-020-01785-9
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10957-020-01785-9
      \endverb
      \keyw{Diagonal steepest descent methods,Multiobjective optimization,Nesterov step,Pareto critical,Unconstrained problems}
    \endentry
    \entry{Elsken2019}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=17831398c93648de0ce8c5bcfbbc9784}{%
           family={Elsken},
           familyi={E\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=528d4af87fd2ecf5fb8a22db913ce088}{%
           family={Hutter},
           familyi={H\bibinitperiod},
           given={Frank},
           giveni={F\bibinitperiod}}}%
        {{hash=21923fb486ef4c63bad4c8fcac6cc124}{%
           family={Metzen},
           familyi={M\bibinitperiod},
           given={Jan\bibnamedelima Hendrik},
           giveni={J\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
      }
      \strng{namehash}{56285a28d618ee3bfa831aa738756418}
      \strng{fullhash}{56285a28d618ee3bfa831aa738756418}
      \strng{bibnamehash}{56285a28d618ee3bfa831aa738756418}
      \strng{authorbibnamehash}{56285a28d618ee3bfa831aa738756418}
      \strng{authornamehash}{56285a28d618ee3bfa831aa738756418}
      \strng{authorfullhash}{56285a28d618ee3bfa831aa738756418}
      \field{sortinit}{E}
      \field{sortinithash}{8da8a182d344d5b9047633dfc0cc9131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Neural Architecture Search aims at automatically finding neural network architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for, e.g., image recognition, they are problematic under resource constraints for two reasons: (1) the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption; (2) most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the entire Pareto front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates child networks that are warm started with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform both hand-crafted as well as automatically-designed networks.}
      \field{booktitle}{7th International Conference on Learning Representations}
      \field{eprinttype}{arXiv}
      \field{title}{{Efficient multi-objective neural architecture search via Lamarckian evolution}}
      \field{year}{2019}
      \verb{eprint}
      \verb 1804.09081
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Elsken, Hutter, Metzen - 2019 - Efficient multi-objective neural architecture search via Lamarckian evolution.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://openreview.net/forum?id=ByME42AqK7
      \endverb
      \verb{url}
      \verb https://openreview.net/forum?id=ByME42AqK7
      \endverb
    \endentry
    \entry{Eschenauer1990}{book}{}
      \name{author}{3}{}{%
        {{hash=146821f6319f539c96c903cb40c0c806}{%
           family={Eschenauer},
           familyi={E\bibinitperiod},
           given={Hans},
           giveni={H\bibinitperiod}}}%
        {{hash=e02beee15e2c7c90d5a4b977ac92c685}{%
           family={Koski},
           familyi={K\bibinitperiod},
           given={Juhani},
           giveni={J\bibinitperiod}}}%
        {{hash=8f5e26e870f981640141cfa9bcf3aa0e}{%
           family={Osyczka},
           familyi={O\bibinitperiod},
           given={Andrzej},
           giveni={A\bibinitperiod}}}%
      }
      \name{editor}{3}{}{%
        {{hash=146821f6319f539c96c903cb40c0c806}{%
           family={Eschenauer},
           familyi={E\bibinitperiod},
           given={Hans},
           giveni={H\bibinitperiod}}}%
        {{hash=e02beee15e2c7c90d5a4b977ac92c685}{%
           family={Koski},
           familyi={K\bibinitperiod},
           given={Juhani},
           giveni={J\bibinitperiod}}}%
        {{hash=8f5e26e870f981640141cfa9bcf3aa0e}{%
           family={Osyczka},
           familyi={O\bibinitperiod},
           given={Andrzej},
           giveni={A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer Berlin Heidelberg}%
      }
      \strng{namehash}{e84ca9ad8bd0deb912fb876d93ef5c20}
      \strng{fullhash}{e84ca9ad8bd0deb912fb876d93ef5c20}
      \strng{bibnamehash}{e84ca9ad8bd0deb912fb876d93ef5c20}
      \strng{authorbibnamehash}{e84ca9ad8bd0deb912fb876d93ef5c20}
      \strng{authornamehash}{e84ca9ad8bd0deb912fb876d93ef5c20}
      \strng{authorfullhash}{e84ca9ad8bd0deb912fb876d93ef5c20}
      \strng{editorbibnamehash}{e84ca9ad8bd0deb912fb876d93ef5c20}
      \strng{editornamehash}{e84ca9ad8bd0deb912fb876d93ef5c20}
      \strng{editorfullhash}{e84ca9ad8bd0deb912fb876d93ef5c20}
      \field{sortinit}{E}
      \field{sortinithash}{8da8a182d344d5b9047633dfc0cc9131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Multicriteria Design Optimization}
      \field{isbn}{978-3-642-48699-9}
      \field{title}{{Multicriteria Design Optimization}}
      \field{year}{1990}
      \verb{doi}
      \verb 10.1007/978-3-642-48697-5
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-3-642-48697-5
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-3-642-48697-5
      \endverb
    \endentry
    \entry{Fliege2009}{article}{}
      \name{author}{3}{}{%
        {{hash=c29438705766e23e4be7e0785b9b5f04}{%
           family={Fliege},
           familyi={F\bibinitperiod},
           given={J\"{o}rg},
           giveni={J\bibinitperiod}}}%
        {{hash=79b9d0386837e14eb2455f224b15bec4}{%
           family={{Gra\~{n}a Drummond}},
           familyi={G\bibinitperiod},
           given={Luis\bibnamedelima Mauricio},
           giveni={L\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=975a8e0ef61c5aca1bf497aac32c8c43}{%
           family={Svaiter},
           familyi={S\bibinitperiod},
           given={Benar\bibnamedelima Fux},
           giveni={B\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
      }
      \list{publisher}{2}{%
        {Society for Industrial}%
        {Applied Mathematics}%
      }
      \strng{namehash}{78d2ad96e0041ee2b8606c8dfcfb4414}
      \strng{fullhash}{78d2ad96e0041ee2b8606c8dfcfb4414}
      \strng{bibnamehash}{78d2ad96e0041ee2b8606c8dfcfb4414}
      \strng{authorbibnamehash}{78d2ad96e0041ee2b8606c8dfcfb4414}
      \strng{authornamehash}{78d2ad96e0041ee2b8606c8dfcfb4414}
      \strng{authorfullhash}{78d2ad96e0041ee2b8606c8dfcfb4414}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose an extension of Newton's method for unconstrained multiobjective optimization (multicriteria optimization). This method does not use a priori chosen weighting factors or any other form of a priori ranking or ordering information for the different objective functions. Newton's direction at each iterate is obtained by minimizing the max-ordering scalarization of the variations on the quadratic approximations of the objective functions. The objective functions are assumed to be twice continuously differentiable and locally strongly convex. Under these hypotheses, the method, as in the classical case, is locally superlinear convergent to optimal points. Again as in the scalar case, if the second derivatives are Lipschitz continuous, the rate of convergence is quadratic. Our convergence analysis uses a Kantorovich-like technique. As a byproduct, existence of optima is obtained under semilocal assumptions. {{$\copyright$}} 2009 Society for Industrial and Applied Mathematics.}
      \field{issn}{10526234}
      \field{journaltitle}{SIAM Journal on Optimization}
      \field{month}{1}
      \field{number}{2}
      \field{title}{{Newton's method for multiobjective optimization}}
      \field{volume}{20}
      \field{year}{2009}
      \field{pages}{602\bibrangedash 626}
      \range{pages}{25}
      \verb{doi}
      \verb 10.1137/08071692X
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Fliege, Drummond, Svaiter - 2009 - Newton's method for multiobjective optimization.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1137/08071692X
      \endverb
      \verb{url}
      \verb https://doi.org/10.1137/08071692X
      \endverb
      \keyw{Multicriteria optimization,Multiobjective programming,Newton's method,Pareto points}
    \endentry
    \entry{Fliege2000}{article}{}
      \name{author}{2}{}{%
        {{hash=c29438705766e23e4be7e0785b9b5f04}{%
           family={Fliege},
           familyi={F\bibinitperiod},
           given={J\"{o}rg},
           giveni={J\bibinitperiod}}}%
        {{hash=975a8e0ef61c5aca1bf497aac32c8c43}{%
           family={Svaiter},
           familyi={S\bibinitperiod},
           given={Benar\bibnamedelima Fux},
           giveni={B\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Physica-Verlag}%
      }
      \strng{namehash}{fb1f2dea789eeb36f1ff502b0fcce29a}
      \strng{fullhash}{fb1f2dea789eeb36f1ff502b0fcce29a}
      \strng{bibnamehash}{fb1f2dea789eeb36f1ff502b0fcce29a}
      \strng{authorbibnamehash}{fb1f2dea789eeb36f1ff502b0fcce29a}
      \strng{authornamehash}{fb1f2dea789eeb36f1ff502b0fcce29a}
      \strng{authorfullhash}{fb1f2dea789eeb36f1ff502b0fcce29a}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose a steepest descent method for unconstrained multicriteria optimization and a "feasible descent direction" method for the constrained case. In the unconstrained case, the objective functions are assumed to be continuously differentiable. In the constrained case, objective and constraint functions are assumed to be Lipshitz-continuously differentiable and a constraint qualification is assumed. Under these conditions, it is shown that these methods converge to a point satisfying certain first-order necessary conditions for Pareto optimality. Both methods do not scalarize the original vector optimization problem. Neither ordering information nor weighting factors for the different objective functions are assumed to be known. In the single objective case, we retrieve the Steepest descent method and Zoutendijk's method of feasible directions, respectively.}
      \field{issn}{1432-2994}
      \field{journaltitle}{Mathematical Methods of Operations Research}
      \field{month}{8}
      \field{number}{3}
      \field{title}{{Steepest descent methods for multicriteria optimization}}
      \field{volume}{51}
      \field{year}{2000}
      \field{pages}{479\bibrangedash 494}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1007/s001860000043
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Fliege, Svaiter - 2000 - Steepest descent methods for multicriteria optimization.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s001860000043
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s001860000043
      \endverb
      \keyw{Multi-objective programming,Multicriteria optimization,Pareto points,Steepest descent,Vector optimization}
    \endentry
    \entry{Fliege2019}{article}{}
      \name{author}{3}{}{%
        {{hash=c29438705766e23e4be7e0785b9b5f04}{%
           family={Fliege},
           familyi={F\bibinitperiod},
           given={J\"{o}rg},
           giveni={J\bibinitperiod}}}%
        {{hash=f1afef4a57c9791597e0aca02638bc38}{%
           family={Vaz},
           familyi={V\bibinitperiod},
           given={A.\bibnamedelimi Ismael\bibnamedelima F.},
           giveni={A\bibinitperiod\bibinitdelim I\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
        {{hash=76659c566fc35b83f4aebc5acd06913c}{%
           family={Vicente},
           familyi={V\bibinitperiod},
           given={Luis\bibnamedelima Nunes},
           giveni={L\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Taylor & Francis}%
      }
      \strng{namehash}{eefef88129a4c2bb84f140d38a70adc8}
      \strng{fullhash}{eefef88129a4c2bb84f140d38a70adc8}
      \strng{bibnamehash}{eefef88129a4c2bb84f140d38a70adc8}
      \strng{authorbibnamehash}{eefef88129a4c2bb84f140d38a70adc8}
      \strng{authornamehash}{eefef88129a4c2bb84f140d38a70adc8}
      \strng{authorfullhash}{eefef88129a4c2bb84f140d38a70adc8}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A number of first-order methods have been proposed for smooth multiobjective optimization for which some form of convergence to first-order criticality has been proved. Such convergence is global in the sense of being independent of the starting point. In this paper, we analyse the rate of convergence of gradient descent for smooth unconstrained multiobjective optimization, and we do it for non-convex, convex, and strongly convex vector functions. These global rates are shown to be the same as for gradient descent in single-objective optimization and correspond to appropriate worst-case complexity bounds. In the convex cases, the rates are given for implicit scalarizations of the problem vector function.}
      \field{issn}{10294937}
      \field{journaltitle}{Optimization Methods and Software}
      \field{month}{8}
      \field{number}{5}
      \field{title}{{Complexity of gradient descent for multiobjective optimization}}
      \field{volume}{34}
      \field{year}{2019}
      \field{pages}{949\bibrangedash 959}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1080/10556788.2018.1510928
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Fliege, Vaz, Vicente - 2019 - Complexity of gradient descent for multiobjective optimization.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1080/10556788.2018.1510928
      \endverb
      \verb{url}
      \verb https://doi.org/10.1080/10556788.2018.1510928
      \endverb
      \keyw{Multiobjective optimization,global rates,gradient descent,steepest descent,worst-case complexity}
    \endentry
    \entry{Fliege2014}{article}{}
      \name{author}{2}{}{%
        {{hash=c29438705766e23e4be7e0785b9b5f04}{%
           family={Fliege},
           familyi={F\bibinitperiod},
           given={J\"{o}rg},
           giveni={J\bibinitperiod}}}%
        {{hash=cfb14fca4467aa99d80d54da40c31d14}{%
           family={Werner},
           familyi={W\bibinitperiod},
           given={Ralf},
           giveni={R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {North-Holland}%
      }
      \strng{namehash}{6c41ad5b40f2312f30bf6dad64aa5e68}
      \strng{fullhash}{6c41ad5b40f2312f30bf6dad64aa5e68}
      \strng{bibnamehash}{6c41ad5b40f2312f30bf6dad64aa5e68}
      \strng{authorbibnamehash}{6c41ad5b40f2312f30bf6dad64aa5e68}
      \strng{authornamehash}{6c41ad5b40f2312f30bf6dad64aa5e68}
      \strng{authorfullhash}{6c41ad5b40f2312f30bf6dad64aa5e68}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Motivated by Markowitz portfolio optimization problems under uncertainty in the problem data, we consider general convex parametric multiobjective optimization problems under data uncertainty. For the first time, this uncertainty is treated by a robust multiobjective formulation in the gist of Ben-Tal and Nemirovski. For this novel formulation, we investigate its relationship to the original multiobjective formulation as well as to its scalarizations. Further, we provide a characterization of the location of the robust Pareto frontier with respect to the corresponding original Pareto frontier and show that standard techniques from multiobjective optimization can be employed to characterize this robust efficient frontier. We illustrate our results based on a standard mean-variance problem. {{$\copyright$}} 2013 Elsevier B.V. All rights reserved.}
      \field{issn}{03772217}
      \field{journaltitle}{European Journal of Operational Research}
      \field{month}{4}
      \field{number}{2}
      \field{title}{{Robust multiobjective optimization \& applications in portfolio optimization}}
      \field{volume}{234}
      \field{year}{2014}
      \field{pages}{422\bibrangedash 433}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1016/j.ejor.2013.10.028
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Fliege, Werner - 2014 - Robust multiobjective optimization & applications in portfolio optimization.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1016/j.ejor.2013.10.028
      \endverb
      \verb{url}
      \verb https://doi.org/10.1016/j.ejor.2013.10.028
      \endverb
      \keyw{Markowitz,Multiobjective,Robustification,Uncertainty}
    \endentry
    \entry{Fukuda2014}{article}{}
      \name{author}{2}{}{%
        {{hash=c5ce024500fe29b568503d51a3c9d530}{%
           family={Fukuda},
           familyi={F\bibinitperiod},
           given={Ellen\bibnamedelima Hidemi},
           giveni={E\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=79b9d0386837e14eb2455f224b15bec4}{%
           family={{Gra\~{n}a Drummond}},
           familyi={G\bibinitperiod},
           given={Luis\bibnamedelima Mauricio},
           giveni={L\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {SOBRAPO}%
      }
      \strng{namehash}{6426ccad8f899ff65f01d0d766d647da}
      \strng{fullhash}{6426ccad8f899ff65f01d0d766d647da}
      \strng{bibnamehash}{6426ccad8f899ff65f01d0d766d647da}
      \strng{authorbibnamehash}{6426ccad8f899ff65f01d0d766d647da}
      \strng{authornamehash}{6426ccad8f899ff65f01d0d766d647da}
      \strng{authorfullhash}{6426ccad8f899ff65f01d0d766d647da}
      \field{extraname}{1}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a rigorous and comprehensive survey on extensions to the multicriteria setting of three well-known scalar optimization algorithms. Multiobjective versions of the steepest descent, the projected gradient and the Newton methods are analyzed in detail. At each iteration, the search directions of these methods are computed by solving real-valued optimization problems and, in order to guarantee an adequate objective value decrease, Armijo-like rules are implemented by means of a backtracking procedure. Under standard assumptions, convergence to Pareto (weak Pareto) optima is established. For the Newton method, superlinear convergence is proved and, assuming Lipschitz continuity of the objectives second derivatives, it is shown that the rate is quadratic.}
      \field{issn}{0101-7438}
      \field{journaltitle}{Pesquisa Operacional}
      \field{month}{12}
      \field{number}{3}
      \field{title}{{A survey on multiobjective descemt methods}}
      \field{volume}{34}
      \field{year}{2014}
      \field{pages}{585\bibrangedash 620}
      \range{pages}{36}
      \verb{doi}
      \verb 10.1590/0101-7438.2014.034.03.0585
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Fukuda, Drummond - 2014 - A survey on multiobjective descent methods.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1590/0101-7438.2014.034.03.0585
      \endverb
      \verb{url}
      \verb https://doi.org/10.1590/0101-7438.2014.034.03.0585
      \endverb
      \keyw{Multiobjective optimization,Newton method,Nonlinear optimization,Projected gradient method,Steepest descent method}
    \endentry
    \entry{Fukuda2013}{article}{}
      \name{author}{2}{}{%
        {{hash=c5ce024500fe29b568503d51a3c9d530}{%
           family={Fukuda},
           familyi={F\bibinitperiod},
           given={Ellen\bibnamedelima Hidemi},
           giveni={E\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=79b9d0386837e14eb2455f224b15bec4}{%
           family={{Gra\~{n}a Drummond}},
           familyi={G\bibinitperiod},
           given={Luis\bibnamedelima Mauricio},
           giveni={L\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer US}%
      }
      \strng{namehash}{6426ccad8f899ff65f01d0d766d647da}
      \strng{fullhash}{6426ccad8f899ff65f01d0d766d647da}
      \strng{bibnamehash}{6426ccad8f899ff65f01d0d766d647da}
      \strng{authorbibnamehash}{6426ccad8f899ff65f01d0d766d647da}
      \strng{authornamehash}{6426ccad8f899ff65f01d0d766d647da}
      \strng{authorfullhash}{6426ccad8f899ff65f01d0d766d647da}
      \field{extraname}{2}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this work, we propose an inexact projected gradient-like method for solving smooth constrained vector optimization problems. In the unconstrained case, we retrieve the steepest descent method introduced by Gra\~{n}a Drummond and Svaiter. In the constrained setting, the method we present extends the exact one proposed by Gra\~{n}a Drummond and Iusem, since it admits relative errors on the search directions. At each iteration, a decrease of the objective value is obtained by means of an Armijo-like rule. The convergence results of this new method extend those obtained by Fukuda and Gra\~{n}a Drummond for the exact version. For partial orders induced by both pointed and nonpointed cones, under some reasonable hypotheses, global convergence to weakly efficient points of all sequences generated by the inexact projected gradient method is established for convex (respect to the ordering cone) objective functions. In the convergence analysis we also establish a connection between the so-called weighting method and the one we propose. {{$\copyright$}} 2012 Springer Science+Business Media, LLC.}
      \field{issn}{09266003}
      \field{journaltitle}{Computational Optimization and Applications}
      \field{month}{4}
      \field{number}{3}
      \field{title}{{Inexact projected gradient method for vector optimization}}
      \field{volume}{54}
      \field{year}{2013}
      \field{pages}{473\bibrangedash 493}
      \range{pages}{21}
      \verb{doi}
      \verb 10.1007/s10589-012-9501-z
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Fukuda, Gra{\~{n}}a Drummond - 2013 - Inexact projected gradient method for vector optimization.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://doi.org/10.1007/s10589-012-9501-z
      \endverb
      \verb{url}
      \verb http://doi.org/10.1007/s10589-012-9501-z
      \endverb
      \keyw{Multiobjective optimization,Projected gradient method,Vector optimization,Weak efficiency}
    \endentry
    \entry{Fukuda2011}{article}{}
      \name{author}{2}{}{%
        {{hash=c5ce024500fe29b568503d51a3c9d530}{%
           family={Fukuda},
           familyi={F\bibinitperiod},
           given={Ellen\bibnamedelima Hidemi},
           giveni={E\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=79b9d0386837e14eb2455f224b15bec4}{%
           family={{Gra\~{n}a Drummond}},
           familyi={G\bibinitperiod},
           given={Luis\bibnamedelima Mauricio},
           giveni={L\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \strng{namehash}{6426ccad8f899ff65f01d0d766d647da}
      \strng{fullhash}{6426ccad8f899ff65f01d0d766d647da}
      \strng{bibnamehash}{6426ccad8f899ff65f01d0d766d647da}
      \strng{authorbibnamehash}{6426ccad8f899ff65f01d0d766d647da}
      \strng{authornamehash}{6426ccad8f899ff65f01d0d766d647da}
      \strng{authorfullhash}{6426ccad8f899ff65f01d0d766d647da}
      \field{extraname}{3}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In 2004, Gra\~{n}a Drummond and Iusem proposed an extension of the projected gradient method for constrained vector optimization problems. Using this method, an Armijo-like rule, implemented with a backtracking procedure, was used in order to determine the step lengths. The authors just showed stationarity of all cluster points and, for another version of the algorithm (with exogenous step lengths), under some additional assumptions, they proved convergence to weakly efficient solutions. In this work, first we correct a slight mistake in the proof of a certain continuity result in that 2004 article, and then we extend its convergence analysis. Indeed, under some reasonable hypotheses, for convex objective functions with respect to the ordering cone, we establish full convergence to optimal points of any sequence produced by the projected gradient method with an Armijo-like rule, no matter how poor the initial guesses may be. {{$\copyright$}} 2011 Copyright Taylor and Francis Group, LLC.}
      \field{issn}{02331934}
      \field{journaltitle}{Optimization}
      \field{number}{8-9}
      \field{title}{{On the convergence of the projected gradient method for vector optimization}}
      \field{volume}{60}
      \field{year}{2011}
      \field{pages}{1009\bibrangedash 1021}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1080/02331934.2010.522710
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Fukuda, Gra{\~{n}}a Drummond - 2011 - On the convergence of the projected gradient method for vector optimization.pdf:pdf
      \endverb
      \keyw{convexity with respect to cones,projected gradient method,quasi-F\'{e}jer convergence,vector optimization,weak efficiency}
    \endentry
    \entry{Fukushima1992}{article}{}
      \name{author}{1}{}{%
        {{hash=2cedf89bdcdd2232e1b2b6fa56667310}{%
           family={Fukushima},
           familyi={F\bibinitperiod},
           given={Masao},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{2cedf89bdcdd2232e1b2b6fa56667310}
      \strng{fullhash}{2cedf89bdcdd2232e1b2b6fa56667310}
      \strng{bibnamehash}{2cedf89bdcdd2232e1b2b6fa56667310}
      \strng{authorbibnamehash}{2cedf89bdcdd2232e1b2b6fa56667310}
      \strng{authornamehash}{2cedf89bdcdd2232e1b2b6fa56667310}
      \strng{authorfullhash}{2cedf89bdcdd2232e1b2b6fa56667310}
      \field{extraname}{1}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Whether or not the general asymmetric variational inequality problem can be formulated as a differentiable optimization problem has been an open question. This paper gives an affirmative answer to this question. We provide a new optimization problem formulation of the variational inequality problem, and show that its objective function is continuously differentiable whenever the mapping involved in the latter problem is continuously differentiable. We also show that under appropriate assumptions on the latter mapping, any stationary point of the optimization problem is a global optimal solution, and hence solves the variational inequality problem. We discuss descent methods for solving the equivalent optimization problem and comment on systems of nonlinear equations and nonlinear complementarity problems.}
      \field{issn}{1436-4646}
      \field{journaltitle}{Mathematical Programming}
      \field{month}{1}
      \field{number}{1}
      \field{title}{{Equivalent differentiable optimization problems and descent methods for asymmetric variational inequality problems}}
      \field{volume}{53}
      \field{year}{1992}
      \field{pages}{99\bibrangedash 110}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1007/BF01585696
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Fukushima - 1992 - Equivalent differentiable optimization problems and descent methods for asymmetric variational inequality problems.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/BF01585696
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/BF01585696
      \endverb
      \keyw{Calculus of Variations and Optimal Control,Combinatorics,Mathematical Methods in Physics,Mathematical and Computational Physics,Mathematics of Computing,Numerical Analysis,Optimization,Theoretical}
    \endentry
    \entry{Fukushima1996}{article}{}
      \name{author}{1}{}{%
        {{hash=2cedf89bdcdd2232e1b2b6fa56667310}{%
           family={Fukushima},
           familyi={F\bibinitperiod},
           given={Masao},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{2cedf89bdcdd2232e1b2b6fa56667310}
      \strng{fullhash}{2cedf89bdcdd2232e1b2b6fa56667310}
      \strng{bibnamehash}{2cedf89bdcdd2232e1b2b6fa56667310}
      \strng{authorbibnamehash}{2cedf89bdcdd2232e1b2b6fa56667310}
      \strng{authornamehash}{2cedf89bdcdd2232e1b2b6fa56667310}
      \strng{authorfullhash}{2cedf89bdcdd2232e1b2b6fa56667310}
      \field{extraname}{2}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Nonlinear Optimization and Applications}
      \field{title}{{Merit functions for variational inequality and complementarity problems}}
      \field{volume}{1996}
      \field{year}{1996}
      \field{pages}{155\bibrangedash 170}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1007/978-1-4899-0289-4_11
      \endverb
      \verb{file}
      \verb :Users/zalgo/Downloads/Fukushima1996_Chapter_MeritFunctionsForVariationalIn.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-1-4899-0289-4_11
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-1-4899-0289-4_11
      \endverb
      \keyw{complementarity problem,gap function,merit function,optimization,problem,variational inequality problem}
    \endentry
    \entry{Fukushima1981}{article}{}
      \name{author}{2}{}{%
        {{hash=2cedf89bdcdd2232e1b2b6fa56667310}{%
           family={Fukushima},
           familyi={F\bibinitperiod},
           given={Masao},
           giveni={M\bibinitperiod}}}%
        {{hash=5f2bb7c93498291426292f81ac12c491}{%
           family={Mine},
           familyi={M\bibinitperiod},
           given={Hisashi},
           giveni={H\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Taylor & Francis Group}%
      }
      \strng{namehash}{31ef582dde2a5a8d6ca52d51d139f6fc}
      \strng{fullhash}{31ef582dde2a5a8d6ca52d51d139f6fc}
      \strng{bibnamehash}{31ef582dde2a5a8d6ca52d51d139f6fc}
      \strng{authorbibnamehash}{31ef582dde2a5a8d6ca52d51d139f6fc}
      \strng{authornamehash}{31ef582dde2a5a8d6ca52d51d139f6fc}
      \strng{authorfullhash}{31ef582dde2a5a8d6ca52d51d139f6fc}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{An algorithm is presented for minimizing a function which is the sum of a continuously differentiable function and a convex function. The class of such problems contains as a special case that of m...}
      \field{issn}{14645319}
      \field{journaltitle}{International Journal of Systems Science}
      \field{number}{8}
      \field{title}{{A generalized proximal point algorithm for certain non-convex minimization problems}}
      \field{volume}{12}
      \field{year}{1981}
      \field{pages}{989\bibrangedash 1000}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1080/00207728108963798
      \endverb
      \verb{urlraw}
      \verb https://www.tandfonline.com/doi/abs/10.1080/00207728108963798
      \endverb
      \verb{url}
      \verb https://www.tandfonline.com/doi/abs/10.1080/00207728108963798
      \endverb
    \endentry
    \entry{Gass1955}{article}{}
      \name{author}{2}{}{%
        {{hash=20f7982e0eeba3f37d8ae86da2dba031}{%
           family={Gass},
           familyi={G\bibinitperiod},
           given={Saul},
           giveni={S\bibinitperiod}}}%
        {{hash=5ac8572e99f9af01c3e6dafee076d3de}{%
           family={Saaty},
           familyi={S\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{4ff6402fb4aca0afbca276d9e21af954}
      \strng{fullhash}{4ff6402fb4aca0afbca276d9e21af954}
      \strng{bibnamehash}{4ff6402fb4aca0afbca276d9e21af954}
      \strng{authorbibnamehash}{4ff6402fb4aca0afbca276d9e21af954}
      \strng{authornamehash}{4ff6402fb4aca0afbca276d9e21af954}
      \strng{authorfullhash}{4ff6402fb4aca0afbca276d9e21af954}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{If a linear programming problem involves two objective functions, it is desirable to learn all solutions depending on the relative weight attached to the two functions. This paper presents details of an algorithm which finds these solutions systematically.}
      \field{issn}{00281441}
      \field{journaltitle}{Naval Research Logistics Quarterly}
      \field{number}{1-2}
      \field{title}{{The computational algorithm for the parametric objective function}}
      \field{volume}{2}
      \field{year}{1955}
      \field{pages}{39\bibrangedash 45}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1002/nav.3800020106
      \endverb
      \verb{file}
      \verb :Users/zalgo/Downloads/Naval Research Logistics Quarterly - March 1955 - Gass - The computational algorithm for the parametric objective function.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1002/nav.3800020106
      \endverb
      \verb{url}
      \verb https://doi.org/10.1002/nav.3800020106
      \endverb
    \endentry
    \entry{Geoffrion1968}{article}{}
      \name{author}{1}{}{%
        {{hash=f5627b1aba659b6648b11be6b5c23a73}{%
           family={Geoffrion},
           familyi={G\bibinitperiod},
           given={Arthur\bibnamedelima M.},
           giveni={A\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \strng{namehash}{f5627b1aba659b6648b11be6b5c23a73}
      \strng{fullhash}{f5627b1aba659b6648b11be6b5c23a73}
      \strng{bibnamehash}{f5627b1aba659b6648b11be6b5c23a73}
      \strng{authorbibnamehash}{f5627b1aba659b6648b11be6b5c23a73}
      \strng{authornamehash}{f5627b1aba659b6648b11be6b5c23a73}
      \strng{authorfullhash}{f5627b1aba659b6648b11be6b5c23a73}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0022247X}
      \field{journaltitle}{Journal of Mathematical Analysis and Applications}
      \field{month}{6}
      \field{number}{3}
      \field{title}{{Proper efficiency and the theory of vector maximization}}
      \field{volume}{22}
      \field{year}{1968}
      \field{pages}{618\bibrangedash 630}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1016/0022-247X(68)90201-1
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1016/0022-247X(68)90201-1
      \endverb
      \verb{url}
      \verb https://doi.org/10.1016/0022-247X(68)90201-1
      \endverb
    \endentry
    \entry{Goldstein1967}{article}{}
      \name{author}{1}{}{%
        {{hash=98596aa50f325610c1351cc0e72e28fd}{%
           family={Goldstein},
           familyi={G\bibinitperiod},
           given={Allen\bibnamedelima Abbey},
           giveni={A\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \list{publisher}{2}{%
        {Society for Industrial}%
        {Applied Mathematics}%
      }
      \strng{namehash}{98596aa50f325610c1351cc0e72e28fd}
      \strng{fullhash}{98596aa50f325610c1351cc0e72e28fd}
      \strng{bibnamehash}{98596aa50f325610c1351cc0e72e28fd}
      \strng{authorbibnamehash}{98596aa50f325610c1351cc0e72e28fd}
      \strng{authornamehash}{98596aa50f325610c1351cc0e72e28fd}
      \strng{authorfullhash}{98596aa50f325610c1351cc0e72e28fd}
      \field{extraname}{1}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0036-1445}
      \field{journaltitle}{SIAM Review}
      \field{month}{10}
      \field{number}{4}
      \field{title}{{Constructive Real Analysis}}
      \field{volume}{9}
      \field{year}{1967}
      \field{pages}{759\bibrangedash 759}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1137/1009133
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Goldstein - 1967 - Constructive real analysis.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1137/1009133
      \endverb
      \verb{url}
      \verb https://doi.org/10.1137/1009133
      \endverb
    \endentry
    \entry{Goldstein1964}{article}{}
      \name{author}{1}{}{%
        {{hash=98596aa50f325610c1351cc0e72e28fd}{%
           family={Goldstein},
           familyi={G\bibinitperiod},
           given={Allen\bibnamedelima Abbey},
           giveni={A\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \strng{namehash}{98596aa50f325610c1351cc0e72e28fd}
      \strng{fullhash}{98596aa50f325610c1351cc0e72e28fd}
      \strng{bibnamehash}{98596aa50f325610c1351cc0e72e28fd}
      \strng{authorbibnamehash}{98596aa50f325610c1351cc0e72e28fd}
      \strng{authornamehash}{98596aa50f325610c1351cc0e72e28fd}
      \strng{authorfullhash}{98596aa50f325610c1351cc0e72e28fd}
      \field{extraname}{2}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0273-0979}
      \field{journaltitle}{Bulletin of the American Mathematical Society}
      \field{number}{5}
      \field{title}{{Convex programming in Hilbert space}}
      \field{volume}{70}
      \field{year}{1964}
      \field{pages}{709\bibrangedash 710}
      \range{pages}{2}
      \verb{doi}
      \verb 10.1090/S0002-9904-1964-11178-2
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Goldstein - 1964 - Convex programming in Hilbert space.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1090/S0002-9904-1964-11178-2
      \endverb
      \verb{url}
      \verb https://doi.org/10.1090/S0002-9904-1964-11178-2
      \endverb
    \endentry
    \entry{Grana-Drummond2004}{article}{}
      \name{author}{2}{}{%
        {{hash=79b9d0386837e14eb2455f224b15bec4}{%
           family={{Gra\~{n}a Drummond}},
           familyi={G\bibinitperiod},
           given={Luis\bibnamedelima Mauricio},
           giveni={L\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=0eba3dfea4ed61f5a6bdfb2e6016fb1d}{%
           family={Iusem},
           familyi={I\bibinitperiod},
           given={Alfredo\bibnamedelima Noel},
           giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \strng{namehash}{ab6ae86491c795e5876ae0fa3b546e52}
      \strng{fullhash}{ab6ae86491c795e5876ae0fa3b546e52}
      \strng{bibnamehash}{ab6ae86491c795e5876ae0fa3b546e52}
      \strng{authorbibnamehash}{ab6ae86491c795e5876ae0fa3b546e52}
      \strng{authornamehash}{ab6ae86491c795e5876ae0fa3b546e52}
      \strng{authorfullhash}{ab6ae86491c795e5876ae0fa3b546e52}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Vector optimization problems are a significant extension of multiobjective optimization, which has a large number of real life applications. In vector optimization the preference order is related to an arbitrary closed and convex cone, rather than the nonnegative orthant. We consider extensions of the projected gradient gradient method to vector optimization, which work directly with vector-valued functions, without using scalar-valued objectives. We provide a direction which adequately substitutes for the projected gradient, and establish results which mirror those available for the scalar-valued case, namely stationarity of the cluster points (if any) without convexity assumptions, and convergence of the full sequence generated by the algorithm to a weakly efficient optimum in the convex case, under mild assumptions. We also prove that our results still hold when the search direction is only approximately computed.}
      \field{issn}{09266003}
      \field{journaltitle}{Computational Optimization and Applications}
      \field{month}{4}
      \field{number}{1}
      \field{title}{{A projected gradient method for vector optimization problems}}
      \field{volume}{28}
      \field{year}{2004}
      \field{pages}{5\bibrangedash 29}
      \range{pages}{25}
      \verb{doi}
      \verb 10.1023/B:COAP.0000018877.86161.8b
      \endverb
      \verb{file}
      \verb :Users/zalgo/Downloads/Drummond-Iusem2004_Article_AProjectedGradientMethodForVec.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://link.springer.com/10.1023/B:COAP.0000018877.86161.8b
      \endverb
      \verb{url}
      \verb http://link.springer.com/10.1023/B:COAP.0000018877.86161.8b
      \endverb
      \keyw{Convexity with respect to cones,Pareto optimality,Projected gradient methods,Quasi-Fej\'{e}r convergence,Weak efficiency}
    \endentry
    \entry{Grapiglia2015}{article}{}
      \name{author}{3}{}{%
        {{hash=7b7784907222d7f470bb6915398e2e3c}{%
           family={Grapiglia},
           familyi={G\bibinitperiod},
           given={Geovani\bibnamedelima N.},
           giveni={G\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=f5b74cc7eaccfaf91859eaa8d0aa8b53}{%
           family={Yuan},
           familyi={Y\bibinitperiod},
           given={Jinyun},
           giveni={J\bibinitperiod}}}%
        {{hash=f579eac1a41797a48b84021d664076c3}{%
           family={Yuan},
           familyi={Y\bibinitperiod},
           given={Ya-xiang},
           giveni={Y\bibinithyphendelim x\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer Verlag}%
      }
      \strng{namehash}{6675d19cf683e2e6d54801403f4c7222}
      \strng{fullhash}{6675d19cf683e2e6d54801403f4c7222}
      \strng{bibnamehash}{6675d19cf683e2e6d54801403f4c7222}
      \strng{authorbibnamehash}{6675d19cf683e2e6d54801403f4c7222}
      \strng{authornamehash}{6675d19cf683e2e6d54801403f4c7222}
      \strng{authorfullhash}{6675d19cf683e2e6d54801403f4c7222}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A nonlinear stepsize control framework for unconstrained optimization was recently proposed by Toint (Optim Methods Softw 28:82--95, 2013), providing a unified setting in which the global convergence can be proved for trust-region algorithms and regularization schemes. The original analysis assumes that the Hessians of the models are uniformly bounded. In this paper, the global convergence of the nonlinear stepsize control algorithm is proved under the assumption that the norm of the Hessians can grow by a constant amount at each iteration. The worst-case complexity is also investigated. The results obtained for unconstrained smooth optimization are extended to some algorithms for composite nonsmooth optimization and unconstrained multiobjective optimization as well.}
      \field{issn}{14364646}
      \field{journaltitle}{Mathematical Programming}
      \field{month}{8}
      \field{number}{1-2}
      \field{title}{{On the convergence and worst-case complexity of trust-region and regularization methods for unconstrained optimization}}
      \field{volume}{152}
      \field{year}{2015}
      \field{pages}{491\bibrangedash 520}
      \range{pages}{30}
      \verb{doi}
      \verb 10.1007/S10107-014-0794-9/TABLES/1
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Grapiglia, Yuan, Yuan - 2015 - On the convergence and worst-case complexity of trust-region and regularization methods for unconstrained.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://link.springer.com/article/10.1007/s10107-014-0794-9
      \endverb
      \verb{url}
      \verb https://link.springer.com/article/10.1007/s10107-014-0794-9
      \endverb
      \keyw{Composite nonsmooth optimization,Global convergence,Multiobjective optimization,Regularization methods,Trust-region methods,Unconstrained Optimization,Worst-case complexity}
    \endentry
    \entry{Hansen2006}{book}{}
      \name{author}{3}{}{%
        {{hash=f7cf8286cb9c038743a3eec9708f00d5}{%
           family={Hansen},
           familyi={H\bibinitperiod},
           given={Per\bibnamedelima Christian},
           giveni={P\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=3023776bfb4a237c1d7f0a61ca9c5d5f}{%
           family={Nagy},
           familyi={N\bibinitperiod},
           given={James\bibnamedelima G.},
           giveni={J\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=07d4991cd6561c5a5568e338fbbda090}{%
           family={O'Leary},
           familyi={O\bibinitperiod},
           given={Dianne\bibnamedelima P.},
           giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \list{publisher}{2}{%
        {Society for Industrial}%
        {Applied Mathematics}%
      }
      \strng{namehash}{cb736f6ca3bede592cae4b8f2cd0f6f9}
      \strng{fullhash}{cb736f6ca3bede592cae4b8f2cd0f6f9}
      \strng{bibnamehash}{cb736f6ca3bede592cae4b8f2cd0f6f9}
      \strng{authorbibnamehash}{cb736f6ca3bede592cae4b8f2cd0f6f9}
      \strng{authornamehash}{cb736f6ca3bede592cae4b8f2cd0f6f9}
      \strng{authorfullhash}{cb736f6ca3bede592cae4b8f2cd0f6f9}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This book is concerned with deconvolution methods for image deblurring, that is, compu- tational techniques for reconstruction of blurred images based on a concise mathematical model for the blurring process. The book describes the algorithms and techniques collec- tively known as spectral filtering methods, in which the singular value decomposition---or a similar decomposition with spectral properties---is used to introduce the necessary regu- larization or filtering in the reconstructed image. The main purpose of the book is to give students and engineers an understanding of the linear algebra behind the filtering methods. Readers in applied mathematics, numerical analysis, and computational science will be exposed to modern techniques to solve realistic large-scale problems in image deblurring.}
      \field{isbn}{978-0-89871-618-4}
      \field{month}{1}
      \field{title}{{Deblurring Images: Matrices, Spectra, and Filtering}}
      \field{year}{2006}
      \verb{doi}
      \verb 10.1137/1.9780898718874
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1137/1.9780898718874
      \endverb
      \verb{url}
      \verb https://doi.org/10.1137/1.9780898718874
      \endverb
    \endentry
    \entry{Hearn1982}{article}{}
      \name{author}{1}{}{%
        {{hash=f7c842b42ae405728f93032fb0b198e1}{%
           family={Hearn},
           familyi={H\bibinitperiod},
           given={Donald\bibnamedelima W.},
           giveni={D\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {North-Holland}%
      }
      \strng{namehash}{f7c842b42ae405728f93032fb0b198e1}
      \strng{fullhash}{f7c842b42ae405728f93032fb0b198e1}
      \strng{bibnamehash}{f7c842b42ae405728f93032fb0b198e1}
      \strng{authorbibnamehash}{f7c842b42ae405728f93032fb0b198e1}
      \strng{authornamehash}{f7c842b42ae405728f93032fb0b198e1}
      \strng{authorfullhash}{f7c842b42ae405728f93032fb0b198e1}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The gap function expresses the duality gap of a convex program as a function of the primal variables only. Differentiability and convexity properties are derived, and a convergent minimization algorithm is given. An example gives a simple one-variable interpretation of weak and strong duality. Application to user-equilibrium traffic assignment yields an appealing alternative optimization problem. {{$\copyright$}} 1982.}
      \field{issn}{0167-6377}
      \field{journaltitle}{Operations Research Letters}
      \field{month}{4}
      \field{number}{2}
      \field{title}{{The gap function of a convex program}}
      \field{volume}{1}
      \field{year}{1982}
      \field{pages}{67\bibrangedash 71}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1016/0167-6377(82)90049-9
      \endverb
      \verb{file}
      \verb :Users/zalgo/Downloads/1-s2.0-0167637782900499-main.pdf:pdf
      \endverb
      \keyw{Convex programming,duality,duality gap}
    \endentry
    \entry{Hoffman1952}{article}{}
      \name{author}{1}{}{%
        {{hash=ec929e099920301f79cf5c57d8dc8f53}{%
           family={Hoffman},
           familyi={H\bibinitperiod},
           given={Alan\bibnamedelima J},
           giveni={A\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{ec929e099920301f79cf5c57d8dc8f53}
      \strng{fullhash}{ec929e099920301f79cf5c57d8dc8f53}
      \strng{bibnamehash}{ec929e099920301f79cf5c57d8dc8f53}
      \strng{authorbibnamehash}{ec929e099920301f79cf5c57d8dc8f53}
      \strng{authornamehash}{ec929e099920301f79cf5c57d8dc8f53}
      \strng{authorfullhash}{ec929e099920301f79cf5c57d8dc8f53}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Let Ax^b be a consistent system of linear inequalities. The principal result is a quantitative formulation of the fact that if JC ''almost" satisfies the inequalities, then x is "close" to a solution. It is further shown how it is possible in certain cases to estimate the size of the vector joining JC to the nearest solution from the magnitude of the positive coordinates of Ax-b.}
      \field{journaltitle}{Journal of Research of the National Bureau of Standards}
      \field{number}{4}
      \field{title}{{On approximate solutions of systems of linear inequalities}}
      \field{volume}{49}
      \field{year}{1952}
      \field{pages}{263\bibrangedash 265}
      \range{pages}{3}
      \verb{doi}
      \verb 10.1142/9789812796936_0018
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Hoffman - 1952 - On Approximate Solutions of Systems of Linear Inequalities.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1142/9789812796936_0018
      \endverb
      \verb{url}
      \verb https://doi.org/10.1142/9789812796936_0018
      \endverb
    \endentry
    \entry{Huang2007}{article}{}
      \name{author}{3}{}{%
        {{hash=2689f5b80f6a3bd210e2b07c6af63632}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Nan\bibnamedelima Jing},
           giveni={N\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=bc20e54573d2db853852f86802ab83de}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
        {{hash=13fe5796c5051f169c118de5b1f5046b}{%
           family={Yao},
           familyi={Y\bibinitperiod},
           given={Jen-Chih},
           giveni={J\bibinithyphendelim C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Plenum Press PUB1205 New York, NY, USA}%
      }
      \strng{namehash}{6319d7f27d87b879b1569cf30be3d23c}
      \strng{fullhash}{6319d7f27d87b879b1569cf30be3d23c}
      \strng{bibnamehash}{6319d7f27d87b879b1569cf30be3d23c}
      \strng{authorbibnamehash}{6319d7f27d87b879b1569cf30be3d23c}
      \strng{authornamehash}{6319d7f27d87b879b1569cf30be3d23c}
      \strng{authorfullhash}{6319d7f27d87b879b1569cf30be3d23c}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, a gap function for a system of vector equilibrium problems is introduced and studied. Some necessary and sufficient conditions for the system of vector equilibrium problems are estab...}
      \field{issn}{00223239}
      \field{journaltitle}{Journal of Optimization Theory and Applications}
      \field{month}{5}
      \field{number}{2}
      \field{title}{{Gap functions and existence of solutions for a system of vector equilibrium problems}}
      \field{volume}{133}
      \field{year}{2007}
      \field{pages}{201\bibrangedash 212}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1007/S10957-007-9202-4
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Huang, Li, Yao - 2007 - Gap Functions and Existence of Solutions for a System of Vector Equilibrium Problems.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/abs/10.1007/s10957-007-9202-4
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/abs/10.1007/s10957-007-9202-4
      \endverb
      \keyw{Convex cones,Gap functions,Point-to-set mappings,Systems of vector equilibrium problems}
    \endentry
    \entry{Jin2001}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=c3da5be922b2509d740ae31f49229c23}{%
           family={Jin},
           familyi={J\bibinitperiod},
           given={Yaochu},
           giveni={Y\bibinitperiod}}}%
        {{hash=1090caf76158f01efad195d51c0f77fb}{%
           family={Olhofer},
           familyi={O\bibinitperiod},
           given={Markus},
           giveni={M\bibinitperiod}}}%
        {{hash=2bf4fe0d55970e86270a528542b5093d}{%
           family={Sendhoff},
           familyi={S\bibinitperiod},
           given={Bernhard},
           giveni={B\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {San Francisco, CA, USA}%
      }
      \list{publisher}{1}{%
        {Morgan Kaufmann Publishers Inc.}%
      }
      \strng{namehash}{5b9d24070b628cf4d0c161b3bf6edfc9}
      \strng{fullhash}{5b9d24070b628cf4d0c161b3bf6edfc9}
      \strng{bibnamehash}{5b9d24070b628cf4d0c161b3bf6edfc9}
      \strng{authorbibnamehash}{5b9d24070b628cf4d0c161b3bf6edfc9}
      \strng{authornamehash}{5b9d24070b628cf4d0c161b3bf6edfc9}
      \strng{authorfullhash}{5b9d24070b628cf4d0c161b3bf6edfc9}
      \field{sortinit}{J}
      \field{sortinithash}{b2f54a9081ace9966a7cb9413811edb4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Evolutionary Dynamic Weighted Aggregation (EDWA) has shown to be both effective and computationally efficient [1] for multi-objective optimization (MOO). Besides, it was also found empirically and surprisingly that EDWA was able to deal with multi-objective optimization problems with a concave Pareto front, which has proved to be beyond the capability of the Conventional Weighted Aggregation (CWA) methods [2]. In this paper, a theory on why CWA fails for multi-objective problems with a concave Pareto front is provided schematically. According to this theory, it can easily be explained why EDWA has worked well for both convex and concave multi-objective problems. Simulation examples are conducted on various test functions to support our theory. It is concluded that EDWA is an effective and efficient method for solving multi-objective optimization problems.}
      \field{booktitle}{Proceedings of the 3rd Annual Conference on Genetic and Evolutionary Computation}
      \field{isbn}{1558607749}
      \field{series}{GECCO'01}
      \field{title}{{Dynamic weighted aggregation for evolutionary multi-objective optimization: Why does it work and how?}}
      \field{year}{2001}
      \field{pages}{1042\bibrangedash 1049}
      \range{pages}{8}
      \verb{doi}
      \verb 10.5555/2955239.2955427
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.5555/2955239.2955427
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.5555/2955239.2955427
      \endverb
    \endentry
    \entry{Kanzow1996}{article}{}
      \name{author}{1}{}{%
        {{hash=97d2e434223bd2d37723183cbed5e8df}{%
           family={Kanzow},
           familyi={K\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{97d2e434223bd2d37723183cbed5e8df}
      \strng{fullhash}{97d2e434223bd2d37723183cbed5e8df}
      \strng{bibnamehash}{97d2e434223bd2d37723183cbed5e8df}
      \strng{authorbibnamehash}{97d2e434223bd2d37723183cbed5e8df}
      \strng{authornamehash}{97d2e434223bd2d37723183cbed5e8df}
      \strng{authorfullhash}{97d2e434223bd2d37723183cbed5e8df}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Several methods for solving the nonlinear complementarity problem (NCP) are developed. These methods are generalizations of the recently proposed algorithms of Mangasarian and Solodov (Ref. 1) and are based on an unconstrianed minimization formulation of the nonlinear complementarity problem. It is shown that, under certain assumptions, any stationary point of the unconstrained objective function is already a solution of NCP. In particulr, these assumptions are satisfied by the mangasarian and Soolodov implicit Lagranian functioin. Furthermore, a special Newton-type method is suggested, and conditions for its local quadratic convergence are given. Finally, some preliminary numerical results are presented.}
      \field{issn}{1573-2878}
      \field{journaltitle}{Journal of Optimization Theory and Applications}
      \field{number}{1}
      \field{title}{{Nonlinear complementarity as unconstrained optimization}}
      \field{volume}{88}
      \field{year}{1996}
      \field{pages}{139\bibrangedash 155}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1007/BF02192026
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Kanzow - 1996 - Nonlinear complementarity as unconstrained optimization.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://link.springer.com/article/10.1007/BF02192026
      \endverb
      \verb{url}
      \verb https://link.springer.com/article/10.1007/BF02192026
      \endverb
      \keyw{Applications of Mathematics,Calculus of Variations and Optimal Control,Engineering,Operations Research/Decision Theory,Optimization,Theory of Computation,general}
    \endentry
    \entry{Karimi2016}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=795002bfdebff4c8ba1aa396c148c625}{%
           family={Karimi},
           familyi={K\bibinitperiod},
           given={Hamed},
           giveni={H\bibinitperiod}}}%
        {{hash=ef2efe010316e17a0f001b9311e73625}{%
           family={Nutini},
           familyi={N\bibinitperiod},
           given={Julie},
           giveni={J\bibinitperiod}}}%
        {{hash=6ec4971d3a33801f19547675e0e66e5e}{%
           family={Schmidt},
           familyi={S\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
      }
      \name{editor}{4}{}{%
        {{hash=f247df2d5de3a9ccb24aa9ee9fdaf277}{%
           family={Frasconi},
           familyi={F\bibinitperiod},
           given={Paolo},
           giveni={P\bibinitperiod}}}%
        {{hash=1b0055e0bc807afc8d575a28cd653fe3}{%
           family={Landwehr},
           familyi={L\bibinitperiod},
           given={Niels},
           giveni={N\bibinitperiod}}}%
        {{hash=3ac8629f73a346e92784afca7539530c}{%
           family={Manco},
           familyi={M\bibinitperiod},
           given={Giuseppe},
           giveni={G\bibinitperiod}}}%
        {{hash=80a7ecb0fd29b36581e59ff5137f9b34}{%
           family={Vreeken},
           familyi={V\bibinitperiod},
           given={Jilles},
           giveni={J\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{f9b50842a92c29bf17a02b29b25c753a}
      \strng{fullhash}{f9b50842a92c29bf17a02b29b25c753a}
      \strng{bibnamehash}{f9b50842a92c29bf17a02b29b25c753a}
      \strng{authorbibnamehash}{f9b50842a92c29bf17a02b29b25c753a}
      \strng{authornamehash}{f9b50842a92c29bf17a02b29b25c753a}
      \strng{authorfullhash}{f9b50842a92c29bf17a02b29b25c753a}
      \strng{editorbibnamehash}{058fcd0ed57fc69d93d9900cf52317eb}
      \strng{editornamehash}{058fcd0ed57fc69d93d9900cf52317eb}
      \strng{editorfullhash}{058fcd0ed57fc69d93d9900cf52317eb}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In 1963, Polyak proposed a simple condition that is sufficient to show a global linear convergence rate for gradient descent. This condition is a special case of the {\L{}}ojasiewicz inequality proposed in the same year, and it does not require strong convexity (or even convexity). In this work, we show that this much-older Polyak-{\L{}}ojasiewicz (PL) inequality is actually weaker than the main conditions that have been explored to show linear convergence rates without strong convexity over the last 25 years. We also use the PL inequality to give new analyses of coordinate descent and stochastic gradient for many non-strongly-convex (and some non-convex) functions. We further propose a generalization that applies to proximal-gradient methods for non-smooth optimization, leading to simple proofs of linear convergence for support vector machines and L1- regularized least squares without additional assumptions.}
      \field{booktitle}{Machine Learning and Knowledge Discovery in Databases}
      \field{eprinttype}{arXiv}
      \field{isbn}{978-3-319-46128-1}
      \field{title}{{Linear convergence of gradient and proximal-gradient methods under the Polyak-{\L{}}ojasiewicz condition}}
      \field{year}{2016}
      \field{pages}{795\bibrangedash 811}
      \range{pages}{17}
      \verb{doi}
      \verb 10.1007/978-3-319-46128-1_50
      \endverb
      \verb{eprint}
      \verb 1608.04636
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Karimi, Nutini, Schmidt - 2016 - Linear convergence of gradient and proximal-gradient methods under the Polyak-{\L}ojasiewicz condition.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-3-319-46128-1_50
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-3-319-46128-1_50
      \endverb
      \keyw{Boosting,Coordinate descent,Gradient descent,L1-regularization,Stochastic gradient,Support vector machines,Variance-reduction}
    \endentry
    \entry{Kim2017}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=75d63de41193da0696437dce887df612}{%
           family={Kim},
           familyi={K\bibinitperiod},
           given={Ye-Hoon},
           giveni={Y\bibinithyphendelim H\bibinitperiod}}}%
        {{hash=54471468074d6ba4cc7458e4f3d3272d}{%
           family={Reddy},
           familyi={R\bibinitperiod},
           given={Bhargava},
           giveni={B\bibinitperiod}}}%
        {{hash=94b45d5d2664ddecbbcd8f9b03710ff3}{%
           family={Yun},
           familyi={Y\bibinitperiod},
           given={Sojung},
           giveni={S\bibinitperiod}}}%
        {{hash=92b59afe37160fa86a509b9a1c5e7907}{%
           family={Seo},
           familyi={S\bibinitperiod},
           given={Chanwon},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{fd6c2de389fc6208c9c8ccbd442f7ada}
      \strng{fullhash}{fd6c2de389fc6208c9c8ccbd442f7ada}
      \strng{bibnamehash}{fd6c2de389fc6208c9c8ccbd442f7ada}
      \strng{authorbibnamehash}{fd6c2de389fc6208c9c8ccbd442f7ada}
      \strng{authornamehash}{fd6c2de389fc6208c9c8ccbd442f7ada}
      \strng{authorfullhash}{fd6c2de389fc6208c9c8ccbd442f7ada}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{ICML'17 AutoML Workshop}
      \field{title}{{NEMO: Neuro-evolution with multiobjective optimization of deep neural network for speed and accuracy}}
      \field{year}{2017}
      \verb{urlraw}
      \verb https://www.semanticscholar.org/paper/0a9c6947a0b6f79526e537cb83925ef60df674e8
      \endverb
      \verb{url}
      \verb https://www.semanticscholar.org/paper/0a9c6947a0b6f79526e537cb83925ef60df674e8
      \endverb
    \endentry
    \entry{Konnov2005}{article}{}
      \name{author}{1}{}{%
        {{hash=7f3d445b30894fb8de4727ea7b1c8668}{%
           family={Konnov},
           familyi={K\bibinitperiod},
           given={Igor\bibnamedelima V.},
           giveni={I\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{7f3d445b30894fb8de4727ea7b1c8668}
      \strng{fullhash}{7f3d445b30894fb8de4727ea7b1c8668}
      \strng{bibnamehash}{7f3d445b30894fb8de4727ea7b1c8668}
      \strng{authorbibnamehash}{7f3d445b30894fb8de4727ea7b1c8668}
      \strng{authornamehash}{7f3d445b30894fb8de4727ea7b1c8668}
      \strng{authorfullhash}{7f3d445b30894fb8de4727ea7b1c8668}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider an approach to convert vector variational inequalities into an equivalent scalar variational inequality problem with a set-valued cost mapping. Being based on this property, we give an equivalence result between weak and strong solutions of set-valued vector variational inequalities and suggest a new gap function for vector variational inequalities. Additional examples of applications in vector optimization, vector network equilibrium and vector migration equilibrium problems are also given}
      \field{issn}{1573-2916}
      \field{journaltitle}{Journal of Global Optimization}
      \field{month}{8}
      \field{number}{4}
      \field{title}{{A Scalarization Approach for Vector Variational Inequalities with Applications}}
      \field{volume}{32}
      \field{year}{2005}
      \field{pages}{517\bibrangedash 527}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1007/S10898-003-2688-X
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Konnov - 2005 - A Scalarization Approach for Vector Variational Inequalities with Applications.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://link.springer.com/article/10.1007/s10898-003-2688-x
      \endverb
      \verb{url}
      \verb https://link.springer.com/article/10.1007/s10898-003-2688-x
      \endverb
      \keyw{Computer Science,Operations Research/Decision Theory,Optimization,Real Functions,general}
    \endentry
    \entry{Li2007}{article}{}
      \name{author}{2}{}{%
        {{hash=bc20e54573d2db853852f86802ab83de}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
        {{hash=2689f5b80f6a3bd210e2b07c6af63632}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Nan\bibnamedelima Jing},
           giveni={N\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{b7cde363f2955dda55992a5bbeda51af}
      \strng{fullhash}{b7cde363f2955dda55992a5bbeda51af}
      \strng{bibnamehash}{b7cde363f2955dda55992a5bbeda51af}
      \strng{authorbibnamehash}{b7cde363f2955dda55992a5bbeda51af}
      \strng{authornamehash}{b7cde363f2955dda55992a5bbeda51af}
      \strng{authorfullhash}{b7cde363f2955dda55992a5bbeda51af}
      \field{extraname}{1}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, the notion of gap functions is extended from scalar case to vector one. Then, gap functions and generalized functions for several kinds of vector equilibrium problems are shown. As an application, the dual problem of a class of optimization problems with a system of vector equilibrium constraints (in short, OP) is established, the concavity of the dual function, the weak duality of (OP) and the saddle point sufficient condition are derived by using generalized gap functions.}
      \field{issn}{1573-2916}
      \field{journaltitle}{Journal of Global Optimization}
      \field{month}{2}
      \field{number}{2}
      \field{title}{{An extension of gap functions for a system of vector equilibrium problems with applications to optimization problems}}
      \field{volume}{39}
      \field{year}{2007}
      \field{pages}{247\bibrangedash 260}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1007/S10898-007-9137-1
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Li, Huang - 2007 - An extension of gap functions for a system of vector equilibrium problems with applications to optimization problems.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://link.springer.com/article/10.1007/s10898-007-9137-1
      \endverb
      \verb{url}
      \verb https://link.springer.com/article/10.1007/s10898-007-9137-1
      \endverb
      \keyw{Computer Science,Operations Research/Decision Theory,Optimization,Real Functions,Subject,The,Vector equilibrium problem {\textperiodcentered{}},Weak duality,Weak duality 2000,general}
    \endentry
    \entry{Li2005}{article}{}
      \name{author}{2}{}{%
        {{hash=bc20e54573d2db853852f86802ab83de}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
        {{hash=2689f5b80f6a3bd210e2b07c6af63632}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Nan\bibnamedelima Jing},
           giveni={N\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{b7cde363f2955dda55992a5bbeda51af}
      \strng{fullhash}{b7cde363f2955dda55992a5bbeda51af}
      \strng{bibnamehash}{b7cde363f2955dda55992a5bbeda51af}
      \strng{authorbibnamehash}{b7cde363f2955dda55992a5bbeda51af}
      \strng{authornamehash}{b7cde363f2955dda55992a5bbeda51af}
      \strng{authorfullhash}{b7cde363f2955dda55992a5bbeda51af}
      \field{extraname}{2}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The purpose of this paper is to introduce a nonlinear scalarisation function for solving a class of implicit vector equilibrium problems. We prove a scalarisation lemma to show the relation between the implicit vector equilibrium problem and the nonlinear scalarisation function. Then we derive some new existence theorems for solutions of implicit vector equilibrium problems, using the scalarisation lemma and the FKKM theorem.}
      \field{issn}{1755-1633}
      \field{journaltitle}{Bulletin of the Australian Mathematical Society}
      \field{number}{1}
      \field{title}{{Implicit vector equilibrium problems via nonlinear scalarisation}}
      \field{volume}{72}
      \field{year}{2005}
      \field{pages}{161\bibrangedash 172}
      \range{pages}{12}
      \verb{doi}
      \verb 10.1017/S000497270003495X
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Li, Huang - 2005 - Implicit vector equilibrium problems via nonlinear scalarisation.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1017/S000497270003495X
      \endverb
      \verb{url}
      \verb https://doi.org/10.1017/S000497270003495X
      \endverb
    \endentry
    \entry{Li2010}{article}{}
      \name{author}{2}{}{%
        {{hash=bc20e54573d2db853852f86802ab83de}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Jun},
           giveni={J\bibinitperiod}}}%
        {{hash=6f7203fe3dbae0d75271d282dcbda955}{%
           family={Mastroeni},
           familyi={M\bibinitperiod},
           given={Giandomenico},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{e85fda39efadad2d661865468801267a}
      \strng{fullhash}{e85fda39efadad2d661865468801267a}
      \strng{bibnamehash}{e85fda39efadad2d661865468801267a}
      \strng{authorbibnamehash}{e85fda39efadad2d661865468801267a}
      \strng{authornamehash}{e85fda39efadad2d661865468801267a}
      \strng{authorfullhash}{e85fda39efadad2d661865468801267a}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, by using the scalarization approach of Konnov, several kinds of strong and weak scalar variational inequalities (SVI and WVI) are introduced for studying strong and weak vector variational inequalities (SVVI and WVVI) with set-valued mappings, and their gap functions are suggested. The equivalence among SVVI, WVVI, SVI, WVI is then established under suitable conditions and the relations among their gap functions are analyzed. These results are finally applied to the error bounds for gap functions. Some existence theorems of global error bounds for gap functions are obtained under strong monotonicity and several characterizations of global (respectively local) error bounds for the gap functions are derived.}
      \field{issn}{00223239}
      \field{journaltitle}{Journal of Optimization Theory and Applications}
      \field{month}{5}
      \field{number}{2}
      \field{title}{{Vector variational inequalities involving set-valued mappings via scalarization with applications to error bounds for gap functions}}
      \field{volume}{145}
      \field{year}{2010}
      \field{pages}{355\bibrangedash 372}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1007/S10957-009-9625-1
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Li, Mastroeni - 2010 - Vector Variational Inequalities Involving Set-valued Mappings via Scalarization with Applications to Error Bounds.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/S10957-009-9625-1
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/S10957-009-9625-1
      \endverb
      \keyw{Error bounds,Gap functions,Scalarization,Set-valued mappings,Vector variational inequalities}
    \endentry
    \entry{Li2006}{article}{}
      \name{author}{4}{}{%
        {{hash=5f56bb7d7006631a83969794336cc7a4}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={S.\bibnamedelimi J.},
           giveni={S\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=bb3f41c78783dfe31a6afed630501b35}{%
           family={Teo},
           familyi={T\bibinitperiod},
           given={Kok\bibnamedelima Lay},
           giveni={K\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=ed6f1cd802fbd72f9d2ba807fe42c15c}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Xiao\bibnamedelima Qi},
           giveni={X\bibinitperiod\bibinitdelim Q\bibinitperiod}}}%
        {{hash=af271e7ff654e38002c54d6a3ae59c16}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={S.\bibnamedelimi Y.},
           giveni={S\bibinitperiod\bibinitdelim Y\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{99f3c418eb03c70b7c26299c0a809b3b}
      \strng{fullhash}{99f3c418eb03c70b7c26299c0a809b3b}
      \strng{bibnamehash}{99f3c418eb03c70b7c26299c0a809b3b}
      \strng{authorbibnamehash}{99f3c418eb03c70b7c26299c0a809b3b}
      \strng{authornamehash}{99f3c418eb03c70b7c26299c0a809b3b}
      \strng{authorfullhash}{99f3c418eb03c70b7c26299c0a809b3b}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper deals with generalized vector quasi-equilibrium problems. By virtue of a nonlinear scalarization function, the gap functions for two classes of generalized vector quasi-equilibrium problems are obtained. Then, from an existence theorem for a generalized quasi-equilibrium problem and a minimax inequality, existence theorems for two classes of generalized vector quasi-equilibrium problems are established.}
      \field{issn}{1573-2916}
      \field{journaltitle}{Journal of Global Optimization}
      \field{month}{3}
      \field{number}{3}
      \field{title}{{Gap Functions and Existence of Solutions to Generalized Vector Quasi-Equilibrium Problems}}
      \field{volume}{34}
      \field{year}{2006}
      \field{pages}{427\bibrangedash 440}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1007/S10898-005-2193-5
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2006 - Gap Functions and Existence of Solutions to Generalized Vector Quasi-Equilibrium Problems.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://link.springer.com/article/10.1007/s10898-005-2193-5
      \endverb
      \verb{url}
      \verb https://link.springer.com/article/10.1007/s10898-005-2193-5
      \endverb
      \keyw{Computer Science,Operations Research/Decision Theory,Optimization,Real Functions,general}
    \endentry
    \entry{Lin2019}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=948727393ebbef74ecbe6a26e05dec18}{%
           family={Lin},
           familyi={L\bibinitperiod},
           given={Xi},
           giveni={X\bibinitperiod}}}%
        {{hash=f01a5b8b3e3813c0945ebd644d711ddc}{%
           family={Zhen},
           familyi={Z\bibinitperiod},
           given={Hui-Ling},
           giveni={H\bibinithyphendelim L\bibinitperiod}}}%
        {{hash=f8e0d654897337665dd7b950ebb4a88d}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Zhenhua},
           giveni={Z\bibinitperiod}}}%
        {{hash=b2e22db866fa79372f07034ed0140d0e}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Qingfu},
           giveni={Q\bibinitperiod}}}%
        {{hash=a8de5a3e513bd670ca4f49686487a8d2}{%
           family={Kwong},
           familyi={K\bibinitperiod},
           given={Sam},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{8afcd9ade4b9730634ec1ef58b12da4c}
      \strng{fullhash}{8afcd9ade4b9730634ec1ef58b12da4c}
      \strng{bibnamehash}{8afcd9ade4b9730634ec1ef58b12da4c}
      \strng{authorbibnamehash}{8afcd9ade4b9730634ec1ef58b12da4c}
      \strng{authornamehash}{8afcd9ade4b9730634ec1ef58b12da4c}
      \strng{authorfullhash}{8afcd9ade4b9730634ec1ef58b12da4c}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multi-task learning is a powerful method for solving multiple correlated tasks simultaneously. However, it is often impossible to find one single solution to optimize all the tasks, since different tasks might conflict with each other. Recently, a novel method is proposed to find one single Pareto optimal solution with good trade-off among different tasks by casting multi-task learning as multiobjective optimization. In this paper, we generalize this idea and propose a novel Pareto multi-task learning algorithm (Pareto MTL) to find a set of well-distributed Pareto solutions which can represent different trade-offs among different tasks. The proposed algorithm first formulates a multi-task learning problem as a multiobjective optimization problem, and then decomposes the multiobjective optimization problem into a set of constrained subproblems with different trade-off preferences. By solving these subproblems in parallel, Pareto MTL can find a set of well-representative Pareto optimal solutions with different trade-off among all tasks. Practitioners can easily select their preferred solution from these Pareto solutions, or use different trade-off solutions for different situations. Experimental results confirm that the proposed algorithm can generate well-representative solutions and outperform some state-of-the-art algorithms on many multi-task learning applications.}
      \field{booktitle}{NIPS'19: Proceedings of the 33rd International Conference on Neural Information Processing Systems}
      \field{month}{12}
      \field{title}{{Pareto multi-Task learning}}
      \field{year}{2019}
      \field{pages}{12060\bibrangedash 12070}
      \range{pages}{11}
      \verb{doi}
      \verb 10.5555/3454287.3455367
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Lin et al. - 2019 - Pareto multi-Task learning.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.5555/3454287.3455367
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.5555/3454287.3455367
      \endverb
    \endentry
    \entry{Liu2009}{article}{}
      \name{author}{3}{}{%
        {{hash=d11c38c8b86974ee0313721c6e55b962}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={C.\bibnamedelimi G.},
           giveni={C\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=de991865948ccc725ce8318c4092070a}{%
           family={Ng},
           familyi={N\bibinitperiod},
           given={K.\bibnamedelimi F.},
           giveni={K\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
        {{hash=d8a55e473aad5d133aa48880c78e1f55}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={W.\bibnamedelimi H.},
           giveni={W\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer-Verlag}%
      }
      \strng{namehash}{8849c17050afd0a6e79b65cfe6adf58e}
      \strng{fullhash}{8849c17050afd0a6e79b65cfe6adf58e}
      \strng{bibnamehash}{8849c17050afd0a6e79b65cfe6adf58e}
      \strng{authorbibnamehash}{8849c17050afd0a6e79b65cfe6adf58e}
      \strng{authornamehash}{8849c17050afd0a6e79b65cfe6adf58e}
      \strng{authorfullhash}{8849c17050afd0a6e79b65cfe6adf58e}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study the weak domination property and weakly efficient solutions in vector optimization problems. In particular scalarization of these problems is obtained by virtue of some suitable merit functions. Some natural conditions to ensure the existence of error bounds for merit functions are also given. {{$\copyright$}} 2008 Springer-Verlag.}
      \field{issn}{00255610}
      \field{journaltitle}{Mathematical Programming}
      \field{month}{7}
      \field{number}{2}
      \field{title}{{Merit functions in vector optimization}}
      \field{volume}{119}
      \field{year}{2009}
      \field{pages}{215\bibrangedash 237}
      \range{pages}{23}
      \verb{doi}
      \verb 10.1007/s10107-008-0208-y
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Ng, Yang - 2009 - Merit functions in vector optimization.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://doi.org/10.1007/s10107-008-0208-y
      \endverb
      \verb{url}
      \verb http://doi.org/10.1007/s10107-008-0208-y
      \endverb
      \keyw{Error bound,Merit function,Weak domination property,Weakly efficient solution}
    \endentry
    \entry{Mangasarian1993}{article}{}
      \name{author}{2}{}{%
        {{hash=1630fca73a93198e4bfa3687bb8b3de3}{%
           family={Mangasarian},
           familyi={M\bibinitperiod},
           given={Olvi\bibnamedelima L.},
           giveni={O\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=4a98f1e2b7ad6a8742ce834f972d7ccd}{%
           family={Solodov},
           familyi={S\bibinitperiod},
           given={Mikhail\bibnamedelima V.},
           giveni={M\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer-Verlag}%
      }
      \strng{namehash}{2584432f1c0c1b0834f7688adcbac5b6}
      \strng{fullhash}{2584432f1c0c1b0834f7688adcbac5b6}
      \strng{bibnamehash}{2584432f1c0c1b0834f7688adcbac5b6}
      \strng{authorbibnamehash}{2584432f1c0c1b0834f7688adcbac5b6}
      \strng{authornamehash}{2584432f1c0c1b0834f7688adcbac5b6}
      \strng{authorfullhash}{2584432f1c0c1b0834f7688adcbac5b6}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The nonlinear complementarity problem is cast as an unconstrained minimization problem that is obtained from an augmented Lagrangian formulation. The dimensionality of the unconstrained problem is the same as that of the original problem, and the penalty parameter need only be greater than one. Another feature of the unconstrained problem is that it has global minima of zero at precisely all the solution points of the complementarity problem without any monotonicity assumption. If the mapping of the complementarity problem is differentiable, then so is the objective of the unconstrained problem, and its gradient vanishes at all solution points of the complementarity problem. Under assumptions of nondegeneracy and linear independence of gradients of active constraints at a complementarity problem solution, the corresponding global unconstrained minimum point is locally unique. A Wolfe dual to a standard constrained optimization problem associated with the nonlinear complementarity problem is also formulated under a monotonicity and differentiability assumption. Most of the standard duality results are established even though the underlying constrained optimization problem may be nonconvex. Preliminary numerical tests on two small nonmonotone problems from the published literature converged to degenerate or nondegenerate solutions from all attempted starting points in 7 to 28 steps of a BFGS quasi-Newton method for unconstrained optimization. {{$\copyright$}} 1993 The Mathematical Programming Society, Inc.}
      \field{issn}{00255610}
      \field{journaltitle}{Mathematical Programming}
      \field{month}{2}
      \field{number}{1-3}
      \field{title}{{Nonlinear complementarity as unconstrained and constrained minimization}}
      \field{volume}{62}
      \field{year}{1993}
      \field{pages}{277\bibrangedash 297}
      \range{pages}{21}
      \verb{doi}
      \verb 10.1007/BF01585171
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Mangasarian, Solodov - 1993 - Nonlinear complementarity as unconstrained and constrained minimization.pdf:pdf
      \endverb
    \endentry
    \entry{Mastroeni2003}{article}{}
      \name{author}{1}{}{%
        {{hash=6f7203fe3dbae0d75271d282dcbda955}{%
           family={Mastroeni},
           familyi={M\bibinitperiod},
           given={Giandomenico},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{6f7203fe3dbae0d75271d282dcbda955}
      \strng{fullhash}{6f7203fe3dbae0d75271d282dcbda955}
      \strng{bibnamehash}{6f7203fe3dbae0d75271d282dcbda955}
      \strng{authorbibnamehash}{6f7203fe3dbae0d75271d282dcbda955}
      \strng{authornamehash}{6f7203fe3dbae0d75271d282dcbda955}
      \strng{authorfullhash}{6f7203fe3dbae0d75271d282dcbda955}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The theory of gap functions, developed in the literature for variational inequalities, is extended to a general equilibrium problem. Descent methods, with exact an inexact line-search rules, are proposed. It is shown that these methods are a generalization of the gap function algorithms for variational inequalities and optimization problems.}
      \field{issn}{1573-2916}
      \field{journaltitle}{Journal of Global Optimization}
      \field{month}{12}
      \field{number}{4}
      \field{title}{{Gap Functions for Equilibrium Problems}}
      \field{volume}{27}
      \field{year}{2003}
      \field{pages}{411\bibrangedash 426}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1023/A:1026050425030
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Mastroeni - 2003 - Gap Functions for Equilibrium Problems.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1023/A:1026050425030
      \endverb
      \verb{url}
      \verb https://doi.org/10.1023/A:1026050425030
      \endverb
      \keyw{Computer Science,Operations Research/Decision Theory,Optimization,Real Functions,general}
    \endentry
    \entry{McCormick1969}{article}{}
      \name{author}{1}{}{%
        {{hash=1b934b23948d917b693196887d6b904d}{%
           family={McCormick},
           familyi={M\bibinitperiod},
           given={Garth\bibnamedelima Philip},
           giveni={G\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \strng{namehash}{1b934b23948d917b693196887d6b904d}
      \strng{fullhash}{1b934b23948d917b693196887d6b904d}
      \strng{bibnamehash}{1b934b23948d917b693196887d6b904d}
      \strng{authorbibnamehash}{1b934b23948d917b693196887d6b904d}
      \strng{authornamehash}{1b934b23948d917b693196887d6b904d}
      \strng{authorfullhash}{1b934b23948d917b693196887d6b904d}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Management Science}
      \field{number}{5}
      \field{title}{{Anti-zig-zagging by bending}}
      \field{volume}{15}
      \field{year}{1969}
      \field{pages}{315\bibrangedash 320}
      \range{pages}{6}
      \verb{file}
      \verb :Users/zalgo/Downloads/2628138.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://www.jstor.org/stable/2628138
      \endverb
      \verb{url}
      \verb https://www.jstor.org/stable/2628138
      \endverb
    \endentry
    \entry{Miettinen1998}{book}{}
      \name{author}{1}{}{%
        {{hash=20376f7658e6889e42babc850cadbf3c}{%
           family={Miettinen},
           familyi={M\bibinitperiod},
           given={Kaisa.},
           giveni={K\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer US}%
      }
      \strng{namehash}{20376f7658e6889e42babc850cadbf3c}
      \strng{fullhash}{20376f7658e6889e42babc850cadbf3c}
      \strng{bibnamehash}{20376f7658e6889e42babc850cadbf3c}
      \strng{authorbibnamehash}{20376f7658e6889e42babc850cadbf3c}
      \strng{authornamehash}{20376f7658e6889e42babc850cadbf3c}
      \strng{authorfullhash}{20376f7658e6889e42babc850cadbf3c}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Problems with multiple objectives and criteria are generally known as multiple criteria optimization or multiple criteria decision-making (MCDM) problems. So far, these types of problems have typically been modelled and solved by means of linear programming. However, many real-life phenomena are of a nonlinear nature, which is why we need tools for nonlinear programming capable of handling several conflicting or incommensurable objectives. In this case, methods of traditional single objective optimization and linear programming are not enough; we need new ways of thinking, new concepts, and new methods - nonlinear multiobjective optimization. Nonlinear Multiobjective Optimization provides an extensive, up-to-date, self-contained and consistent survey, review of the literature and of the state of the art on nonlinear (deterministic) multiobjective optimization, its methods, its theory and its background. The amount of literature on multiobjective optimization is immense. The treatment in this book is based on approximately 1500 publications in English printed mainly after the year 1980. Problems related to real-life applications often contain irregularities and nonsmoothnesses. The treatment of nondifferentiable multiobjective optimization in the literature is rather rare. For this reason, this book contains material about the possibilities, background, theory and methods of nondifferentiable multiobjective optimization as well. This book is intended for both researchers and students in the areas of (applied) mathematics, engineering, economics, operations research and management science; it is meant for both professionals and practitioners in many different fields of application. The intention has been to provide a consistent summary that may help in selecting an appropriate method for the problem to be solved. It is hoped the extensive bibliography will be of value to researchers. I Terminology And Theory -- 2. Concepts -- 3. Theoretical Background -- II Methods -- 2. No-Preference Methods -- 3. A Posteriori Methods -- 4. A Priori Methods -- 5. Interactive Methods -- III Related Issues -- 1. Comparing Methods -- 2. Software -- 3. Graphical Illustration -- 4. Future Directions -- 5. Epilogue -- References.}
      \field{isbn}{9781461375449}
      \field{title}{{Nonlinear Multiobjective Optimization}}
      \field{year}{1998}
      \field{pages}{298}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1007/978-1-4615-5563-6
      \endverb
      \verb{file}
      \verb :Users/zalgo/Downloads/bok_978-1-4615-5563-6.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-1-4615-5563-6
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-1-4615-5563-6
      \endverb
    \endentry
    \entry{Morishita2016}{thesis}{}
      \name{author}{1}{}{%
        {{hash=10779d1215935640b383d9a15886e6df}{%
           family={Morishita},
           familyi={M\bibinitperiod},
           given={Miko},
           giveni={M\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {Kyoto University}%
      }
      \strng{namehash}{10779d1215935640b383d9a15886e6df}
      \strng{fullhash}{10779d1215935640b383d9a15886e6df}
      \strng{bibnamehash}{10779d1215935640b383d9a15886e6df}
      \strng{authorbibnamehash}{10779d1215935640b383d9a15886e6df}
      \strng{authornamehash}{10779d1215935640b383d9a15886e6df}
      \strng{authorfullhash}{10779d1215935640b383d9a15886e6df}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{A descent method for robust multiobjective optimization in the presence of implementation errors}}
      \field{type}{Master}
      \field{year}{2016}
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Morishita - 2016 - A descent method for robust multiobjective optimization in the presence of implementation errors.pdf:pdf
      \endverb
    \endentry
    \entry{Nesterov1983}{article}{}
      \name{author}{1}{}{%
        {{hash=8a41b35fe7b3d1725cb95bd7eec40b01}{%
           family={Nesterov},
           familyi={N\bibinitperiod},
           given={Yurii},
           giveni={Y\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {Russian}%
      }
      \strng{namehash}{8a41b35fe7b3d1725cb95bd7eec40b01}
      \strng{fullhash}{8a41b35fe7b3d1725cb95bd7eec40b01}
      \strng{bibnamehash}{8a41b35fe7b3d1725cb95bd7eec40b01}
      \strng{authorbibnamehash}{8a41b35fe7b3d1725cb95bd7eec40b01}
      \strng{authornamehash}{8a41b35fe7b3d1725cb95bd7eec40b01}
      \strng{authorfullhash}{8a41b35fe7b3d1725cb95bd7eec40b01}
      \field{sortinit}{N}
      \field{sortinithash}{22369a73d5f88983a108b63f07f37084}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Page 1. . . Soviel Math Doki. 269 < I9K3), 3 Vol 27 19 , No. 2 OF WITH 0l/k2) UDC il YU. E. NESTEROV}
      \field{annotation}{(In Russian)}
      \field{journaltitle}{Doklady Akademii Nauk SSSR}
      \field{title}{{A method for solving the convex programming problem with convergence rate $O(1/k^2)$}}
      \field{volume}{269}
      \field{year}{1983}
      \field{pages}{543\bibrangedash 547}
      \range{pages}{5}
      \verb{urlraw}
      \verb http://mi.mathnet.ru/eng/dan/v269/i3/p543
      \endverb
      \verb{url}
      \verb http://mi.mathnet.ru/eng/dan/v269/i3/p543
      \endverb
    \endentry
    \entry{Parikh2014}{book}{}
      \name{author}{2}{}{%
        {{hash=2e1b520f90d2b5de02de6cf5f9896399}{%
           family={Parikh},
           familyi={P\bibinitperiod},
           given={Neal},
           giveni={N\bibinitperiod}}}%
        {{hash=ec94d94cc487dd71939a90cbeaaf47d0}{%
           family={Boyd},
           familyi={B\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Boston - Delft}%
      }
      \list{publisher}{1}{%
        {Now Publishers, Inc.}%
      }
      \strng{namehash}{05c18423218cfff8f407fe111419ae33}
      \strng{fullhash}{05c18423218cfff8f407fe111419ae33}
      \strng{bibnamehash}{05c18423218cfff8f407fe111419ae33}
      \strng{authorbibnamehash}{05c18423218cfff8f407fe111419ae33}
      \strng{authornamehash}{05c18423218cfff8f407fe111419ae33}
      \strng{authorfullhash}{05c18423218cfff8f407fe111419ae33}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Proximal Algorithms}
      \field{booktitle}{Foundations and Trends{\textregistered{}} in Optimization}
      \field{isbn}{9781601987174}
      \field{issn}{2167-3888}
      \field{number}{3}
      \field{title}{{Proximal Algorithms}}
      \field{volume}{1}
      \field{year}{2014}
      \field{pages}{127\bibrangedash 239}
      \range{pages}{113}
      \verb{doi}
      \verb 10.1561/2400000003
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Parikh, Boyd - 2014 - Proximal Algorithms.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1561/2400000003
      \endverb
      \verb{url}
      \verb https://doi.org/10.1561/2400000003
      \endverb
      \keyw{Machine Learning,Optimization}
    \endentry
    \entry{Polyak1967}{article}{}
      \name{author}{1}{}{%
        {{hash=2bad1be7cd84eab3968cd6c52a0fd906}{%
           family={Polyak},
           familyi={P\bibinitperiod},
           given={Boris\bibnamedelima Teodorovich},
           giveni={B\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
      }
      \strng{namehash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \strng{fullhash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \strng{bibnamehash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \strng{authorbibnamehash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \strng{authornamehash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \strng{authorfullhash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \field{extraname}{1}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Doklady Akademii Nauk SSSR}
      \field{number}{1}
      \field{title}{{A general method for solving extremal problems}}
      \field{volume}{174}
      \field{year}{1967}
      \field{pages}{33\bibrangedash 36}
      \range{pages}{4}
      \verb{urlraw}
      \verb http://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=dan&paperid=33049&option_lang=eng
      \endverb
      \verb{url}
      \verb http://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=dan&paperid=33049&option_lang=eng
      \endverb
    \endentry
    \entry{Polyak1963}{article}{}
      \name{author}{1}{}{%
        {{hash=2bad1be7cd84eab3968cd6c52a0fd906}{%
           family={Polyak},
           familyi={P\bibinitperiod},
           given={Boris\bibnamedelima Teodorovich},
           giveni={B\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
      }
      \strng{namehash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \strng{fullhash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \strng{bibnamehash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \strng{authorbibnamehash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \strng{authornamehash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \strng{authorfullhash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \field{extraname}{2}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Let tf(t) be a functional defined in the (real) Hubert space H. The problem consists in finding its minimum value tff* = inf tf(x) and some minimum point x* (if such exists). {{$\copyright$}} 1962.}
      \field{issn}{00415553}
      \field{journaltitle}{USSR Computational Mathematics and Mathematical Physics}
      \field{month}{1}
      \field{number}{4}
      \field{title}{{Gradient methods for minimizing functionals}}
      \field{volume}{3}
      \field{year}{1963}
      \field{pages}{864\bibrangedash 878}
      \range{pages}{15}
      \verb{doi}
      \verb 10.1016/0041-5553(63)90382-3
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1016/0041-5553(63)90382-3
      \endverb
      \verb{url}
      \verb https://doi.org/10.1016/0041-5553(63)90382-3
      \endverb
    \endentry
    \entry{Polyak1969}{article}{}
      \name{author}{1}{}{%
        {{hash=2bad1be7cd84eab3968cd6c52a0fd906}{%
           family={Polyak},
           familyi={P\bibinitperiod},
           given={Boris\bibnamedelima Teodorovich},
           giveni={B\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {No longer published by Elsevier}%
      }
      \strng{namehash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \strng{fullhash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \strng{bibnamehash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \strng{authorbibnamehash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \strng{authornamehash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \strng{authorfullhash}{2bad1be7cd84eab3968cd6c52a0fd906}
      \field{extraname}{3}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{WE consider the minimization of a convex, but but necessarily differentiable, functional in a convex set of Hilbert space. The minimization method amounts to movement along a reference functional. The step length is evaluated here, not assigned; all we require for its evaluation is a knowledge of the minimum value of the functional. Under some natural assumptions, this method proves to be convergent both for smooth and non-differentiable functionals, at the rate of a geometric progression. We show how the method may be modified if the minimum of the functional is unknown. We also consider another minimization method, whereby the convergence rate can be increased considerably; this is based on approximation of the functional close to its minimum by a piecewise linear functional. Finally, we quote examples of problems to which our methods are applicable, and examine the computational aspects of the methods. {{$\copyright$}} 1971.}
      \field{issn}{0041-5553}
      \field{journaltitle}{USSR Computational Mathematics and Mathematical Physics}
      \field{month}{1}
      \field{number}{3}
      \field{title}{{Minimization of unsmooth functionals}}
      \field{volume}{9}
      \field{year}{1969}
      \field{pages}{14\bibrangedash 29}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1016/0041-5553(69)90061-5
      \endverb
    \endentry
    \entry{Rockafellar1998}{book}{}
      \name{author}{2}{}{%
        {{hash=18de24308c36b3348b1477a2f7aa442f}{%
           family={Rockafellar},
           familyi={R\bibinitperiod},
           given={R.\bibnamedelimi Tyrrell},
           giveni={R\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=804ae762a20092c80078ef061d74e6c3}{%
           family={Wets},
           familyi={W\bibinitperiod},
           given={Roger\bibnamedelimb J.\bibnamedelimi B.},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer Berlin Heidelberg}%
      }
      \strng{namehash}{8165449b565446abe2899184551c5587}
      \strng{fullhash}{8165449b565446abe2899184551c5587}
      \strng{bibnamehash}{8165449b565446abe2899184551c5587}
      \strng{authorbibnamehash}{8165449b565446abe2899184551c5587}
      \strng{authornamehash}{8165449b565446abe2899184551c5587}
      \strng{authorfullhash}{8165449b565446abe2899184551c5587}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{978-3-540-62772-2}
      \field{series}{Grundlehren der mathematischen Wissenschaften}
      \field{title}{{Variational Analysis}}
      \field{volume}{317}
      \field{year}{1998}
      \verb{doi}
      \verb 10.1007/978-3-642-02431-3
      \endverb
      \verb{file}
      \verb :Users/zalgo/Downloads/Rockafellar-Wets1998_Book_VariationalAnalysis.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://link.springer.com/10.1007/978-3-642-02431-3
      \endverb
      \verb{url}
      \verb http://link.springer.com/10.1007/978-3-642-02431-3
      \endverb
    \endentry
    \entry{Scheinberg2014}{article}{}
      \name{author}{3}{}{%
        {{hash=227fd756890273896e67d801c215790f}{%
           family={Scheinberg},
           familyi={S\bibinitperiod},
           given={Katya},
           giveni={K\bibinitperiod}}}%
        {{hash=5bf0f5ae463d891111f759554ade71fa}{%
           family={Goldfarb},
           familyi={G\bibinitperiod},
           given={Donald},
           giveni={D\bibinitperiod}}}%
        {{hash=fcaf8aecffe1495e6d6934a9e2109173}{%
           family={Bai},
           familyi={B\bibinitperiod},
           given={Xi},
           giveni={X\bibinitperiod}}}%
      }
      \strng{namehash}{3b343050cd2cc837408ce9dbaf6dcfa0}
      \strng{fullhash}{3b343050cd2cc837408ce9dbaf6dcfa0}
      \strng{bibnamehash}{3b343050cd2cc837408ce9dbaf6dcfa0}
      \strng{authorbibnamehash}{3b343050cd2cc837408ce9dbaf6dcfa0}
      \strng{authornamehash}{3b343050cd2cc837408ce9dbaf6dcfa0}
      \strng{authorfullhash}{3b343050cd2cc837408ce9dbaf6dcfa0}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{1615-3375}
      \field{journaltitle}{Foundations of Computational Mathematics}
      \field{month}{6}
      \field{number}{3}
      \field{title}{{Fast first-order methods for composite convex optimization with backtracking}}
      \field{volume}{14}
      \field{year}{2014}
      \field{pages}{389\bibrangedash 417}
      \range{pages}{29}
      \verb{doi}
      \verb 10.1007/s10208-014-9189-9
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10208-014-9189-9
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10208-014-9189-9
      \endverb
    \endentry
    \entry{Sener2018}{inproceedings}{}
      \name{author}{1}{}{%
        {{hash=549735d3e9dd65c976e6eb0a084fe121}{%
           family={Sener},
           familyi={S\bibinitperiod},
           given={Ozan},
           giveni={O\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Red Hook, NY, USA}%
      }
      \list{publisher}{1}{%
        {Curran Associates Inc.}%
      }
      \strng{namehash}{549735d3e9dd65c976e6eb0a084fe121}
      \strng{fullhash}{549735d3e9dd65c976e6eb0a084fe121}
      \strng{bibnamehash}{549735d3e9dd65c976e6eb0a084fe121}
      \strng{authorbibnamehash}{549735d3e9dd65c976e6eb0a084fe121}
      \strng{authornamehash}{549735d3e9dd65c976e6eb0a084fe121}
      \strng{authorfullhash}{549735d3e9dd65c976e6eb0a084fe121}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.}
      \field{booktitle}{Proceedings of the 32nd International Conference on Neural Information Processing Systems}
      \field{title}{{Multi-task learning as multi-objective optimization}}
      \field{year}{2018}
      \field{pages}{525\bibrangedash 536}
      \range{pages}{12}
      \verb{doi}
      \verb 10.5555/3326943.3326992
      \endverb
      \verb{file}
      \verb :Users/zalgo/Downloads/1810.04650.pdf:pdf;:Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Sener - Unknown - Multi-Task Learning as Multi-Objective Optimization.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://dl.acm.org/doi/10.5555/3326943.3326992
      \endverb
      \verb{url}
      \verb https://dl.acm.org/doi/10.5555/3326943.3326992
      \endverb
    \endentry
    \entry{Shor1985}{book}{}
      \name{author}{1}{}{%
        {{hash=def0935ac20c39ef5be973673b228276}{%
           family={Shor},
           familyi={S\bibinitperiod},
           given={Naum\bibnamedelima Zuselevich},
           giveni={N\bibinitperiod\bibinitdelim Z\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin, Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer Berlin Heidelberg}%
      }
      \strng{namehash}{def0935ac20c39ef5be973673b228276}
      \strng{fullhash}{def0935ac20c39ef5be973673b228276}
      \strng{bibnamehash}{def0935ac20c39ef5be973673b228276}
      \strng{authorbibnamehash}{def0935ac20c39ef5be973673b228276}
      \strng{authornamehash}{def0935ac20c39ef5be973673b228276}
      \strng{authorfullhash}{def0935ac20c39ef5be973673b228276}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{les auteurs : Alexander, Lisa Allen, Simon Bindoff, Nathaniel L. Br\'{e}on, Fran\c{c}ois-Marie Church, John Cubasch, Ulrich Emori, Seita Forster, Piers Friedlingstein, Pierre Gillett, Nathan Gregory, Jonathan Hartmann, Dennis Jansen, Eystein Kirtman, Ben Knutti, Reto Kanikicharla, Krishna Kumar Lemke, Peter Marotzke, Jochem Masson-Delmotte, Val\'{e}rie Meehl, Gerald Mokhov, Igor Piao, Shilong Plattner, Gian-Kasper Dahe, Qin Ramaswamy, Venkatachalam Randall, David Rhein, Monika Rojas, Maisa Sabine, Christopher Shindell, Drew Stocker, Thomas F. Talley, Lynne Vaughan, David Xie, Shang-Ping}
      \field{edition}{1}
      \field{isbn}{978-3-642-82120-2}
      \field{series}{Springer Series in Computational Mathematics}
      \field{title}{{Minimization Methods for Non-Differentiable Functions}}
      \field{volume}{3}
      \field{year}{1985}
      \field{pages}{1\bibrangedash 36}
      \range{pages}{36}
      \verb{doi}
      \verb 10.1007/978-3-642-82118-9
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-3-642-82118-9
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-3-642-82118-9
      \endverb
      \keyw{icle}
    \endentry
    \entry{Sion1958}{article}{}
      \name{author}{1}{}{%
        {{hash=2224c2451350b84fa5cda65cd0b23565}{%
           family={Sion},
           familyi={S\bibinitperiod},
           given={Maurice},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{2224c2451350b84fa5cda65cd0b23565}
      \strng{fullhash}{2224c2451350b84fa5cda65cd0b23565}
      \strng{bibnamehash}{2224c2451350b84fa5cda65cd0b23565}
      \strng{authorbibnamehash}{2224c2451350b84fa5cda65cd0b23565}
      \strng{authornamehash}{2224c2451350b84fa5cda65cd0b23565}
      \strng{authorfullhash}{2224c2451350b84fa5cda65cd0b23565}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0030-8730}
      \field{journaltitle}{Pacific Journal of Mathematics}
      \field{month}{3}
      \field{number}{1}
      \field{title}{{On general minimax theorems}}
      \field{volume}{8}
      \field{year}{1958}
      \field{pages}{171\bibrangedash 176}
      \range{pages}{6}
      \verb{doi}
      \verb 10.2140/pjm.1958.8.171
      \endverb
      \verb{file}
      \verb :Users/zalgo/Downloads/pjm-v8-n1-p14-p.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://doi.org/10.2140/pjm.1958.8.171
      \endverb
      \verb{url}
      \verb http://doi.org/10.2140/pjm.1958.8.171
      \endverb
    \endentry
    \entry{Sturm1999}{article}{}
      \name{author}{1}{}{%
        {{hash=fe1ced4ee77685732ac1a8aa62e7d6ac}{%
           family={Sturm},
           familyi={S\bibinitperiod},
           given={Jos\bibnamedelima F.},
           giveni={J\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Taylor & Francis}%
      }
      \strng{namehash}{fe1ced4ee77685732ac1a8aa62e7d6ac}
      \strng{fullhash}{fe1ced4ee77685732ac1a8aa62e7d6ac}
      \strng{bibnamehash}{fe1ced4ee77685732ac1a8aa62e7d6ac}
      \strng{authorbibnamehash}{fe1ced4ee77685732ac1a8aa62e7d6ac}
      \strng{authornamehash}{fe1ced4ee77685732ac1a8aa62e7d6ac}
      \strng{authorfullhash}{fe1ced4ee77685732ac1a8aa62e7d6ac}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{SeDuMi is an add-on for MATLAB, which lets you solve optimization problems with linear, quadratic and semidefiniteness constraints. It is possible to have complex valued data and variables in SeDuMi. Moreover, large scale optimization problems are solved efficiently, by exploiting sparsity. This paper describes how to work with this toolbox.}
      \field{issn}{10556788}
      \field{journaltitle}{Optimization Methods and Software}
      \field{number}{1}
      \field{title}{{Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones}}
      \field{volume}{11}
      \field{year}{1999}
      \field{pages}{625\bibrangedash 653}
      \range{pages}{29}
      \verb{doi}
      \verb 10.1080/10556789908805766
      \endverb
    \endentry
    \entry{Su2016}{article}{}
      \name{author}{3}{}{%
        {{hash=023e479291de1262454c46e62e12f856}{%
           family={Su},
           familyi={S\bibinitperiod},
           given={Weijie},
           giveni={W\bibinitperiod}}}%
        {{hash=ec94d94cc487dd71939a90cbeaaf47d0}{%
           family={Boyd},
           familyi={B\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
        {{hash=ae404faac8c8429af059d0185dcc9643}{%
           family={Cand\`{e}s},
           familyi={C\bibinitperiod},
           given={Emmanuel\bibnamedelima J.},
           giveni={E\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{b80a7ea3e696a4194f8530b88861b534}
      \strng{fullhash}{b80a7ea3e696a4194f8530b88861b534}
      \strng{bibnamehash}{b80a7ea3e696a4194f8530b88861b534}
      \strng{authorbibnamehash}{b80a7ea3e696a4194f8530b88861b534}
      \strng{authornamehash}{b80a7ea3e696a4194f8530b88861b534}
      \strng{authorfullhash}{b80a7ea3e696a4194f8530b88861b534}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We derive a second-order ordinary differential equation (ODE) which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{number}{153}
      \field{title}{{A differential equation for modeling {N}esterov's accelerated gradient method: theory and insights}}
      \field{volume}{17}
      \field{year}{2016}
      \field{pages}{1\bibrangedash 43}
      \range{pages}{43}
      \verb{file}
      \verb :Users/zalgo/Downloads/15-084.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://jmlr.org/papers/v17/15-084.html
      \endverb
      \verb{url}
      \verb https://jmlr.org/papers/v17/15-084.html
      \endverb
    \endentry
    \entry{Sun2019}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=727938c60e783da449b4314fcefc4b67}{%
           family={Sun},
           familyi={S\bibinitperiod},
           given={Yifan},
           giveni={Y\bibinitperiod}}}%
        {{hash=adecffa53b3f98f59e2880222285a3af}{%
           family={Jeong},
           familyi={J\bibinitperiod},
           given={Halyun},
           giveni={H\bibinitperiod}}}%
        {{hash=ef2efe010316e17a0f001b9311e73625}{%
           family={Nutini},
           familyi={N\bibinitperiod},
           given={Julie},
           giveni={J\bibinitperiod}}}%
        {{hash=6ec4971d3a33801f19547675e0e66e5e}{%
           family={Schmidt},
           familyi={S\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{2be0282508da3ae29be32ef91ae62573}
      \strng{fullhash}{2be0282508da3ae29be32ef91ae62573}
      \strng{bibnamehash}{2be0282508da3ae29be32ef91ae62573}
      \strng{authorbibnamehash}{2be0282508da3ae29be32ef91ae62573}
      \strng{authornamehash}{2be0282508da3ae29be32ef91ae62573}
      \strng{authorfullhash}{2be0282508da3ae29be32ef91ae62573}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, PMLR}
      \field{title}{{Are we there yet? Manifold identification of gradient-related proximal methods}}
      \field{year}{2019}
      \field{pages}{1110\bibrangedash 1119}
      \range{pages}{10}
      \verb{urlraw}
      \verb http://proceedings.mlr.press/v89/sun19a.html
      \endverb
      \verb{url}
      \verb http://proceedings.mlr.press/v89/sun19a.html
      \endverb
    \endentry
    \entry{Taji1993}{article}{}
      \name{author}{3}{}{%
        {{hash=95d0371d22ec6c347dad0e7bb2369cee}{%
           family={Taji},
           familyi={T\bibinitperiod},
           given={Kouichi},
           giveni={K\bibinitperiod}}}%
        {{hash=2cedf89bdcdd2232e1b2b6fa56667310}{%
           family={Fukushima},
           familyi={F\bibinitperiod},
           given={Masao},
           giveni={M\bibinitperiod}}}%
        {{hash=23f86eab33aa66835c47244a2f6224b2}{%
           family={Ibaraki},
           familyi={I\bibinitperiod},
           given={Toshihide},
           giveni={T\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{724ad959227b64129741877a139e2ae0}
      \strng{fullhash}{724ad959227b64129741877a139e2ae0}
      \strng{bibnamehash}{724ad959227b64129741877a139e2ae0}
      \strng{authorbibnamehash}{724ad959227b64129741877a139e2ae0}
      \strng{authornamehash}{724ad959227b64129741877a139e2ae0}
      \strng{authorfullhash}{724ad959227b64129741877a139e2ae0}
      \field{sortinit}{T}
      \field{sortinithash}{9af77f0292593c26bde9a56e688eaee9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Variational inequality problems have been used to formulate and study equilibrium problems, which arise in many fields including economics, operations research and regional sciences. For solving variational inequality problems, various iterative methods such as projection methods and the nonlinear Jacobi method have been developed. These methods are convergent to a solution under certain conditions, but their rates of convergence are typically linear. In this paper we propose to modify the Newton method for variational inequality problems by using a certain differentiable merit function to determine a suitable step length. The purpose of introducing this merit function is to provide some measure of the discrepancy between the solution and the current iterate. It is then shown that, under the strong monotonicity assumption, the method is globally convergent and, under some additional assumptions, the rate of convergence is quadratic. Limited computational experience indicates the high efficiency of the proposed method.}
      \field{issn}{1436-4646}
      \field{journaltitle}{Mathematical Programming 1993 58:1}
      \field{month}{1}
      \field{number}{1}
      \field{title}{{A globally convergent Newton method for solving strongly monotone variational inequalities}}
      \field{volume}{58}
      \field{year}{1993}
      \field{pages}{369\bibrangedash 383}
      \range{pages}{15}
      \verb{doi}
      \verb 10.1007/BF01581276
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Taji, Fukushima, Ibaraki - 1993 - A globally convergent Newton method for solving strongly monotone variational inequalities.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/BF01581276
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/BF01581276
      \endverb
      \keyw{Calculus of Variations and Optimal Control,Combinatorics,Mathematical Methods in Physics,Mathematical and Computational Physics,Mathematics of Computing,Numerical Analysis,Optimization,Theoretical}
    \endentry
    \entry{Tanabe2022a}{misc}{}
      \name{author}{3}{}{%
        {{hash=fe79ff046b379ee499d9981349434f9c}{%
           family={Tanabe},
           familyi={T\bibinitperiod},
           given={Hiroki},
           giveni={H\bibinitperiod}}}%
        {{hash=c5ce024500fe29b568503d51a3c9d530}{%
           family={Fukuda},
           familyi={F\bibinitperiod},
           given={Ellen\bibnamedelima Hidemi},
           giveni={E\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=e93d47fe50ff15742fad700a20e29a10}{%
           family={Yamashita},
           familyi={Y\bibinitperiod},
           given={Nobuo},
           giveni={N\bibinitperiod}}}%
      }
      \strng{namehash}{50829e7ec3808f1d9fd656ad5bc088e7}
      \strng{fullhash}{50829e7ec3808f1d9fd656ad5bc088e7}
      \strng{bibnamehash}{50829e7ec3808f1d9fd656ad5bc088e7}
      \strng{authorbibnamehash}{50829e7ec3808f1d9fd656ad5bc088e7}
      \strng{authornamehash}{50829e7ec3808f1d9fd656ad5bc088e7}
      \strng{authorfullhash}{50829e7ec3808f1d9fd656ad5bc088e7}
      \field{sortinit}{T}
      \field{sortinithash}{9af77f0292593c26bde9a56e688eaee9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many descent methods for multiobjective optimization problems have been developed in recent years. In 2000, the steepest descent method was proposed for differentiable multiobjective optimization problems. Afterward, the proximal gradient method, which can solve composite problems, was also considered. However, the accelerated versions are not sufficiently studied. In this paper, we propose a multiobjective accelerated proximal gradient algorithm, in which we solve subproblems with terms that only appear in the multiobjective case. We also show the proposed method's global convergence rate ($O(1/k^2)$) under reasonable assumptions, using a merit function to measure the complexity. Moreover, we present an efficient way to solve the subproblem via its dual, and we confirm the validity of the proposed method through preliminary numerical experiments.}
      \field{eprinttype}{arXiv}
      \field{howpublished}{arXiv:2202.10994}
      \field{month}{2}
      \field{title}{{An accelerated proximal gradient method for multiobjective optimization}}
      \field{year}{2022}
      \verb{eprint}
      \verb 2202.10994
      \endverb
      \verb{file}
      \verb :Users/zalgo/Downloads/2202.10994.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.48550/arXiv.2202.10994
      \endverb
      \verb{url}
      \verb https://doi.org/10.48550/arXiv.2202.10994
      \endverb
    \endentry
    \entry{Villa2013}{article}{}
      \name{author}{4}{}{%
        {{hash=4f3c23dd29010763e330fac99e111eb5}{%
           family={Villa},
           familyi={V\bibinitperiod},
           given={Silvia},
           giveni={S\bibinitperiod}}}%
        {{hash=810f538b5dc1ef52eb54d3d08054a1b8}{%
           family={Salzo},
           familyi={S\bibinitperiod},
           given={Saverio},
           giveni={S\bibinitperiod}}}%
        {{hash=fe6aac8417b0807f7b01c4f4678e452e}{%
           family={Baldassarre},
           familyi={B\bibinitperiod},
           given={Luca},
           giveni={L\bibinitperiod}}}%
        {{hash=e56db0c2247cc015feff1be538dd96ef}{%
           family={Verri},
           familyi={V\bibinitperiod},
           given={Alessandro},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{2}{%
        {Society for Industrial}%
        {Applied Mathematics}%
      }
      \strng{namehash}{2007a842e2d60286e393e5b8b4eadd03}
      \strng{fullhash}{2007a842e2d60286e393e5b8b4eadd03}
      \strng{bibnamehash}{2007a842e2d60286e393e5b8b4eadd03}
      \strng{authorbibnamehash}{2007a842e2d60286e393e5b8b4eadd03}
      \strng{authornamehash}{2007a842e2d60286e393e5b8b4eadd03}
      \strng{authorfullhash}{2007a842e2d60286e393e5b8b4eadd03}
      \field{sortinit}{V}
      \field{sortinithash}{afb52128e5b4dc4b843768c0113d673b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose a convergence analysis of accelerated forward-backward splitting methods for composite function minimization, when the proximity operator is not available in closed form, and can only be...}
      \field{issn}{10526234}
      \field{journaltitle}{SIAM Journal on Optimization}
      \field{month}{8}
      \field{number}{3}
      \field{title}{{Accelerated and inexact forward-backward algorithms}}
      \field{volume}{23}
      \field{year}{2013}
      \field{pages}{1607\bibrangedash 1633}
      \range{pages}{27}
      \verb{doi}
      \verb 10.1137/110844805
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Villa et al. - 2013 - Accelerated and Inexact Forward-Backward Algorithms.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1137/110844805
      \endverb
      \verb{url}
      \verb https://doi.org/10.1137/110844805
      \endverb
      \keyw{49M07,65K10,90C25,94A08,accelerated forward-backward splitting,convex optimization,estimate sequences,inexact proximity operator,total variation}
    \endentry
    \entry{WayneStateUniversity1972}{article}{}
      \name{author}{1}{}{%
        {{hash=28742572fcf7c3f19d8eff379976f458}{%
           family={{Wayne State University, Mathematics Department Cofee Room}},
           familyi={W\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Taylor & Francis, Ltd.}%
      }
      \strng{namehash}{28742572fcf7c3f19d8eff379976f458}
      \strng{fullhash}{28742572fcf7c3f19d8eff379976f458}
      \strng{bibnamehash}{28742572fcf7c3f19d8eff379976f458}
      \strng{authorbibnamehash}{28742572fcf7c3f19d8eff379976f458}
      \strng{authornamehash}{28742572fcf7c3f19d8eff379976f458}
      \strng{authorfullhash}{28742572fcf7c3f19d8eff379976f458}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{00029890}
      \field{journaltitle}{The American Mathematical Monthly}
      \field{month}{12}
      \field{number}{10}
      \field{title}{{Every convex function is locally Lipschitz}}
      \field{volume}{79}
      \field{year}{1972}
      \field{pages}{1121\bibrangedash 1124}
      \range{pages}{4}
      \verb{doi}
      \verb 10.2307/2317434
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.2307/2317434
      \endverb
      \verb{url}
      \verb https://doi.org/10.2307/2317434
      \endverb
    \endentry
    \entry{Yang2003}{article}{}
      \name{author}{1}{}{%
        {{hash=ed6f1cd802fbd72f9d2ba807fe42c15c}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Xiao\bibnamedelima Qi},
           giveni={X\bibinitperiod\bibinitdelim Q\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer New York LLC}%
      }
      \strng{namehash}{ed6f1cd802fbd72f9d2ba807fe42c15c}
      \strng{fullhash}{ed6f1cd802fbd72f9d2ba807fe42c15c}
      \strng{bibnamehash}{ed6f1cd802fbd72f9d2ba807fe42c15c}
      \strng{authorbibnamehash}{ed6f1cd802fbd72f9d2ba807fe42c15c}
      \strng{authornamehash}{ed6f1cd802fbd72f9d2ba807fe42c15c}
      \strng{authorfullhash}{ed6f1cd802fbd72f9d2ba807fe42c15c}
      \field{sortinit}{Y}
      \field{sortinithash}{fd67ad5a9ef0f7456bdd9aab10fe1495}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Gap functions play a crucial role in transforming a variational inequality problem into an optimization problem. Then, methods solving an optimization problem can be exploited for finding a solution of a variational inequality problem. It is known that the so-called prevariational inequality is closely related to some generalized convex functions, such as linear fractional functions. In this paper, gap functions for several kinds of prevariational inequalities are investigated. More specifically, prevariational inequalities, extended prevariational inequalities, and extended weak vector prevariational inequalities are considered. Furthermore, a class of gap functions for inequality constrained prevariational inequalities is investigated via a nonlinear Lagrangian.}
      \field{issn}{00223239}
      \field{journaltitle}{Journal of Optimization Theory and Applications}
      \field{number}{2}
      \field{title}{{On the gap functions of prevariational inequalities}}
      \field{volume}{116}
      \field{year}{2003}
      \field{pages}{437\bibrangedash 452}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1023/A:1022422407705
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Yang - 2003 - On the gap functions of prevariational inequalities.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1023/A:1022422407705
      \endverb
      \verb{url}
      \verb https://doi.org/10.1023/A:1022422407705
      \endverb
      \keyw{Lagrangian functions,Variational inequalities,gap functions,linear fractional functions}
    \endentry
    \entry{Yang2002}{article}{}
      \name{author}{2}{}{%
        {{hash=ed6f1cd802fbd72f9d2ba807fe42c15c}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Xiao\bibnamedelima Qi},
           giveni={X\bibinitperiod\bibinitdelim Q\bibinitperiod}}}%
        {{hash=13fe5796c5051f169c118de5b1f5046b}{%
           family={Yao},
           familyi={Y\bibinitperiod},
           given={Jen-Chih},
           giveni={J\bibinithyphendelim C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer New York LLC}%
      }
      \strng{namehash}{4fa2b5391e6000d0dcdba2325c699935}
      \strng{fullhash}{4fa2b5391e6000d0dcdba2325c699935}
      \strng{bibnamehash}{4fa2b5391e6000d0dcdba2325c699935}
      \strng{authorbibnamehash}{4fa2b5391e6000d0dcdba2325c699935}
      \strng{authornamehash}{4fa2b5391e6000d0dcdba2325c699935}
      \strng{authorfullhash}{4fa2b5391e6000d0dcdba2325c699935}
      \field{sortinit}{Y}
      \field{sortinithash}{fd67ad5a9ef0f7456bdd9aab10fe1495}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The variational inequality problem with set-valued mappings is very useful in economics and nonsmooth optimization. In this paper, we study the existence of solutions and the formulation of solution methods for vector variational inequalities (VVI) with set-valued mappings. We introduce gap functions and establish necessary and sufficient conditions for the existence of a solution of the VVI. It is shown that the optimization problem formulated by using gap functions can be transformed into a semi-infinite programming problem. We investigate also the existence of a solution for the generalized VVI with a set-valued mapping by virtue of the existence of a solution of the VVI with a single-valued function and a continuous selection theorem.}
      \field{issn}{00223239}
      \field{journaltitle}{Journal of Optimization Theory and Applications}
      \field{number}{2}
      \field{title}{{Gap functions and existence of solutions to set-valued vector variational inequalities}}
      \field{volume}{115}
      \field{year}{2002}
      \field{pages}{407\bibrangedash 417}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1023/A:1020844423345
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Yang, Yao - 2002 - Gap functions and existence of solutions to set-valued vector variational inequalities.pdf:pdf
      \endverb
      \keyw{Vector variational inequalities,existence of a solution,gap functions,semi-infinite programming,set-valued mappings}
    \endentry
    \entry{Zadeh1963}{article}{}
      \name{author}{1}{}{%
        {{hash=596c823166720165ec6627a9c36a9b7d}{%
           family={Zadeh},
           familyi={Z\bibinitperiod},
           given={L.\bibnamedelimi A.},
           giveni={L\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \strng{namehash}{596c823166720165ec6627a9c36a9b7d}
      \strng{fullhash}{596c823166720165ec6627a9c36a9b7d}
      \strng{bibnamehash}{596c823166720165ec6627a9c36a9b7d}
      \strng{authorbibnamehash}{596c823166720165ec6627a9c36a9b7d}
      \strng{authornamehash}{596c823166720165ec6627a9c36a9b7d}
      \strng{authorfullhash}{596c823166720165ec6627a9c36a9b7d}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{15582523}
      \field{journaltitle}{IEEE Transactions on Automatic Control}
      \field{number}{1}
      \field{title}{{Optimality and non-scalar-valued performance criteria}}
      \field{volume}{8}
      \field{year}{1963}
      \field{pages}{59\bibrangedash 60}
      \range{pages}{2}
      \verb{doi}
      \verb 10.1109/TAC.1963.1105511
      \endverb
      \verb{file}
      \verb :Users/zalgo/Downloads/Optimality_and_non-scalar-valued_performance_criteria.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1109/TAC.1963.1105511
      \endverb
      \verb{url}
      \verb https://doi.org/10.1109/TAC.1963.1105511
      \endverb
    \endentry
    \entry{Zou2005}{article}{}
      \name{author}{2}{}{%
        {{hash=f15b129e88b4820c7ed982265e79986b}{%
           family={Zou},
           familyi={Z\bibinitperiod},
           given={Hui},
           giveni={H\bibinitperiod}}}%
        {{hash=0cb8fe4210baa81c4b0e67913b4d2768}{%
           family={Hastie},
           familyi={H\bibinitperiod},
           given={Trevor},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{c346171da05683239c65d0053413406e}
      \strng{fullhash}{c346171da05683239c65d0053413406e}
      \strng{bibnamehash}{c346171da05683239c65d0053413406e}
      \strng{authorbibnamehash}{c346171da05683239c65d0053413406e}
      \strng{authornamehash}{c346171da05683239c65d0053413406e}
      \strng{authorfullhash}{c346171da05683239c65d0053413406e}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p ≫ n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso. {{$\copyright$}} 2005 Royal Statistical Society.}
      \field{issn}{1369-7412}
      \field{journaltitle}{Journal of the Royal Statistical Society: Series B (Statistical Methodology)}
      \field{month}{4}
      \field{number}{2}
      \field{title}{{Regularization and variable selection via the elastic net}}
      \field{volume}{67}
      \field{year}{2005}
      \field{pages}{301\bibrangedash 320}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1111/j.1467-9868.2005.00503.x
      \endverb
      \verb{file}
      \verb :Users/zalgo/Library/Application Support/Mendeley Desktop/Downloaded/Zou, Hastie - 2005 - Regularization and variable selection via the elastic net.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1111/j.1467-9868.2005.00503.x
      \endverb
      \verb{url}
      \verb https://doi.org/10.1111/j.1467-9868.2005.00503.x
      \endverb
      \keyw{Grouping effect,LARS algorithm,Lasso,Penalization,Variable selection,p≫n problem}
    \endentry
  \enddatalist
\endrefsection
\endinput

