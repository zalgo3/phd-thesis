\documentclass[../main]{subfiles}

\begin{document}

\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface}
To accurately answer all human needs with optimization problems, we cannot avoid considering multi-objective optimization.
Humankind is a \emph{greedy} creature that cannot tolerate only a single desire and always has multiple preferences.
Unfortunately, many of them conflict with each other, and the best choice to answer all of them seldom exists. This trade-off is what makes multi-objective optimization a tough challenge.
Even for problems ideally with multi-objectives, the single-objective models tend to be adopted.
However, thanks to the long-standing wisdom of scientists, the development of theories and algorithms for multi-objective optimization has been gradually gaining speed in recent years.
As one of the ``\emph{dwarfs who ride above the giants},'' I would like to contribute to their development, even if only slightly.

This thesis provides theories and algorithms for problems in multi-objective optimization where the objective function is non-smooth. These types of problems are very complex.
Thus, it is not practical to consider general non-smooth models for large-scale problems, which have been recently in high demand.
Therefore, this thesis mainly focuses on multi-objective optimization problems with a specific structure, called composite models.
In detail, this model's every objective function is the sum of differentiable and convex functions.
Such models work well, for example, with the loss and regularization models in machine learning.

There are three main contributions of this thesis.
One is new merit functions for multi-objective optimization and the elucidation of their properties.
A merit function is a function that returns zero in the solution of the problem and a positive number otherwise.
We can use it to reformulate the original problem and estimate the rate of convergence of the algorithm.
Another contribution is the proximal gradient method for multi-objective optimization problems.
It is a first-order method using information from first-order derivatives for composite multi-objective optimization problems.
It is more efficient than existing first-order methods for non-smooth multi-objective problems; it has an~$O(1/k)$ convergence rate.
It can also generate stationary points for non-convex problems. Another contribution is the accelerated proximal gradient method for multi-objective optimization.
It does not work for non-convex problems, but it is faster than the proximal gradient method and solves problems with~$O(1/k^2)$.
The proposed algorithm is also novel for single-objective problems if the parameters are well-chosen.
Its numerical results are better than existing algorithms for single objectives.

\begin{flushright}
\TheAuthor \\
\TheDate
\end{flushright}

\end{document}
