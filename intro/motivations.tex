\documentclass[../main]{subfiles}

\begin{document}
\section{Motivations and contributions} \zlabel{sec:intro:motivations}
As discussed in \zcref{sec:intro:multi_objective}, multi-objective optimization~\zcref{eq:MOO} is an indispensable model in dealing with real-world problems, and the studies on its theories and algorithms have great significance.
On the other hand, many previous studies on multi-objective optimization, particularly on the descent methods described in \zcref{sec:intro:multi_objective:descent} and the merit functions described in \zcref{sec:intro:merit:MO}, have dealt with smooth problems, and there is still room for exploration of non-smooth problems.
The projected subgradient method introduced in \zcref{ex:descent} can handle non-smooth multi-objective optimization, but it may not work well for large-scale problems due to the stepsize decay.

This thesis focuses on non-smooth multi-objective optimization problems with specific structures, mainly the generalization of the composite model introduced in \zcref{sec:intro:composite}, i.e.,
\begin{equation} \label{eq:composite_MO}
    \min_{x \in C} \quad F(x) = f(x) + g(x)
,\end{equation} 
where~$C \subseteq \setR^n$ is a non-empty, closed, and convex set, and~$F \colon \setR^n \to (-\infty, +\infty]^m, f \colon \setR^n \to \setR^m, g \colon \setR^n \to (-\infty, +\infty]^m$ are vector-valued functions with~$F \coloneqq (F_1, \dots, F_m)^\top, f \coloneqq (f_1, \dots, f_m)^\top, g \coloneqq (g_1, \dots, g_m)^\top$ such that~$f_i \colon \setR^n \to \setR$ is continuously differentiable and~$g_i \colon \setR^n \to (-\infty, +\infty]$ is closed, proper, and convex.
Then, we present their theory and algorithms.
\end{document}
