\documentclass[../../main]{subfiles}

\begin{document}
\subsection{Semi-definite programming} \label{sec:pgm:robust:SDP}
Suppose that\footnote{We denote $A \succeq (\succ) O$ when $A$ is positive semidefinite (positive definite). Also, $A \succeq (\succ) B$ if and only if $A - B \succeq (\succ) O$.} $\hat{g}_i (x,u) = (x + u)^\T A_i (x + u)$ and ${\mathcal{U}}_i = \{ a_i + P_i v \in \setR^n \mid \| v \| \leq 1 \}$, where $A_i \in \setR^{n \times n}$ and $A_i \succeq O,\, a_i \in \setR^n$ and $P_i \in \setR^{n \times n}$. Then, there exists a matrix $M_i \in \setR^{n \times n}$ such that $A_i = M_i M_i^\T$. Note that $\hat{g}_i$ is convex quadratic and $\mathcal{U}_i$ is an ellipsoid. Here, without loss of generality we can assume that $A$ is a symmetric matrix since $(x + u)^\T A_i (x + u) = (x + u)^\T \tilde{A}_i (x + u)$, where $\tilde{A}_i \coloneqq (A_i+A_i^\T) / 2$. Then, $g_i(x)$ can be given as
\begin{equation} \label{eq:sdp_g_i}
g_i(x) = \max_{v : \| v \| \leq 1} (x + a_i + P_i v)^\T A_i (x + a_i + P_i v).
\end{equation}
Since problem~\cref{eq:sdp_g_i} is a maximization problem of a convex function, it is not a convex optimization problem. Fortunately, it can be seen as a subproblem of a trust region method, so its optimal value~$g_i(x)$ can be obtained efficiently. Considering~\cref{eq:sdp_g_i}, we observe that
\begin{equation} \label{eq:sdp_g_i_}
g_i(x + d) = \max_{v : \| v \| \leq 1} (x + d + a_i + P_i v)^\T A_i (x + d + a_i + P_i v).
\end{equation}
From~\cite[Section~3]{Beck2006}, the Lagrangian dual of the maximization problem~\cref{eq:sdp_g_i_} is given by
\begin{align}
\begin{aligned} \label{eq:dual}
\min_{\alpha, w} \quad   & -w \\ 
\st    \quad   & 
\begin{bmatrix}
-P_i^\T A_i P_i &\quad -P_i^\T A_i (x + d + a_i) \\
-(x + d + a_i)^\T A_i^\T P_i &\quad -(x + d + a_i)^\T A_i (x + d + a_i) - w
\end{bmatrix} \\
& \hspace{20.3em} \succeq
\alpha \begin{bmatrix}
-I_n & 0 \\
0 & 1
\end{bmatrix}, \\
& \alpha \geq 0,
\end{aligned}
\end{align}
where $I_n$ stands for the identity matrix of dimension $n$. Let $(\alpha^\ast, w^\ast)$ be an optimal solution of~\cref{eq:dual} and assume that\footnote{Here, $\dim$ denotes dimension of a space and $\ker$ means kernel of a matrix.} $\dim(\ker(A_i + \alpha^\ast I_n)) \neq 1$. Since both~\cref{eq:sdp_g_i_} and \cref{eq:dual} have strictly feasible solutions and $I_n \succ O$, then the strong duality holds from~\cite[Theorem~3.5]{Beck2006}. Therefore, recalling~\cref{eq:subproblem}, the subproblem~\cref{eq:prox subprob} is equivalent to
\begin{align}
\begin{aligned}
\min_{\gamma, d, w_i, \alpha_i} \quad & \gamma + \frac\ell2 \| d \|^2 \\ 
\st    \quad   & \nabla f_i (x)^\T d - w_i - g_i (x) \leq \gamma, \\
& \begin{bmatrix}
-P_i^\T A_i P_i + \alpha_i I_n &\quad -P_i^\T A_i (x + d + a_i) \\
-(x + d + a_i)^\T A_i^\T P_i &\quad -(x + d + a_i)^\T A_i (x + d + a_i) - w_i -\alpha_i
\end{bmatrix} \\
& \hspace{26em} \succeq O, \\
& \alpha_i \geq 0, \quad i = 1,\dots,m.
\end{aligned}
\end{align}
Now, by using slack variables $\tau \in \setR$ and $\zeta_i \in \setR$ and converting the convex quadratic constraints to second-order cone ones, we get the following semidefinite programming problem:
\begin{align} \label{eq:sdp}
\begin{aligned}
\min_{\tau, \alpha_i, w_i, \gamma, d} \quad   &\tau \\ 
\st    \quad   &\nabla f_i(x)^\T d - w_i - g_i(x) \leq \gamma, \\
& \begin{bmatrix} 1 - \gamma + \tau \\
1 + \gamma - \tau \\
\sqrt{2 \ell} d
\end{bmatrix} \in \mathcal{K}_{n + 2}, \\
& 
\begin{bmatrix}
-P_i^\T A_i P_i + \alpha_i I_n & -P_i^\T A_i (x + d + a_i) \\
-(x + d + a_i)^\T A_i^\T P_i & \zeta_i
\end{bmatrix} \succeq
O, \\
& \begin{bmatrix} \dfrac{1 - \zeta_i - w_i - \alpha_i}{2} \\
\dfrac{1 + \zeta_i + w_i + \alpha_i}{2} \\
M_i^\T (x + d + a_i)
\end{bmatrix} \in \mathcal{K}_{n + 2}, \\
& \alpha_i \geq 0,  \quad i = 1,\ldots,m,
\end{aligned}
\end{align}
where $O$ stands for a zero matrix with appropriate dimension. Note that the second-order cone constraints can be converted further into semidefinite constraints.

\end{document}
