\documentclass[../main]{subfiles}

\begin{document}
\section{Convergence of the method} \zlabel{sec:pgm:convergence}
We first show the (classical) global convergence of the proposed algorithm.
\begin{theorem} \zlabel{thm:pgm_stationary}
    Every accumulation point of~$\set{x^k}$ generated by \zcref{alg:pgm_MO} with any stepsize selection procedure given in \zcref{sec:pgm:algorithm:armijo,sec:pgm:algorithm:sdc,sec:pgm:algorithm:constant}, if it exists, is Pareto stationary for~\zcref{eq:composite_MO}.
\end{theorem}
\begin{proof}
    Assume that~$\set{x^{k_j}}$ converges to~$\bar{x}$.
    According to \zcref{thm:pgm_stop:equivalence}, it suffices to check that~$\theta_\alpha(\bar{x}) = 0$ for some~$\alpha > 0$.
    Considering~\zcref{eq:pgm_nonincrementality} and the existence of a subsequence of~$\set{F_i(x^k)}$ converging to~$F_i(\bar{x})$, we have
    \begin{equation} \label{eq:pgm_stationary:convergence}
        \lim_{k \to \infty} F_i(x^k) = F_i(\bar{x})
    .\end{equation}
    Let us now prove the claim for each stepsize rule.

    \zcref[ref=title,noname,S]{sec:pgm:algorithm:armijo}:
    \zcref[S]{eq:pgm_armijo_step,eq:pgm_armijo_rule,eq:pgm_optimal_function_ub} give
    \begin{equation}
        F_i(x^{k + 1}) - F_i(x^k) \le \rho s_k \left[ \theta_\alpha(x^k) - \frac{1}{2 \alpha} \norm{z^k - x^k}_2^2\right] \le \rho s_k \theta_\alpha(x^k) \le 0
    .\end{equation} 
    for any~$i = 1, \dots, m$.
    Combined with~\zcref{eq:pgm_stationary:convergence}, we get
    \begin{equation} \label{eq:pgm_stationary:step_merit_zero}
        \lim_{k \to \infty} s_k \theta_\alpha(x^k) = 0
    .\end{equation}
    If~$\limsup_{k \to \infty} s_k > 0$, it is clear that~$\theta_\alpha(\bar{x}) = 0$.
    Thus, we now suppose that~$\lim_{k \to \infty} s_k = 0$.
    If we fix some positive integer~$q$, we have~$s_k < \xi^q$ for sufficiently large~$k$.
    This means that the Armijo condition~\zcref{eq:pgm_armijo_rule} does not hold for the stepsize~$\xi^q$, i.e.,
    \begin{equation} \label{eq:pgm_stationary:non_armijo}
        F_{i_k}(x^k + \xi^q (z^k - x^k)) > F_{i_k}(x^k) + \rho \xi^q \left[ \theta_\alpha(x^k) - \frac{1}{2 \alpha} \norm{z^k - x^k}_2^2 \right]
    \end{equation} 
    for some~$i_k = 1, \dots, m$.
    Since the number of values that~$i_k$ can take is finite, some subsequence of~$\set{i_k}$ converges to some~$\bar{i} = 1, \dots, m$.
    Therefore, if we choose a proper subsequence and taking limit in~\zcref{eq:pgm_stationary:non_armijo}, we obtain
    \begin{equation}
        F_{\bar{i}}(\bar{x} + \xi^q (p_\alpha(\bar{x}) - \bar{x})) \ge F_{\bar{i}}(\bar{x}) + \rho \xi^q \left[ \theta_\alpha(\bar{x}) - \frac{1}{2 \alpha} \norm{p_\alpha(\bar{x}) - \bar{x}}_2^2 \right]
    .\end{equation} 
    Since~$q$ can take arbitrary positive integer values, \zcref{thm:pgm_armijo_existence} shows that~$\bar{x}$ is Pareto stationary.

    \zcref[ref=title,noname,S]{sec:pgm:algorithm:sdc}:
    Similar to the deriviation of~\zcref{eq:pgm_stationary:step_merit_zero}, the condition~\zcref{eq:pgm_sdc} leads to~$\theta_{\alpha_k}(x^k) \to 0$ as~$k \to \infty$.
    If~$\alpha \coloneqq \lim_{k \to \infty} \alpha_k > 0$, it is easy to see that~$\theta_\alpha(\bar{x}) = 0$.
    Assume that~$\lim_{k \to \infty} \alpha_k = 0$.
    Similar to the last paragraph, fixing some positive integer~$q$ and considering that~\zcref{eq:pgm_sdc} does not hold, we get
    \begin{equation}
        F_{\bar{i}}(p_{\xi^q}(\bar{x})) \ge F_{\bar{i}}(\bar{x}) + \theta_{\xi^q}(\bar{x})
    .\end{equation} 
    Since~$q$ can take any positive integer, the locally Lipschitz continuity of~$\nabla f_i$ shows that~$\bar{x}$ is Pareto stationary.

    \zcref[ref=title,noname,S]{sec:pgm:algorithm:constant}:
    It is clear from~\zcref{eq:pgm_constant} and the previous paragraph.
\end{proof}

We now introduce the following assumption, standard in the analysis of descent methods for vector optimization~\cite{Fliege2000,Grana-Drummond2004,Fukuda2011}.
\begin{assumption} \zlabel{asm:pgm_existence_lb}
    For all sequence~$\set{y^k} \subseteq F(\setR^n)$ such that~$y^{k + 1} \le y^k, k = 0, 1, \dots$, there exists~$x \in \setR^n$ satisfying~$F(x) \le y^k, k = 0, 1, \dots$.
\end{assumption}
Under convexity and reasonable assumptions, we can also prove the true convergence of iterates as follows:
\begin{theorem} \zlabel{thm:pgm_convergence}
    Suppose that~$f_i$ is convex for~$i = 1, \dots, m$ and let~$\set{x^k}$ generated by \zcref{alg:pgm_MO} with the stepsize selection procedure given in any of \zcref{sec:pgm:algorithm:armijo,sec:pgm:algorithm:sdc,sec:pgm:algorithm:constant}.
    Under \zcref{asm:pgm_existence_lb},~$\set{x^k}$ converges to some~$x^* \in T \coloneqq \Set{x \in \setR^n}{F(x) \le F(x^k), k = 0, 1, \dots}$ and~$x^*$ is weakly Pareto optimal for~\zcref{eq:composite_MO}.
\end{theorem}
\begin{proof}
    Let~$x \in T$.
    We have
    \begin{equation} \label{eq:pgm_convergence:fejer_pre}
        \begin{aligned}
            \norm{x^{k + 1} - x}_2^2 &= \norm{x^k - x}_2^2 + \norm{x^{k + 1} - x^k}_2^2 + 2 \innerp{x^k - x^{k + 1}}{x - x^k} \\
                                     &= \norm{x^k - x}_2^2 + s_k^2 \norm{z^k - x^k}_2^2 - 2 s_k \innerp{z^k - x^k}{x - x^k} \\
                                     &\le \norm{x^k - x}_2^2 + s_k \norm{z^k - x^k}_2^2 - 2 s_k \innerp{z^k - x^k}{x - x^k}
        \end{aligned}
    ,\end{equation}
    where the second equality follows from \zcref{alg:pgm_MO:update} of \zcref{alg:pgm_MO}, and the inequality comes from the fact that~$s_k \in (0, 1]$.
    Recall that~$\simplex^m$ and~$\mathcal{I}_{\alpha_k}$ are defined by~\zcref{eq:simplex,eq:pgm_active_set}, respectively.
    Then, \zcref{eq:pgm_optimality} implies that there exists~$\lambda(x^k) \in \simplex^m$ such that~$\lambda_j(x^k) = 0$ for any~$j \in \mathcal{I}_{\alpha_k}(x^k)$ and
    \begin{align}
        \frac{1}{\alpha_k} \innerp{x^k - z^k}{x - z^k} \le{}& \sum_{i = 1}^{m} \lambda_i(x^k) \left[ \innerp{\nabla f_i(x^k)}{x - z^k} + g_i(x) - g_i(z^k) \right] \\
        ={}& \sum_{i = 1}^{m} \lambda_i(x^k) \left[ \innerp{\nabla f_i(x^k)}{x - x^k} + g_i(x) - g_i(x^k) \right] \\
           &+ \sum_{i = 1}^{m} \lambda_i(x^k) \left[ \innerp{\nabla f_i(x^k)}{x^k - z^k} + g_i(x^k) - g_i(z^k) \right]
    .\end{align}
    Since~$f_i$ is convex and~$x \in T$, the first term in the right-hand side is non-negative.
    Therefore,~\zcref{eq:pgm_optimal_function,eq:pgm_active_set} and \zcref{alg:pgm_MO:subproblem} of~\zcref{alg:pgm_MO} give
    \begin{equation}
        \frac{1}{\alpha_k} \innerp{x^k - z^k}{x - z^k} \le - \theta_{\alpha_k}(x^k) + \frac{1}{2 \alpha_k} \norm{z^k - x^k}_2^2
    .\end{equation} 
    Combining the above inequality with~\zcref{eq:pgm_convergence:fejer_pre} gives
    \begin{equation}
        \norm{x^{k + 1} - x}_2^2 \le \norm{x^k - x}_2^2 - 2 \alpha_k s_k \theta_{\alpha_k}(x^k) = \norm{x^k - x}_2^2 + 2 \alpha_k s_k \abs{\theta_{\alpha_k}(x^k)}
    .\end{equation} 
    Since~$T \neq \emptyset$, it is easy to see that~$\sum_{k = 0}^{\infty} \alpha_k s_k \abs{\theta_{\alpha_k}(x^k)}$ holds for each stepsize selection.
    Hence, \zcref{def:fejer} implies that~$\set{x^k}$ is quasi-F\'ejer convergent to~$T$, and thus~$\set{x^k}$ is bounded due to \zcref{thm:fejer}.
    Let~$x^*$ be an accumulation point of~$\set{x^k}$ and assume that~$x^{k_j} \to x^*$.
    If we fix some non-negative integer~$\bar{k}$, for sufficiently large~$j$, we have
    \begin{equation}
        F(x^{k_j}) \le F(x^{\bar{k}})
    .\end{equation} 
    Taking~$j \to \infty$ yields that
    \begin{equation}
        F(x^*) \le F(x^{\bar{k}})
    .\end{equation} 
    Since~$\bar{k}$ can take any non-negative integer,~$x^*$ belongs to~$T$.
    Using again \zcref{thm:fejer}, we can complete the proof.
\end{proof}
\end{document}
