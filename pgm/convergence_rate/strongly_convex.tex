\documentclass[../../main]{subfiles}

\begin{document}
\subsection{The strongly convex case}
For~\zcref{eq:MOO} with~$m = 1$, it is known that~$\{x^k\}$ converges linearly to the optimal point when~$f_1$ is strongly convex~\cite{Beck2017}.
Now, we show that the same result holds for \zcref{alg:pgm_MO}.
\begin{theorem} \zlabel{thm:pgm_linear_convergence_sc}
    Suppose that \zcref{asm:pgm_Lipschitz_gradient,asm:pgm_convexity} holds and~$\mu_f + \mu_g > 0$.
    Then,~$\set{x^k}$ generated by \zcref{alg:pgm_MO} with the stepsize selection rule given in any of \zcref{sec:pgm:algorithm:armijo,sec:pgm:algorithm:sdc,sec:pgm:algorithm:constant} converges linearly to a Pareto optimal solution~$x^*$ of \zcref{eq:MOO}, i.e.,
    \begin{equation}
        \norm*{x^k - x^*}_2 = O\left(\exp(-rk)\right) 
    \end{equation}
    for some~$r > 0$ with~$O \colon [0, + \infty) \to \setR$ satisfying~$\limsup_{t \to 0} O(t) / t < + \infty$.
\end{theorem}
\begin{proof}
    Since each~$F_i$ is strongly convex for~$i = 1, \dots, m$, the level set of~$F$ is bounded.
    Thus, $\set{x^k}$ has an accumulation point~$x^* \in \setR^n$, and~$x^*$ is Pareto optimal because of \zcref{thm:Pareto:strict_convex,thm:pgm_stationary}.
    Note that~$F(x^*) \le F(x^k)$ for all~$k = 0, 1, \dots$.

    We now assume that we adopt \zcref[ref=title,noname]{sec:pgm:algorithm:armijo}.
    \zcref[S]{thm:pgm_recursive,thm:pgm_armijo_stepsize_lb} implies
    \begin{multline}
        \min_{i = 1, \dots, m} \left[F_i(x^{k + 1}) - F_i(x^*)\right] + \frac{\rho (1 + \alpha \mu_g)}{2 \alpha} \norm*{x^{k + 1} - x^*}_2^2 \\
        \le (1 - \rho s_{\min}) \min_{i = 1, \dots, m} \left[F_i(x^k) - F_i(x^*)\right] + \frac{\rho [1 + \alpha \mu_g - \alpha s_{\min} (\mu_f + \mu_g)]}{2 \alpha} \norm*{x^k - x^*}_2^2
    .\end{multline}
    Thus, if we write
    \begin{equation}
        M \coloneqq \max \left( 1 - \rho s_{\min}, \frac{1 + \alpha \mu_g - \alpha s_{\min} (\mu_f + \mu_g)}{1 + \alpha \mu_g} \right) 
    ,\end{equation}
    we have
    \begin{multline}
        \min_{i = 1, \dots, m} \left[F_i(x^{k + 1}) - F_i(x^*)\right] + \frac{\rho (1 + \alpha \mu_g)}{2 \alpha} \norm*{x^{k + 1} - x^*}_2^2 \\
        \le M \left\{ \min_{i = 1, \dots, m} [F_i(x^k) - F_i(x^*)] + \frac{\rho (1 + \alpha \mu_g)}{2 \alpha} \norm*{x^k - x^*}_2^2 \right\}  
    .\end{multline}
    Applying the above inequality recursively, we obtain
    \begin{multline} \label{eq:pgm_linear_convergence_sc:rec}
        \min_{i = 1, \dots, m} \left[F_i(x^k) - F_i(x^*)\right] + \frac{\rho (1 + \alpha \mu_g)}{2 \alpha} \norm*{x^k - x^*}_2^2 \\
        \le M^k \left\{ \min_{i = 1, \dots, m} [F_i(x^0) - F_i(x^*)] + \frac{\rho (1 + \alpha \mu_g)}{2 \alpha} \norm*{x^0 - x^*}_2^2 \right\}  
    .\end{multline}
    Let~$\eta_i^* \in \partial F_i(x^*)$.
    Then,~$(\mu_{f_i} + \mu_{g_i})$-convexity of~$F_i$ shows that
    \begin{equation} \label{eq:pgm_linear_convergence_sc:sc}
        \begin{aligned}
            \min_{i = 1, \dots, m} \left[ F_i(x^k) - F_i(x^*) \right] &\ge \min_{i = 1, \dots, m} \left[  \innerp{\eta_i^*}{x^k - x^*} + \frac{\mu_{f_i} + \mu_{g_i}}{2} \norm{x^k - x^*}_2^2 \right] \\
                                                                      &\ge \min_{i = 1, \dots, m} \norm{\eta_i^*}_2 \norm{x^k - x^*}_2 + \frac{\mu_f + \mu_g}{2} \norm{x^k - x^*}_2^2
        .\end{aligned}
    \end{equation}
    Since~$M \in (0, 1)$, \zcref{eq:pgm_linear_convergence_sc:rec,eq:pgm_linear_convergence_sc:sc} proves the desired result.

    We can get the similar result for the \zcref[ref=title,noname]{sec:pgm:algorithm:sdc} by \zcref{thm:pgm_recursive:sdc,thm:pgm_sdc_stepsize_lb} and for the \zcref[ref=title,noname]{sec:pgm:algorithm:constant} by \zcref{thm:pgm_recursive:constant}.
\end{proof}
\end{document}
