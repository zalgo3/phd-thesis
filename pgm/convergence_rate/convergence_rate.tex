\documentclass[../../main]{subfiles}

\begin{document}
\section{Convergence rate of the method} \zlabel{sec:pgm:rate}
For the convergence rate analysis, we assume the Lipschitz gradient condition.
\begin{assumption} \zlabel{asm:pgm_Lipschitz_gradient}
    Each~$f_i$ is~$L_{f_i}$-smooth with~$L_{f_i} > 0$ for~$i = 1, \dots, m$.
    We write
    \begin{equation} \label{eq:pgm_Lipschitz_constant}
        L_f \coloneqq \max_{i = 1, \dots, m} L_{f_i}
        .\end{equation}
\end{assumption}
%Let us now analyze the convergence rates of the proposed method.
%Define~$\psi_x \colon \setR^n \to \setR$ by
%\begin{equation} \label{eq:psi}
%    \psi_x(d) \coloneqq \max_{i = 1, \dots, m} \left[ \nabla f_i(x)^\T d + g_i(x + d) - g_i(x) \right].
%\end{equation}
%Looking at \zcref{alg:pgm_MO} differently, it generates a sequence~$\{ x^k \}$ iteratively with the following procedure:
%\begin{equation}
%    x^{k + 1} \coloneqq x^k + d^k,
%\end{equation}
%where~$d^k$ is a search direction.
%At every iteration~$k$, we define this~$d^k$ by solving
%\begin{equation} \label{eq:subprob}
%    d^k \coloneqq \argmin_{d \in \setR^n} \left[ \psi_{x^k}(d) + \frac{\ell}{2} \norm*{ d }^2 \right],
%\end{equation}
%with a positive constant~$\ell > 0$.
%Note that we have
%\begin{equation} \label{eq:psi w_ell}
%    \psi_{x^k}(d^k) + \frac{\ell}{2} \norm*{ d^k }^2 = - w_\ell\left(x^k\right),
%\end{equation}
%where~$w_\ell$ is defined by~\zcref{eq:reg_lin_gap_MO}.
%We suppose that the algorithm generates an infinite sequence of iterates from now on.
%The following result shows an important property for~$\psi_x$.
%\begin{lemma}[{\cite[Lemma~4.1]{Tanabe2019}}] \zlabel{lem: psi property}
%    Let~$\{ d^k \}$ be generated by \zcref{alg:pgm_MO} and recall the definition~\zcref{eq:psi} of~$\psi_x$. Then, we have
%    \begin{equation}
%        \psi_{x^k}(d^k) \le - \ell \norm*{d^k}^2 \forallcondition{k}.
%    \end{equation}
%\end{lemma}
%If~$\ell > L$, from \zcref{thm:descent}, for all $i = 1, \dots, m$ we have
%\begin{equation} \label{eq:descent}
%    F_i\left(x^{k + 1}\right) - F_i\left(x^k\right) \le \innerp{\nabla f_i\left(x^k\right)}{d^k} + g_i\left(x^{k + 1}\right) - g_i\left(x^k\right) + \frac{\ell}{2}\norm*{d^k}.
%\end{equation}
%The right-hand side of~\zcref{eq:descent} is less than zero since~$d^k$ is the optimal solution of~\zcref{eq:subprob}, so we get
%\begin{equation} \label{eq:nonincreasing}
%    F_i\left(x^{k + 1}\right) \le F_i\left(x^k\right).
%\end{equation}
%\begin{remark}
%    When the Lipschitz constant~$L$ is unknown or incomputable, we can use~$\ell$ for~\zcref{eq:subprob} calculated by backtracking instead, i.e., we can set the initial value of~$\ell$ appropriately and multiply~$\ell$ by a prespecified scalar~$\gamma > 1$ at each iteration until~\zcref{eq:descent} is satisfied.
%    Since~$L$ is finite, the backtracking only requires a finite number of steps.
%\end{remark}
\subfile{non_convex}
\subfile{convex}
%\subfile{strongly_convex}
\subfile{prox_PL}

\end{document}
