\documentclass[../../main]{subfiles}

\begin{document}
\subsection{The convex case}
For \cref{eq:MOO} with~$m = 1$, if~$f_1$ is convex, then $\{ F_1(x^k) - F_1^\ast \}$ converges to zero with rate~$O(1/k)$ using the proximal gradient method, where~$F_1^\ast$ is the optimal objective value of~$F_1$~\cite{Beck2009}.
In this subsection, we show how fast~$\{u_0(x^k)\}$ converges to zero with \cref{alg:pgm_MO}.
Let us start by proving the following lemma.
Note, however, that we state it with $f_i$ and $g_i$ having general (nonnegative) convexity parameters\footnote{We say that~$h \colon \setR^n \to (-\infty, +\infty]$ has a \emph{convexity parameter}~$\varsigma \in \setR$ if~$h(\alpha x + (1 - \alpha)y) \le \alpha h(x) + (1 - \alpha)h(y) - (1 / 2)\alpha (1 - \alpha) \varsigma \norm{x - y}^2$ holds for all~$x, y \in \setR^n$ and~$\alpha \in [0, 1]$. When~$\varsigma > 0$, we call~$h$ strongly convex. Note that this definition allows non-convex cases, i.e.,~$\varsigma < 0$ can also be considered.}, but they turn out to be zero in this subsection.
\begin{lemma} \label{lem:without_line_lem}
	Let $f_i$ and $g_i$ have convexity parameters~$\mu_i \in \setR$ and $\nu_i \in \setR$, respectively, and write $\mu \coloneqq \min_{i = 1, \dots, m} \mu_i$ and $\nu \coloneqq \min_{i = 1, \dots, m} \nu_i$.
    Then, for all $x \in \setR^n$ it follows that
    \begin{multline}
        \sum_{i = 1}^m \lambda_i^k \left(F_i(x^{k + 1}) - F_i(x)\right) \le \frac{\ell}{2} \left( \norm*{ x^k - x}_2^2 - \norm*{ x^{k + 1} - x}_2^2 \right) \\
        - \frac{\nu}{2} \norm*{ x^{k + 1} - x }_2^2 - \frac{\mu}{2} \norm*{ x^k - x}_2^2,
    \end{multline}
	where $\lambda_i^k$ satisfies the following conditions:
    \begin{enumerate}
        \item There exists $\eta_i^k \in \partial g_i(x^k + d^k)$ such that $\sum_{i = 1}^m \lambda_i^k (\nabla f_i(x^k) + \eta_i^k) + \ell d^k = 0$,
        \item $\sum_{i = 1}^m \lambda_i^k = 1, \lambda_i^k \ge 0 \left(i \in \mathcal{I}_{x^k}(d^k)\right)$ and $\lambda_i^k = 0 \left(i \notin \mathcal{I}_{x^k}(d^k)\right)$,
    \end{enumerate}
	where~$\mathcal{I}_x(d) \coloneqq \Set*{i = 1, \dots, m}{\psi_x(d) = \nabla f_i(x)^\T d + g_i(x + d) - g_i(x)}$.
\end{lemma}
\begin{proof}
    From~\cref{eq:descent}, we have
	\[
        F_i(x^{k + 1}) - F_i(x^k) \le \nabla f_i(x^k)^\T (x^{k + 1} - x^k) + g_i(x^{k + 1}) - g_i(x^k) + \frac{\ell}{2} \norm*{ d^k}_2^2
	.\]
    The above inequality and convexity of $f_i$ with modulus~$\mu_i$ give
    \[
        \begin{split}
            \MoveEqLeft[.5] F_i(x^{k + 1}) - F_i(x) = ( F_i(x^k) - F_i(x) ) + ( F_i(x^{k + 1}) - F_i(x^k) ) \\
        &\le \left(\nabla f_i(x^k)^\T (x^k - x) - \frac{\mu_i}{2} \norm*{ x^k - x}_2^2 + g_i(x^k) - g_i(x)\right) \\
        &\quad + \left(\nabla f_i(x^k)^\T (x^{k + 1} - x^k) + g_i(x^{k + 1}) - g_i(x^k) + \frac{\ell}{2} \norm*{ x^{k + 1} - x^k}_2^2\right) \\
        &\le \nabla f_i(x^k)^\T (x^k + d^k - x) + g_i(x^k + d^k) - g_i(x)  - \frac{\mu}{2} \norm*{ x^k - x}_2^2 + \frac{\ell}{2} \norm*{ d^k}_2^2 \\
        &\le (\nabla f_i(x^k) + \eta_i^k)^\T (x^k + d^k - x)
        - \frac{\mu}{2} \norm*{ x^k - x}_2^2 - \frac{\nu}{2} \norm*{ x^{k + 1} - x }_2^2 + \frac{\ell}{2} \norm*{d^k}_2^2
        ,\end{split}
    \]
	where the second inequality follows from the definition of $\mu$ and the fact that $x^{k + 1} = x^k + d^k$, and the last one comes from the convexity of~$g_i$. Multiplying the above inequality by $\lambda_i^k$ and summing for all~$i = 1, \dots, m$, the conditions~(i) and (ii) give
    \[
        \begin{split}
            \MoveEqLeft[.5] \sum_{i = 1}^m \lambda_i^k \left( F_i(x^{k + 1}) - F_i(x) \right) \\
        &= {- \ell (d^k)^\T (x^k + d^k - x) - \frac{\mu}{2} \norm*{ x^k - x}_2^2 - \frac{\nu}{2} \norm*{ x^{k + 1} - x }_2^2 + \frac{\ell}{2} \norm*{ d^k}_2^2} \\
        &=  - \frac{\ell}{2} \left( 2 (d^k)^\T (x^k - x) + \norm*{ d^k }_2^2 \right) - \frac{\mu}{2} \norm*{ x^k - x}_2^2 - \frac{\nu}{2} \norm*{ x^{k + 1} - x }_2^2 \\
        &=  \frac{\ell}{2} \left( \norm*{ x^k - x}_2^2 - \norm*{ x^{k + 1} - x}_2^2 \right) - \frac{\mu}{2} \norm*{ x^k - x}_2^2 - \frac{\nu}{2} \norm*{ x^{k + 1} - x }_2^2
        .\end{split}
    \]
\end{proof}

Now, we show that~$\set{u_0(x^k)}$ converges to zero with rate~$O(1 / k)$ with \cref{alg:pgm_MO} under the following assumption.
\begin{assumption} \label{asm:bound}
    Let~$X^\ast$ be the set of weakly Pareto optimal points for~\cref{eq:MOO}, and define the level set of~$F$ for~$\alpha \in \setR^m$ by~$\Omega_F(\alpha) \coloneqq \{ x \in S \mid F(x) \le \alpha \}$.
    Then, for all~$x \in \Omega_F(F(x^0))$ there exists~$x^\ast \in X^\ast$ such that~$F(x^\ast) \le F(x)$ and
    \[ \label{eq:R}
        R \coloneqq \sup_{F^\ast \in F(X^\ast \cap \Omega_F(F(x^0)))} \inf_{x \in F^{-1}(\set*{F^\ast})} \norm*{x - x^0}_2^2 < \infty
    .\]
\end{assumption}
\begin{remark}
    \begin{enumerate}
        \item In single-objective cases, \cref{asm:bound} is valid if the optimization problem has at least one optimal solution; when~$m = 1$,~$X^\ast$ coincides with the optimal solution set, and the equality~$X^\ast \cap \Omega_F(F(x^0)) = X^\ast$ holds, so we have~$R = \inf_{x \in X^\ast} \norm*{x - x^0}_2^2 < \infty$.
        \item When the level set~$\Omega_F(F(x^0))$ is bounded, \cref{asm:bound} is also satisfied.
            For example, this is the case when~$F_i$ is strongly convex for at least one~$i$.
    \end{enumerate}
\end{remark}
\begin{theorem} \label{thm:pgm_rate_convex}
	Assume that~$F_i$ is convex for all~$i = 1, \dots, m$.
    Under \cref{asm:bound}, \cref{alg:pgm_MO} generates a sequence~$\{ x^k \}$ such that
    \[
        u_0(x^k) \le \frac{\ell R}{2 k} \forallcondition{k \ge 1}.
    \]
\end{theorem}
\begin{proof}
	From \cref{lem:without_line_lem} and the convexity of~$f_i$ and~$g_i$, for all~$x \in \setR^n$ we have
    \[
		\sum_{i = 1}^m \lambda_i^k \left(F_i(x^{k + 1}) - F_i(x)\right) \le \frac{\ell}{2} \left( \norm*{x^k - x}_2^2 - \norm*{x^{k + 1} - x}_2^2 \right)
    .\]
    Adding up the above inequality from~$k = 0$ to~$k = \hat{k}$, we obtain
    \[
        \begin{split}
            \sum_{k = 0}^{\hat{k}} \sum_{i = 1}^m \lambda_i^k \left(F_i(x^{k + 1}) - F_i(x)\right) &\le \frac{\ell}{2} \left( \norm*{ x^0 - x}_2^2 - \norm*{ x^{\hat{k} + 1} - x}_2^2 \right) \\
        &\le \frac{\ell}{2} \norm*{x^0 - x}_2^2
        .\end{split}
    \]
    Since~$F_i(x^{\hat{k} + 1}) \le F_i(x^{k + 1})$ for all~$k \le \hat{k}$ (see~\cref{eq:nonincreasing}), we get
    \[
        \sum_{k = 0}^{\hat{k}} \sum_{i = 1}^m \lambda_i^k \left(F_i(x^{\hat{k} + 1}) - F_i(x)\right) \le \frac{\ell}{2} \norm*{x^0 - x}_2^2.
    \]
    Let~$\bar{\lambda}_i^{\hat{k}} \coloneqq \sum_{k = 0}^{\hat{k}} \lambda_i^k / (\hat{k} + 1)$. Then, it follows that
	\[
        \sum_{i = 1}^m \bar{\lambda}_i^{\hat{k}} \left(F_i(x^{\hat{k} + 1}) - F_i(x)\right)  \le \frac{\ell}{2 (\hat{k} + 1)} \norm*{x^0 - x}_2^2.
	\]
    Since~$\bar{\lambda}_i^{\hat{k}} \ge 0$ and $\sum_{i = 1}^m \bar{\lambda}_i^{\hat{k}} = 1$, we see that
	\[
        \min_{i = 1, \dots, m} \left( F_i(x^{\hat{k} + 1}) - F_i(x) \right) \le \frac{\ell}{2 (\hat{k} + 1)} \norm*{x^0 - x}_2^2.
	\]
    Therefore, we get
	\[
        \sup_{F^\ast \in F(X^\ast \cap \Omega_F(F(x^0)))} \inf_{x \in F^{-1}(\{F^\ast\})} \min_{i = 1, \dots, m} \left( F_i(x^{\hat{k} + 1}) - F_i(x) \right) \le \frac{\ell R}{2 (\hat{k} + 1)}.
	\]
    Thus, we obtain
    \[
        \sup_{F^\ast \in F(X^\ast \cap \Omega_F(F(x^0)))} \min_{i = 1, \dots, m} \left( F_i(x^{\hat{k} + 1}) - F_i^\ast \right) \le \frac{\ell R}{2 (\hat{k} + 1)},
    \]
    which gives
    \begin{equation} \label{eq:pareto level bound}
        \sup_{x \in X^\ast \cap \Omega_F(F(x^0))} \min_{i = 1, \dots, m} \left( F_i(x^{\hat{k} + 1}) - F_i(x) \right) \le \frac{\ell R}{2 (\hat{k} + 1)}.
    \end{equation}
    Now, the inequality~$F_i(x^k) \le F_i(x^0)$ from~\cref{eq:nonincreasing} gives
    \[
        \begin{split}
            \MoveEqLeft[.5] \sup_{x \in \Omega_F(F(x^0))} \min_{i = 1, \dots, m} \left( F_i(x^{\hat{k} + 1}) - F_i(x) \right) \\
        &= \sup_{x \in \Omega_F(F(x^{\hat{k} + 1}))} \min_{i = 1, \dots, m} \left( F_i(x^{\hat{k} + 1}) - F_i(x) \right)
        ,\end{split}
    \]
    so we have
    \begin{equation} \label{eq:level merit}
        \begin{split}
            \MoveEqLeft \sup_{x \in \Omega_F(F(x^0))} \min_{i = 1, \dots, m} \left( F_i(x^{\hat{k} + 1}) - F_i(x) \right) \\
        &= \sup_{x \in \setR^n} \min_{i = 1, \dots, m} \left( F_i(x^{\hat{k} + 1}) - F_i(x) \right).
        \end{split}
    \end{equation}
    Moreover, from the assumption that for all~$x \in \Omega_F(F(x^0))$ there exists~$x^\ast \in X^\ast$ such that~$F(x^\ast) \le F(x)$, it follows that
    \begin{equation} \label{eq:pareto level}
        \begin{split}
            \MoveEqLeft \sup_{x \in X^\ast \cap \Omega_F(F(x^0))} \min_{i = 1, \dots, m} \left( F_i(x^{\hat{k} + 1}) - F_i(x) \right) \\
            &= \sup_{x \in \Omega_F(F(x^0))} \min_{i = 1, \dots, m} \left( F_i(x^{\hat{k} + 1}) - F_i(x) \right)
        .\end{split}
    \end{equation}
    Finally, from~\cref{eq:pareto level bound,eq:level merit,eq:pareto level} we conclude that
    \[
        u_0(x^{\hat{k} + 1}) \coloneqq \sup_{x \in \setR^n} \min_{i = 1, \dots, m} \left( F_i(x^{\hat{k} + 1}) - F_i(x) \right) \le \frac{\ell R}{2 (\hat{k} + 1)}.
    \]
\end{proof}

\end{document}
