\documentclass[../../main]{subfiles}

\begin{document}
\subsection{The convex case}
This subsection assumes the convexity of the objective functions of~\zcref{eq:composite_MO}.
More precisely, let us suppose the following.
\begin{assumption} \zlabel{asm:pgm_convexity}
    Let~$f_i$ and~$g_i$ be~$\mu_{f_i}$-convex and~$\mu_{g_i}$-convex, respectively, with~$\mu_{f_i} \in \setR$ and~$\mu_{g_i} \ge 0$ for~$i = 1, \dots, m$.
    Write~$\mu_f \coloneqq \min_{i = 1, \dots, m} \mu_{f_i}$ and~$\mu_g \coloneqq \min_{i = 1, \dots, m} \mu_{g_i}$.
    Then, we suppose that~$\mu_f + \mu_g \ge 0$.
\end{assumption}

Then, we can show the following recursive relation, which is helpful for the subsequent discussion.
\begin{lemma} \zlabel{thm:pgm_recursive}
    Under \zcref{asm:pgm_convexity}, the following three statements hold:
    \begin{enumerate}
        \item \zcref{alg:pgm_MO} with the \zcref[ref=title,noname]{sec:pgm:algorithm:armijo} generates~$\set*{x^k}$ such that
              \begin{multline}
                  \min_{i = 1, \dots, m} \left[F_i\left(x^{k + 1}\right) - F_i(x)\right] + \frac{\rho (1 + \alpha \mu_g)}{2 \alpha} \norm*{x^{k + 1} - x}_2^2 \\
                  \le (1 - \rho s_k) \min_{i = 1, \dots, m} \left[F_i\left(x^k\right) - F_i(x)\right] + \frac{\rho [1 + \alpha \mu_g - \alpha s_k (\mu_f + \mu_g)]}{2 \alpha} \norm*{x^k - x}_2^2
              \end{multline}
              for all~$x \in \setR^n$ and~$k \ge 0$. \zlabel{thm:pgm_recursive:armijo}
        \item With the \zcref[ref=title,noname]{sec:pgm:algorithm:sdc}, the sequence~$\set*{x^k}$ generated by \zcref{alg:pgm_MO} satisfies
              \begin{equation}
                  \min_{i = 1, \dots, m} \left[F_i\left(x^{k + 1}\right) - F_i(x)\right] + \frac{1 + \alpha_k \mu_g}{2 \alpha_k} \norm*{x^{k + 1} - x}_2^2 \le \frac{1 - \alpha_k \mu_f}{2 \alpha_k} \norm*{x^k - x}_2^2
              \end{equation}
              for all~$x \in \setR^n$ and~$k \ge 0$. \zlabel{thm:pgm_recursive:sdc}
        \item Let~$\set*{x^k}$ be generated by \zcref{alg:pgm_MO} with the \zcref[ref=title,noname]{sec:pgm:algorithm:constant}.
              Then, we have
              \begin{equation}
                  \min_{i = 1, \dots, m} \left[F_i\left(x^{k + 1}\right) - F_i(x)\right] + \frac{1 + \alpha \mu_g}{2 \alpha} \norm*{x^{k + 1} - x}_2^2 \le \frac{1 - \alpha \mu_f}{2 \alpha} \norm*{x^k - x}_2^2
              \end{equation}
              for all~$x \in \setR^n$ and~$k \ge 0$. \zlabel{thm:pgm_recursive:constant}
    \end{enumerate}
\end{lemma}
\begin{proof}
    \zcref[S]{thm:pgm_recursive:armijo}:
    Let~$x \in \setR^n$.
    \zcref[S]{eq:pgm_armijo_step,eq:pgm_armijo_rule} give
    \begin{equation}
        \min_{i = 1, \dots, m} \left[F_i\left(x^{k + 1}\right) - F_i(x)\right] \le \min_{i = 1, \dots, m} \left[F_i\left(x^k\right) - F_i(x)\right] + \rho s_k \left[ \theta_\alpha\left(x^k\right) - \frac{1}{2 \alpha} \norm*{z^k - x^k}_2^2 \right]
        .\end{equation}
    Let~$\lambda\left(x^k\right) \in \simplex^m$ such that~$\lambda_j\left(x^k\right) = 0$ for any~$j \in \mathcal{I}_\alpha\left(x^k\right)$, where~$\simplex^m$ and~$\mathcal{I}_\alpha$ are defined by~\zcref{eq:simplex,eq:reg_lin_gap_MO_active}, respectively.
    Then,~\zcref{eq:pgm_optimal_function} and \zcref{alg:pgm_MO:update} of \zcref{alg:pgm_MO} yield
    \begin{align}
        \MoveEqLeft \min_{i = 1, \dots, m} \left[F_i\left(x^{k + 1}\right) - F_i(x)\right] - \min_{i = 1, \dots, m} \left[F_i\left(x^k\right) - F_i(x)\right]                 \\
         & \le \rho s_k \sum_{i = 1}^{m} \lambda_i\left(x^k\right) \left[ \innerp*{\nabla f_i\left(x^k\right)}{z^k - x^k} + g_i\left(z^k\right) - g_i\left(x^k\right) \right]
        .\end{align}
    Since~$\lambda\left(x^k\right) \in \simplex^m$, we have
    \begin{align}
        \MoveEqLeft \min_{i = 1, \dots, m} \left[F_i\left(x^{k + 1}\right) - F_i(x)\right] - (1 - \rho s_k) \min_{i = 1, \dots, m}\left[F_i\left(x^k\right) - F_i(x)\right]                     \\
         & \le \rho s_k \sum_{i = 1}^{m} \lambda_i\left(x^k\right) \left[ \innerp*{\nabla f_i\left(x^k\right)}{z^k - x^k} + f_i\left(x^k\right) - f_i(x) + g_i\left(z^k\right) - g_i(x) \right]
        .\end{align}
    It follows from the~$\mu_{f_i}$-convexity of~$f_i$ that
    \begin{align}
        \MoveEqLeft \min_{i = 1, \dots, m} \left[F_i\left(x^{k + 1}\right) - F_i(x)\right] - (1 - \rho s_k) \min_{i = 1, \dots, m}\left[F_i\left(x^k\right) - F_i(x)\right]                                   \\
         & \le \rho s_k \sum_{i = 1}^{m} \lambda_i\left(x^k\right) \left[ \innerp*{\nabla f_i\left(x^k\right)}{z^k - x} + g_i\left(z^k\right) - g_i(x) - \frac{\mu_{f_i}}{2} \norm*{x^k - x}_2^2 \right]      \\
         & \le \rho s_k \sum_{i = 1}^{m} \lambda_i\left(x^k\right) \left[ \innerp*{\nabla f_i\left(x^k\right)}{z^k - x} + g_i\left(z^k\right) - g_i(x) \right] - \frac{\rho s_k \mu_f}{2} \norm*{x^k - x}_2^2
        ,\end{align}
    where the second inequality comes from the fact that~$\lambda\left(x^k\right) \in \simplex^m$ and~$\mu_f = \min_{i = 1, \dots, m} \mu_{f_i}$.
    Without loss of generality,~\zcref{eq:reg_lin_gap_optimality}, $g_i$'s~$\mu_{g_i}$-convexity, and the fact that~$\mu_g = \min_{i = 1, \dots, m} \mu_{g_i}$ lead to
    \begin{align}
        \MoveEqLeft \min_{i = 1, \dots, m} \left[F_i\left(x^{k + 1}\right) - F_i(x)\right] - (1 - \rho s_k) \min_{i = 1, \dots, m} \left[F_i\left(x^k\right) - F_i(x)\right]       \\
        \le{} & - \frac{\rho s_k}{\alpha} \innerp*{z^k - x^k}{z^k - x} - \frac{\rho s_k \mu_f}{2} \norm*{x^k - x}_2^2 - \frac{\rho s_k \mu_g}{2} \norm*{z^k - x}_2^2               \\
        ={}   & - \frac{\rho}{\alpha} \innerp*{x^{k + 1} - x^k}{x^k - x} - \frac{\rho}{\alpha s_k} \norm*{x^{k + 1} - x^k}_2^2                                                     \\
              & - \frac{\rho s_k \mu_f}{2} \norm{x^k - x}_2^2 - \frac{\rho s_k \mu_g}{2} \norm*{\frac{1}{s_k} (x^{k + 1} - x^k) + x^k - x}_2^2                                     \\
        ={}   & - \frac{\rho (1 + \alpha \mu_g)}{\alpha} \innerp*{x^{k + 1} - x^k}{x^k - x} - \frac{\rho (2 s_k + \alpha \mu_g)}{2 \alpha s_k} \norm*{x^{k + 1} - x^k}_2^2         \\
              & - \frac{\rho s_k (\mu_f + \mu_g)}{2} \norm*{x^k - x}_2^2                                                                                                           \\
        ={}   & - \frac{\rho (1 + \alpha \mu_g)}{2 \alpha} \left( \norm*{x^{k + 1} - x}_2^2 - \norm*{x^k - x}_2^2 - \norm*{x^{k + 1} - x^k}_2^2 \right)                            \\
              & - \frac{\rho (2 s_k + \alpha \mu_g)}{2 \alpha s_k} \norm*{x^{k + 1} - x^k}_2^2 - \frac{\rho s_k (\mu_f + \mu_g)}{2} \norm*{x^k - x}_2^2                            \\
        \le{} & - \frac{\rho (1 + \alpha \mu_g)}{2 \alpha} \left( \norm*{x^{k + 1} - x}_2^2 - \norm*{x^k - x}_2^2 \right) - \frac{\rho s_k (\mu_f + \mu_g)}{2} \norm*{x^k - x}_2^2
        ,\end{align}
    where the first equality follows from \zcref{alg:pgm_MO:update} of \zcref{alg:pgm_MO}, and the second inequality holds since~$s_k \in (0, 1]$.
    The above inequality is equivalent to the desired one.

    \zcref[S]{thm:pgm_recursive:sdc}:
    Let~$x \in \setR^n$.
    \zcref[S]{eq:pgm_sdc,eq:pgm_sdc_step} yield
    \begin{equation}
        \min_{i = 1, \dots, m} \left[F_i\left(x^{k + 1}\right) - F_i(x)\right] \le \min_{i = 1, \dots, m} \left[F_i\left(x^k\right) - F_i(x)\right] + \theta_{\alpha_k}\left(x^k\right)
        .\end{equation}
    For the proof of \zcref{thm:pgm_recursive:armijo}, by replacing~$\alpha$ with~$\alpha_k$,~$\rho$ with~$1$,~$s_k$ with~$1$, and~$z^k$ with~$x^{k + 1}$, and adding~$(1 / 2 \alpha_k) \norm{x^{k + 1} - x^k}_2^2$ to the right-hand side, we obtain the desired inequality.

    \zcref[S]{thm:pgm_recursive:constant}:
    This claim is clear from~\zcref{thm:pgm_recursive:sdc}.
\end{proof}

Now, we show that~$\set*{u_\infty\left(x^k\right)}$ converges to zero with rate~$O(1 / k)$ with \zcref{alg:pgm_MO} under the following assumption.
\begin{assumption} \zlabel{asm:bound}
    There exists a bounded set~$\Omega \subseteq \setR^n$ such that for all~$x \in \level_{F(x^0)}(F)$ with~$\level_{F(x^0)}$ given by~\zcref{eq:level}, some~$z \in \Omega$ satisfies~$F(z) \le F(x)$.
    %    Then, for all~$x \in \level_{F(x^0)}(F)$, there exists~$x^{\text{WP}} \in X^{\text{WP}}$ such that~$F\left(x^{\text{WP}}\right) \le F(x)$ and
    %    \begin{equation} \label{eq:R}
    %        R \coloneqq \sup_{F^{\text{WP}} \in F\left(X^{\text{WP}} \cap \level_{F\left(x^0\right)}(F)\right)} \inf_{x^{\text{WP}} \in F^{-1}\left(\set*{F^{\text{WP}}}\right)} \norm*{x^{\text{WP}} - x^0}_2^2 < \infty
    %    .\end{equation}
\end{assumption}
\begin{remark}
    \begin{enumerate}
        \item In single-objective cases, if the optimization problem has at least one optimal solution~$x^*$, then~$\Omega = \set{x^*}$ satisfies \zcref{asm:bound}.
        \item When the level set~$\level_{F(x^0)}(F)$ is bounded, \zcref{asm:bound} is also satisfied.
              For example, this is the case when~$F_i$ is strongly convex for at least one~$i$.
    \end{enumerate}
\end{remark}
\begin{theorem} \zlabel{thm:pgm_rate_convex}
    Under \zcref{asm:pgm_existence_lb,asm:pgm_Lipschitz_gradient,asm:pgm_convexity}, \zcref{alg:pgm_MO} with the stepsize selection rule given in any of \zcref{sec:pgm:algorithm:armijo,sec:pgm:algorithm:sdc,sec:pgm:algorithm:constant} generates a sequence~$\set*{x^k}$ converging to~$x^*$ such that~$\set*{\min_{i = 1, \dots, m} \left[F_i\left(x^k\right) - F_i(x^*)\right]}$ is summable, in particular
    \begin{equation}
        \liminf_{k \to \infty} (k \log k) \min_{i = 1, \dots, m} \left[F_i\left(x^k\right) - F_i(x^*)\right] = 0
    \end{equation}
    and
    \begin{equation}
        \limsup_{k \to \infty} k \min_{i = 1, \dots, m} \left[F_i\left(x^k\right) - F_i(x^*)\right] < + \infty
        .\end{equation}
    Supposing \zcref{asm:bound} additionally, we also have
    \begin{equation}
        u_\infty\left(x^k\right) = O(1 / k) \forallcondition{k \ge 1}
    \end{equation}
    with~$O \colon [0, + \infty) \to \setR$ satisfying~$\limsup_{t \to 0} O(t) / t < + \infty$ and~$u_\infty$ given by~\zcref{eq:gap_MO}.
\end{theorem}
\begin{proof}
    Let~$T \coloneqq \Set*{x \in \setR^n}{F(x) \le F\left(x^k\right), k = 0, 1, \dots}$.
    From \zcref{thm:pgm_convergence},~$\set*{x^k}$ converges to~$x^* \in T$.

    We first prove the claim for the~\zcref[ref=title,noname]{sec:pgm:algorithm:armijo}.
    Since~$\mu_g \ge 0$ and~$\mu_f + \mu_g \ge 0$, \zcref{thm:pgm_recursive:armijo,thm:pgm_armijo_stepsize_lb} yield
    \begin{multline}
        \min_{i = 1, \dots, m} \left[F_i\left(x^{k + 1}\right) - F_i(x^*)\right] + \frac{\rho}{2 \alpha} \norm*{x^{k + 1} - x^*}_2^2 \\
        \le \left(1 - \rho s_{\min}\right) \min_{i = 1, \dots, m} \left[F_i\left(x^k\right) - F_i(x^*)\right] + \frac{\rho}{2 \alpha} \norm*{x^k - x^*}_2^2
    \end{multline}
    Adding up the above inequality from~$k = 0$ to~$k = \ell$, we obtain
    \begin{align}
        \MoveEqLeft \rho s_{\min} \sum_{k = 0}^{\ell} \min_{i = 1, \dots, m} \left[F_i\left(x^k\right) - F_i(x^*)\right]                             \\
        \le{} & \min_{i = 1, \dots, m} \left[F_i(x^0) - F_i(x^*)\right] + \frac{\rho}{2 \alpha} \norm*{x^0 - x^*}_2^2                                \\
              & - \min_{i = 1, \dots, m} \left[F_i\left(x^{\ell + 1}\right) - F_i(x^*)\right] - \frac{\rho}{2 \alpha} \norm*{x^{\ell + 1} - x^*}_2^2 \\
        \le{} & \min_{i = 1, \dots, m} \left[F_i(x^0) - F_i(x^*)\right] + \frac{\rho}{2 \alpha} \norm*{x^0 - x^*}_2^2
        ,\end{align}
    which means the summability of~$\set*{\min_{i = 1, \dots, m} \left[F_i\left(x^k\right) - F_i(x^*)\right]}$.
    We now suppose \zcref{asm:bound}.
    Since~\zcref{eq:pgm_nonincrementality} implies~$F\left(x^\ell\right) \le F\left(x^k\right)$ for all~$k = 0, \dots, \ell$, the above inequality holds even if we replace~$x^*$ by~$z \in \Omega$ such that~$F(z) \le F\left(x^\ell\right)$.
    Again using the relation~$F\left(x^\ell\right) \le F\left(x^k\right)$, we have
    \begin{multline}
        \rho s_{\min} (\ell + 1) \min_{i = 1, \dots, m} \left[ F_i(x^\ell) - F_i(z) \right] \le \min_{i = 1, \dots, m} \left[F_i(x^0) - F_i(z)\right] + \frac{\rho}{2 \alpha} \norm*{x^0 - z}_2^2
        .\end{multline}
    Therefore, we get
    \begin{equation}
        \min_{i = 1, \dots, m} \left[ F_i(x^\ell) - F_i(z) \right] \le \frac{\displaystyle \min_{i = 1, \dots, m} \left[F_i(x^0) - F_i(z)\right] + \frac{\rho}{2 \alpha} \norm*{x^0 - z}_2^2
        }{\rho s_{\min} (\ell + 1)}
        .\end{equation}
    Since~$x^\ell \in \level_{F(x^0)}(F)$, \zcref[S]{asm:bound} implies that
    \begin{equation}
        u_\infty(x^\ell) = \sup_{z \in \setR^n} \min_{i = 1, \dots, m} \left[ F_i(x^\ell) - F_i(z) \right] = \sup_{z \in \Omega} \min_{i = 1, \dots, m} \left[ F_i(x^\ell) - F_i(z) \right]
    \end{equation}
    Due to the boundedness of~$\Omega$, we conclude that~$u_\infty(x^\ell) = O(1 / \ell)$.

    We can likewise show the claim for the \zcref[ref=title,noname]{sec:pgm:algorithm:sdc} by~\zcref{thm:pgm_recursive:sdc,thm:pgm_sdc_stepsize_lb} and for the \zcref[ref=title,noname]{sec:pgm:algorithm:constant} by~\zcref{thm:pgm_recursive:constant}.
\end{proof}

\end{document}
