\documentclass[../../main]{subfiles}

\begin{document}
\subsection{The convex case}
This subsection assumes the convexity of the objective functions of~\zcref{eq:composite_MO}.
More precisely, let us suppose the following.
\begin{assumption} \zlabel{asm:pgm_convexity}
    Let~$f_i$ and~$g_i$ be~$\mu_{f_i}$-convex and~$\mu_{g_i}$-convex, respectively, with~$\mu_{f_i} \in \setR$ and~$\mu_{g_i} \ge 0$ for~$i = 1, \dots, m$.
    Write~$\mu_f \coloneqq \min_{i = 1, \dots, m} \mu_{f_i}$ and~$\mu_g \coloneqq \min_{i = 1, \dots, m} \mu_{g_i}$.
    Then, we suppose that~$\mu_f + \mu_g \ge 0$.
\end{assumption}

Then, we can show the following recursive relation, which is useful for the subsequent discussion.
\begin{lemma} \zlabel{thm:pgm_recursive}
    Under \zcref{asm:pgm_convexity}, the following three statements hold:
    \begin{enumerate} 
        \item \zcref{alg:pgm_MO} with the \zcref[ref=title,noname]{sec:pgm:algorithm:armijo} generates~$\set{x^k}$ such that
            \begin{multline}
                \min_{i = 1, \dots, m} [F_i(x^{k + 1}) - F_i(x)] + \frac{\rho (1 + \alpha \mu_g)}{2 \alpha} \norm*{x^{k + 1} - x}_2^2 \\
                \le (1 - \rho s_k) \min_{i = 1, \dots, m} [F_i(x^k) - F_i(x)] + \frac{\rho [1 + \alpha \mu_g - \alpha s_k (\mu_f + \mu_g)]}{2 \alpha} \norm*{x^k - x}_2^2
            \end{multline}
            for all~$x \in \setR^n$ and~$k \ge 0$. \zlabel{thm:pgm_recursive:armijo}
        \item With the \zcref[ref=title,noname]{sec:pgm:algorithm:sdc}, the sequence~$\set{x^k}$ generated by \zcref{alg:pgm_MO} satisfies 
            \begin{equation}
                \min_{i = 1, \dots, m} [F_i(x^{k + 1}) - F_i(x)] + \frac{1 + \alpha_k \mu_g}{2 \alpha_k} \norm*{x^{k + 1} - x}_2^2 \le \frac{1 - \alpha_k \mu_f}{2 \alpha_k} \norm*{x^k - x}_2^2
            \end{equation}
            for all~$x \in \setR^n$ and~$k \ge 0$. \zlabel{thm:pgm_recursive:sdc}
        \item Let~$\set{x^k}$ be generated by \zcref{alg:pgm_MO} with the \zcref[ref=title,noname]{sec:pgm:algorithm:constant}.
            Then, we have
            \begin{equation}
                \min_{i = 1, \dots, m} [F_i(x^{k + 1}) - F_i(x)] + \frac{1 + \alpha \mu_g}{2 \alpha} \norm*{x^{k + 1} - x}_2^2 \le \frac{1 - \alpha \mu_f}{2 \alpha} \norm*{x^k - x}_2^2
            \end{equation}
            for all~$x \in \setR^n$ and~$k \ge 0$. \zlabel{thm:pgm_recursive:constant}
    \end{enumerate}
\end{lemma}
\begin{proof}
    \zcref[S]{thm:pgm_recursive:armijo}:
    Let~$x \in \setR^n$.
    \zcref[S]{eq:pgm_armijo_step,eq:pgm_armijo_rule} give
    \begin{equation}
        \min_{i = 1, \dots, m} [F_i(x^{k + 1}) - F_i(x)] \le \min_{i = 1, \dots, m} [F_i(x^k) - F_i(x)] + \rho s_k \left[ \theta_\alpha(x^k) - \frac{1}{2 \alpha} \norm{z^k - x^k}_2^2 \right]
    .\end{equation} 
    Let~$\lambda(x^k) \in \simplex^m$ such that~$\lambda_j(x^k) = 0$ for any~$j \in \mathcal{I}_\alpha(x^k)$, where~$\simplex^m$ and~$\mathcal{I}_\alpha$ are defined by~\zcref{eq:simplex,eq:pgm_active_set}, respectively.
    Then,~\zcref{eq:pgm_optimal_function} and \zcref{alg:pgm_MO:update} of \zcref{alg:pgm_MO} yield
    \begin{align}
        \MoveEqLeft \min_{i = 1, \dots, m} [F_i(x^{k + 1}) - F_i(x)] - \min_{i = 1, \dots, m} [F_i(x^k) - F_i(x)] \\
        &\le \rho s_k \sum_{i = 1}^{m} \lambda_i(x^k) \left[ \innerp*{\nabla f_i(x^k)}{z^k - x^k} + g_i(z^k) - g_i(x^k) \right]
    .\end{align} 
    Since~$\lambda(x^k) \in \simplex^m$, we have
    \begin{align}
        \MoveEqLeft \min_{i = 1, \dots, m} [F_i(x^{k + 1}) - F_i(x)] - (1 - \rho s_k) \min_{i = 1, \dots, m}[F_i(x^k) - F_i(x)] \\
             &\le \rho s_k \sum_{i = 1}^{m} \lambda_i(x^k) \left[ \innerp*{\nabla f_i(x^k)}{z^k - x^k} + f_i(x^k) - f_i(x) + g_i(z^k) - g_i(x) \right]
    .\end{align} 
    It follows from the~$\mu_{f_i}$-convexity of~$f_i$ that
    \begin{align}
        \MoveEqLeft \min_{i = 1, \dots, m} [F_i(x^{k + 1}) - F_i(x)] - (1 - \rho s_k) \min_{i = 1, \dots, m}[F_i(x^k) - F_i(x)] \\
             &\le \rho s_k \sum_{i = 1}^{m} \lambda_i(x^k) \left[ \innerp*{\nabla f_i(x^k)}{z^k - x} + g_i(z^k) - g_i(x) - \frac{\mu_{f_i}}{2} \norm*{x^k - x}_2^2 \right] \\
             &\le \rho s_k \sum_{i = 1}^{m} \lambda_i(x^k) \left[ \innerp*{\nabla f_i(x^k)}{z^k - x} + g_i(z^k) - g_i(x) \right] - \frac{\rho s_k \mu_f}{2} \norm*{x^k - x}_2^2
    ,\end{align} 
    where the second inequality comes from the fact that~$\lambda(x^k) \in \simplex^m$ and~$\mu_f = \min_{i = 1, \dots, m} \mu_{f_i}$.
    Without loss of generality,~\zcref{eq:pgm_optimality}, $g_i$'s~$\mu_{g_i}$-convexity, and the fact that~$\mu_g = \min_{i = 1, \dots, m} \mu_{g_i}$ lead to
    \begin{align}
        \MoveEqLeft \min_{i = 1, \dots, m} [F_i(x^{k + 1}) - F_i(x)] - (1 - \rho s_k) \min_{i = 1, \dots, m} [F_i(x^k) - F_i(x)] \\
        \le{}& - \frac{\rho s_k}{\alpha} \innerp*{z^k - x^k}{z^k - x} - \frac{\rho s_k \mu_f}{2} \norm*{x^k - x}_2^2 - \frac{\rho s_k \mu_g}{2} \norm*{z^k - x}_2^2 \\
        ={}& - \frac{\rho}{\alpha} \innerp*{x^{k + 1} - x^k}{x^k - x} - \frac{\rho}{\alpha s_k} \norm*{x^{k + 1} - x^k}_2^2 \\
             &- \frac{\rho s_k \mu_f}{2} \norm{x^k - x}_2^2 - \frac{\rho s_k \mu_g}{2} \norm*{\frac{1}{s_k} (x^{k + 1} - x^k) + x^k - x}_2^2 \\
        ={}& - \frac{\rho (1 + \alpha \mu_g)}{\alpha} \innerp*{x^{k + 1} - x^k}{x^k - x} - \frac{\rho (2 s_k + \alpha \mu_g)}{2 \alpha s_k} \norm*{x^{k + 1} - x^k}_2^2  \\
             &- \frac{\rho s_k (\mu_f + \mu_g)}{2} \norm*{x^k - x}_2^2 \\
        ={}& - \frac{\rho (1 + \alpha \mu_g)}{2 \alpha} \left( \norm*{x^{k + 1} - x}_2^2 - \norm*{x^k - x}_2^2 - \norm*{x^{k + 1} - x^k}_2^2 \right) \\
           &- \frac{\rho (2 s_k + \alpha \mu_g)}{2 \alpha s_k} \norm*{x^{k + 1} - x^k}_2^2 - \frac{\rho s_k (\mu_f + \mu_g)}{2} \norm*{x^k - x}_2^2 \\
        \le{}& - \frac{\rho (1 + \alpha \mu_g)}{2 \alpha} \left( \norm*{x^{k + 1} - x}_2^2 - \norm*{x^k - x}_2^2 \right) - \frac{\rho s_k (\mu_f + \mu_g)}{2} \norm*{x^k - x}_2^2
    ,\end{align}
    where the first equality follows from \zcref{alg:pgm_MO:update} of \zcref{alg:pgm_MO}, and the second inequality holds since~$s_k \in (0, 1]$.
    The above inequality is equivalent to the desired one.

    \zcref[S]{thm:pgm_recursive:sdc}:
    Let~$x \in \setR^n$.
    \zcref[S]{eq:pgm_sdc,eq:pgm_sdc_step} yield
    \begin{equation}
        \min_{i = 1, \dots, m} [F_i(x^{k + 1}) - F_i(x)] \le \min_{i = 1, \dots, m} [F_i(x^k) - F_i(x)] + \theta_{\alpha_k}(x^k)
    .\end{equation}
    For the proof of \zcref{thm:pgm_recursive:armijo}, by replacing~$\alpha$ with~$\alpha_k$,~$\rho$ with~$1$,~$s_k$ with~$1$, and~$z^k$ with~$x^{k + 1}$, and adding~$(1 / 2 \alpha_k) \norm{x^{k + 1} - x^k}_2^2$ to the right-hand side, we obtain the desired inequality.

    \zcref[S]{thm:pgm_recursive:constant}:
    This claim is clear from~\zcref{thm:pgm_recursive:sdc}.
\end{proof}

Now, we show that~$\set{u_0(x^k)}$ converges to zero with rate~$O(1 / k)$ with \zcref{alg:pgm_MO} under the following assumption.
\begin{assumption} \zlabel{asm:bound}
    Let~$X^{\text{WP}}$ be the set of weakly Pareto optimal points for~\zcref{eq:composite_MO} and~$\level$ be defined by~\zcref{eq:level}.
    Then, for all~$x \in \level_{F(x^0)}(F)$, there exists~$x^{\text{WP}} \in X^{\text{WP}}$ such that~$F(x^{\text{WP}}) \le F(x)$ and
    \begin{equation} \label{eq:R}
        R \coloneqq \sup_{F^{\text{WP}} \in F\left(X^{\text{WP}} \cap \level_{F(x^0)}(F)\right)} \inf_{x^{\text{WP}} \in F^{-1}(\set{F^{\text{WP}}})} \norm*{x^{\text{WP}} - x^0}_2^2 < \infty
    .\end{equation}
\end{assumption}
\begin{remark}
    \begin{enumerate}
        \item In single-objective cases, \zcref{asm:bound} is valid if the optimization problem has at least one optimal solution; when~$m = 1$,~$X^{\text{WP}}$ coincides with the optimal solution set, and the equality~$X^{\text{WP}} \cap \level_{F(x^0)}(F) = X^{\text{WP}}$ holds, so we have~$R = \inf_{x \in X^{\text{WP}}} \norm*{x - x^0}_2^2 < \infty$.
        \item When the level set~$\level_{F(x^0)}(F)$ is bounded, \zcref{asm:bound} is also satisfied.
            For example, this is the case when~$F_i$ is strongly convex for at least one~$i$.
    \end{enumerate}
\end{remark}
\begin{theorem} \zlabel{thm:pgm_rate_convex}
    Under \zcref{asm:pgm_existence_lb,asm:pgm_Lipschitz_gradient,asm:pgm_convexity}, \zcref{alg:pgm_MO} with the stepesize selection rule given in any of \zcref{sec:pgm:algorithm:armijo,sec:pgm:algorithm:sdc,sec:pgm:algorithm:constant} generates a sequence~$\set{x^k}$ converging to~$x^*$ such that~$\set{\min_{i = 1, \dots, m} [F_i(x^k) - F_i(x^*)]}$ is summable, in particular
    \begin{equation}
        \liminf_{k \to \infty} (k \log k) \min_{i = 1, \dots, m} \left[F_i(x^k) - F_i(x^*)\right] = 0
    \end{equation}
    and
    \begin{equation}
        \limsup_{k \to \infty} k \min_{i = 1, \dots, m} \left[F_i(x^k) - F_i(x^*)\right] < + \infty
    .\end{equation}
    Supposing \zcref{asm:bound} additionaly, we also have
    \begin{equation}
        u_0(x^k) = O(1 / k) \forallcondition{k \ge 1}
    \end{equation}
    with~$O \colon [0, + \infty) \to \setR$ satisfying~$\limsup_{t \to 0} O(t) / t < + \infty$ and~$u_0$ given by~\zcref{eq:gap_MO}.
\end{theorem}
\begin{proof}
    Let~$T \coloneqq \Set{x \in \setR^n}{F(x) \le F(x^k), k = 0, 1, \dots}$.
    From \zcref{thm:pgm_convergence},~$\set{x^k}$ converges to~$x^* \in T$.

    We first prove the claim for the~\zcref[ref=title,noname]{sec:pgm:algorithm:armijo}.
    Since~$\mu_g \ge 0$ and~$\mu_f + \mu_g \ge 0$, \zcref{thm:pgm_recursive:armijo,thm:pgm_armijo_stepsize_lb} yield
    \begin{multline}
        \min_{i = 1, \dots, m} [F_i(x^{k + 1}) - F_i(x^*)] + \frac{\rho}{2 \alpha} \norm*{x^{k + 1} - x^*}_2^2 \\
        \le (1 - \rho s_{\min}) \min_{i = 1, \dots, m} [F_i(x^k) - F_i(x^*)] + \frac{\rho}{2 \alpha} \norm*{x^k - x^*}_2^2
    \end{multline}
    Adding up the above inequality from~$k = 0$ to~$k = \ell$, we obtain
    \begin{align}
        \MoveEqLeft \rho s_{\min} \sum_{k = 0}^{\ell} \min_{i = 1, \dots, m} \left[F_i(x^k) - F_i(x^*)\right] \\
        \le{}& \min_{i = 1, \dots, m} \left[F_i(x^0) - F_i(x^*)\right] + \frac{\rho}{2 \alpha} \norm*{x^0 - x^*}_2^2 \\
             &- \min_{i = 1, \dots, m} \left[F_i(x^{\ell + 1}) - F_i(x^*)\right] - \frac{\rho}{2 \alpha} \norm*{x^{\ell + 1} - x^*}_2^2 \\
        \le{}& \min_{i = 1, \dots, m} \left[F_i(x^0) - F_i(x^*)\right] + \frac{\rho}{2 \alpha} \norm*{x^0 - x^*}_2^2
    ,\end{align}
    which means the summability of~$\set{\min_{i = 1, \dots, m} [F_i(x^k) - F_i(x^*)]}$.
    We now suppose \zcref{asm:bound}.
    Since~\zcref{eq:pgm_nonincrementality} implies~$F(x^\ell) \le F(x^k)$ for all~$k = 0, \dots, \ell$, the above inequality holds even if we replace~$x^*$ by~$x^{\text{WP}} \in X^{\text{WP}} \cap \level_{F(x^\ell)}(F)$.
    Again using~\zcref{eq:pgm_nonincrementality}, we have
    \begin{multline}
        \rho s_{\min} (\ell + 1) \min_{i = 1, \dots, m} \left[ F_i(x^\ell) - F_i(x^{\text{WP}}) \right] \le \min_{i = 1, \dots, m} \left[F_i(x^0) - F_i(x^{\text{WP}})\right] \\
        + \frac{\rho}{2 \alpha} \norm*{x^0 - x^{\text{WP}}}_2^2
    .\end{multline}
    Therefore, we get
    \begin{equation}
        \min_{i = 1, \dots, m} \left[ F_i(x^\ell) - F_i(x^{\text{WP}}) \right] \le \frac{\displaystyle \min_{i = 1, \dots, m} \left[F_i(x^0) - F_i(x^{\text{WP}})\right] + \frac{\rho}{2 \alpha} \norm*{x^0 - x^{\text{WP}}}_2^2
}{\rho s_{\min} (\ell + 1)} 
    .\end{equation}
    Because~$F(x^\ell) \le F(x^0)$, it follows that~$\level_{F(x^\ell)} \subseteq \level_{F(x^0)}$, so~$x^{\text{WP}} \in X^{\text{WP}} \cap \level_{F(x^0)}(F)$.
    Thus, \zcref{asm:bound} lead to~$u_0(x^\ell) = O(1 / \ell)$.

    We can likewise show the claim for the \zcref[ref=title,noname]{sec:pgm:algorithm:sdc} by~\zcref{thm:pgm_recursive:sdc,thm:pgm_sdc_stepsize_lb} and for the \zcref[ref=title,noname]{sec:pgm:algorithm:constant} by~\zcref{thm:pgm_recursive:constant}.
\end{proof}

\end{document}
