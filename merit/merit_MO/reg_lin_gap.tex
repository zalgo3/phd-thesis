\documentclass[../../main]{subfiles}

\begin{document}
\subsection{A regularized and partially linearized gap function for composite multi-objective optimization} \zlabel{sec:merit:merit:reg_lin_gap}
Now, let us consider the composite case~\zcref{eq:composite_MO}.
Since they are generally non-convex, we can regard them as a relaxation of the assumptions of the previous subsection.
For~\zcref{eq:composite_MO}, we propose a regularized and partially linearized gap function~$w_\alpha \colon \setR^n \to \setR$ with a given~$\alpha > 0$ as follows:
\begin{equation} \label{eq:reg_lin_gap_MO}
    w_\alpha(x) \coloneqq \max_{y \in C} \min_{i = 1, \dots, m} \left[ \innerp{\nabla f_i(x)}{x - y} + g_i(x) - g_i(y) - \frac{\alpha}{2} \norm{x - y}_2^2 \right] 
.\end{equation} 
Like~$u_\alpha$, the convexity of~$g_i$ leads to the finiteness of~$w_\alpha$ and the existence of a unique solution that attains~$\max_{y \in C}$.
As the following remark shows,~$w_\alpha$ generalizes other kinds of merit functions.
\begin{remark} \zlabel{rem:reg_lin_gap}
    \begin{enumerate}
        \item When~$g_i = 0$,~$w_\alpha$ corresponds to the regularized gap function~\zcref{eq:reg_gap_SVVIw} for vector variational inequality. \zlabel{rem:reg_lin_gap:reg_gap_SVVIw}
        \item When~$f_i = 0$,~$w_\alpha$ matches~$u_\alpha$ defined by~\zcref{eq:reg_gap_MO}. \zlabel{rem:reg_lin_gap:reg_gap}
    \end{enumerate}
\end{remark}
As shown in the following theorem,~$w_\alpha$ is a merit function in the sense of Pareto stationarity.
\begin{theorem} \zlabel{thm:reg_lin_gap}
    Let~$w_\alpha$ be given by~\zcref{eq:reg_lin_gap_MO} for some~$\alpha > 0$.
    Then, we have~$w_\alpha(x) \ge 0$ for all~$x \in C$.
    Furthermore,~$x \in C$ is Pareto stationary for~\zcref{eq:composite_MO} if and only if~$w_\alpha(x) = 0$.
\end{theorem}
\begin{proof}
    We first show the nonnegativity of~$w_\alpha$ for all~$\alpha > 0$.
    Let~$x \in C$.
    The definition of~$w_\alpha$ gives
    \begin{align}
        w_\alpha(x) &= \sup_{y \in C} \min_{i = 1, \dots, m} \left[ \innerp{\nabla f_i(x)}{x - y} + g_i(x) - g_i(y) - \frac{\alpha}{2} \norm{x - y}_2^2 \right] \\
                    &\ge \min_{i = 1, \dots, m} \left[ \innerp{\nabla f_i(x)}{x - x} + g_i(x) - g_i(x) - \frac{\alpha}{2} \norm*{x - x}_2^2 \right] = 0
                .\end{align}

                Let us prove the second statement.
                Assume that~$w_\alpha(x) = 0$.
                Then, again using the definition of~$w_\alpha$, we get
                \begin{equation}
                    \min_{i = 1, \dots, m} \left[ \innerp{\nabla f_i(x)}{x - y} + g_i(x) - g_i(y) - \frac{\alpha}{2} \norm{x - y}_2^2 \right] \le 0 \forallcondition{y \in C}
                .\end{equation} 
                Let~$z \in C$ and~$t \in (0, 1)$.
                Since~$C \subseteq \setR^n$ is convex,~$x, z \in C$ implies~$x + t (z - x) \in C$.
                Therefore, by substituting~$y = x + t (z - x)$ into the above inequality, we obtain
                \begin{equation}
                    \min_{i = 1, \dots, m} \left[ - \innerp{\nabla f_i(x)}{t (z - x)} + g_i(x) - g_i(x + t (z - x)) - \frac{\alpha}{2} \norm{t (z - x)}^2 \right] \le 0
                .\end{equation} 
                Dividing both sides by~$\alpha$ yields
                \begin{equation}
                    \min_{i = 1, \dots, m} \left[ - \innerp{\nabla f_i(x)}{z - x} - \frac{g_i(x + t (z - x)) - g_i(x)}{t} - \frac{\alpha t}{2} \norm{z - x}^2 \right] \le 0
                .\end{equation} 
                By taking~$t \searrow 0$ and multiplying both sides by~$- 1$, we get
                \begin{equation}
                    \max_{i = 1, \dots, m} F_i'(x; z - x) \ge 0
                ,\end{equation} 
                which means that~$x$ is Pareto stationary for~\zcref{eq:composite_MO}.

                Now, we prove the converse by contrapositive.
                Suppose that~$w_\alpha(x) > 0$.
                Then, from the definition of~$w_\alpha$, there exists some~$y \in C$ such that
                \begin{equation}
                    \min_{i = 1, \dots, m} \left[ \innerp{\nabla f_i(x)}{x - y} + g_i(x) - g_i(y) - \frac{\alpha}{2} \norm{x - y}_2^2 \right] > 0
                .\end{equation} 
                Since~$g_i$ is convex, we obtain
                \begin{equation}
                    \min_{i = 1, \dots, m} \left[ \innerp{\nabla f_i(x)}{x - y} - g_i'(x; y - x) - \frac{\alpha}{2} \norm{x - y}_2^2 \right] > 0
                .\end{equation} 
                Thus, we have
                \begin{equation}
                    \max_{i = 1, \dots, m} F_i'(x; y - x) \le - \frac{\alpha}{2} \norm{x - y}_2^2 < 0
                ,\end{equation} 
                which shows that~$x$ is not Pareto stationary for~\zcref{eq:composite_MO}.
            \end{proof}
            While~$u_0$ and~$u_\alpha$ given by~\zcref{eq:gap_MO,eq:reg_gap_MO} are merit functions in the sense of weak Pareto optimality,~$w_\alpha$ defined by~\zcref{eq:reg_lin_gap_MO} is a merit function only in the sense of Pareto stationarity.
            As indicated by the following example, even if~$w_\alpha(x) = 0$,~$x$ is not necessarily weakly Pareto optimal for~\zcref{eq:composite_MO}.
            \begin{example}
                Consider the single-objective function~$F\colon \setR \to \setR$ defined by~$F(x) \coloneqq f(x) + g(x)$, where
                \begin{equation}
                    f(x) \coloneqq - x^2 \eqand g(x) \coloneqq 0
                ,\end{equation}
                and set~$C = \setR$.
                Then, we have
                \begin{equation}
                    w_\alpha(0) = \max_{y \in \setR} \left[ f'(0) (0 - y) + g(0) - g(y) - \frac{\alpha}{2} (y - 0)^2 \right] 
                    = \max_{y \in \setR} \left[ - \frac{\alpha}{2} y^2 \right] = 0
                ,\end{equation} 
                but $x = 0$ is not global minimal (i.e., weakly Pareto optimal) for~$F$.
            \end{example}

            We now define the optimal solution mapping~$W_\alpha \colon \setR^n \to \setR^n$ associated with~\zcref{eq:reg_lin_gap_MO} by
            \begin{equation} \label{eq:reg_lin_gap_MO_sol}
                W_\alpha(x) \coloneqq \argmax_{y \in C} \min_{i = 1, \dots, m} \left[ \innerp{\nabla f_i(x)}{x - y} + g_i(x) - g_i(y) - \frac{\alpha}{2} \norm{x - y}_2^2 \right] 
            .\end{equation} 
            From the optimality condition of the maximization problem associated with~\zcref{eq:reg_lin_gap_MO,eq:reg_lin_gap_MO_sol}, we obtain
            \begin{equation}
                \alpha [x - W_\alpha(x)] \in \conv_{i \in \mathcal{J}(x)} [ \nabla f_i(x) + \partial g_i(W_\alpha(x)) ] + N_C(W_\alpha(x)) \forallcondition{x \in C}
            ,\end{equation} 
            where~$N_C$ is the normal cone to the convex set~$C$ and
            \begin{equation}
                \mathcal{J}(x) \coloneqq \argmin_{i = 1, \dots, m} [\innerp{\nabla f_i(x)}{x - W_\alpha(x)} + g_i(x) - g_i(W_\alpha(x))]
            .\end{equation} 
            Therefore, for any~$x \in C$ there exists~$\pi(x)$ belonging to the unit $m$-simplex~$\simplex^m$ defined by~\zcref{eq:simplex} such that~$\pi_j(x) = 0$ for all~$j \notin \mathcal{J}(x)$ and
            \begin{equation} \label{eq:reg_lin_gap_optimality}
                \alpha \innerp{x - W_\alpha(x)}{z - W_\alpha(x)} \le \sum_{i = 1}^{m} \pi_i(x) [\innerp{\nabla f_i(x)}{z - W_\alpha(x)} + g_i(z) - g_i(W_\alpha(x))]
            \end{equation} 
            for all~$z \in C$.
            Particularly, if we substitute~$z = x$, we get
            \begin{equation}
                \alpha \norm{x - W_\alpha(x)}_2^2 \le w_\alpha(x) + \frac{\alpha}{2} \norm{x - W_\alpha(x)}_2^2
            ,\end{equation}
            which reduces to
            \begin{equation} \label{eq:reg_lin_gap_MO_LB}
                w_\alpha(x) \ge \frac{\alpha}{2} \norm{x - W_\alpha(x)}_2^2
            .\end{equation}

            We can also show the continuity of~$w_\alpha$ and~$W_\alpha$.
            \begin{theorem} \zlabel{thm:cont_w_alpha}
                For all~$\alpha > 0$,~$w_\alpha$ and~$W_\alpha$ defined by~\zcref{eq:reg_lin_gap_MO,eq:reg_lin_gap_MO_sol} are continuous on~$C$.
                Moreover, if every~$\nabla f_i, i = 1, \dots, m$ is locally Lipschitz continuous,~$w_\alpha$ and~$W_\alpha$ are locally Lipschitz continuous and locally H\"older continuous with exponent~$1 / 2$ on~$C$, respectively.
            \end{theorem}
            \begin{proof}
                Let~$\Omega$ be a bounded subset of~$C$ and let~$x^1, x^2 \in \Omega$.
                Adding the two inequalities gotten by substituting~$(x, z) = (x^1, W_\alpha(x^2))$ and~$(x, z) = (x^2, W_\alpha(x^1))$ into~\zcref{eq:reg_lin_gap_optimality}, we obtain
                \begin{align}
                    \MoveEqLeft \alpha \innerp*{W_\alpha(x^1) - W_\alpha(x^2) - (x^1 - x^2)}{W_\alpha(x^1) - W_\alpha(x^2)} \\
                    \le{}& \sum_{i = 1}^{m} \pi_i(x^1)\left[ \innerp{\nabla f_i(x^1)}{x^1 - W_\alpha(x^1)} + g_i(x^1) - g_i(W_\alpha(x^1)) \right] \\
                         &+ \sum_{i = 1}^{m} \pi_i(x^2) \left[ \innerp{\nabla f_i(x^2)}{x^2 - W_\alpha(x^2)} + g_i(x^2) - g_i(W_\alpha(x^2)) \right] \\
                         &+ \sum_{i = 1}^{m} \pi_i(x^1) \left[ \innerp{\nabla f_i(x^1)}{W_\alpha(x^2) - x^1} + g_i(W_\alpha(x^2)) - g_i(x^1) \right] \\
                         &+ \sum_{i = 1}^{m} \pi_i(x^2) \left[ \innerp{\nabla f_i(x^2)}{W_\alpha(x^1) - x^2} + g_i(W_\alpha(x^1)) - g_i(x^2) \right]
                .\end{align}
                Since~$\pi_j(x) = 0$ for~$j \in \mathcal{J}(x)$, we have
                \begin{align}
                    \MoveEqLeft \alpha \innerp*{W_\alpha(x^1) - W_\alpha(x^2) - (x^1 - x^2)}{W_\alpha(x^1) - W_\alpha(x^2)} \\
                    \le{}& \min_{i = 1, \dots, m} \left[ \innerp{\nabla f_i(x^1)}{x^1 - W_\alpha(x^1)} + g_i(x^1) - g_i(W_\alpha(x^1)) \right] \\
                         &+ \min_{i = 1, \dots, m} \left[ \innerp{\nabla f_i(x^2)}{x^2 - W_\alpha(x^2)} + g_i(x^2) - g_i(W_\alpha(x^2)) \right] \\
                         &+ \sum_{i = 1}^{m} \pi_i(x^1) \left[ \innerp{\nabla f_i(x^1)}{W_\alpha(x^2) - x^1} + g_i(W_\alpha(x^2)) - g_i(x^1) \right] \\
                         &+ \sum_{i = 1}^{m} \pi_i(x^2) \left[ \innerp{\nabla f_i(x^2)}{W_\alpha(x^1) - x^2} + g_i(W_\alpha(x^1)) - g_i(x^2) \right] \\
                    \le{}& \sum_{i = 1}^{m} \pi_i(x^2) \left[ \innerp{\nabla f_i(x^1)}{x^1 - W_\alpha(x^1)} + g_i(x^1) - g_i(W_\alpha(x^1)) \right] \\
                         &+ \sum_{i = 1}^{m} \pi_i(x^1) \left[ \innerp{\nabla f_i(x^2)}{x^2 - W_\alpha(x^2)} + g_i(x^2) - g_i(W_\alpha(x^2)) \right] \\
                         &+ \sum_{i = 1}^{m} \pi_i(x^1) \left[ \innerp{\nabla f_i(x^1)}{W_\alpha(x^2) - x^1} + g_i(W_\alpha(x^2)) - g_i(x^1) \right] \\
                         &+ \sum_{i = 1}^{m} \pi_i(x^2) \left[ \innerp{\nabla f_i(x^2)}{W_\alpha(x^1) - x^2} + g_i(W_\alpha(x^1)) - g_i(x^2) \right]
                .\end{align}
                Therefore, simple calculations give
                \begin{multline} \label{eq:cont_w_alpha:cont}
                    \alpha \norm{W_\alpha(x^1) - W_\alpha(x^2)}_2^2 \le \alpha \innerp{W_\alpha(x^1) - W_\alpha(x^2)}{x^1 - x^2} \\
                    + \sum_{i = 1}^{m} \left[ \pi(x^2) - \pi(x^1) \right] \left[ g_i(x^1) - g_i(x^2) + \innerp*{\nabla f_i(x^1)}{x^1 - x^2} \right. \\
                \left. - \innerp*{\nabla f_i(x^1) - \nabla f_i(x^2)}{x^2} \right] \\
                        + \sum_{i = 1}^{m} \pi_i(x^1) \innerp{\nabla f_i(x^1) - \nabla f_i(x^2)}{W_\alpha(x^2)}
                        + \sum_{i = 1}^{m} \pi_i(x^2) \innerp{\nabla f_i(x^2) - \nabla f_i(x^1)}{W_\alpha(x^1)}
                .\end{multline}
               When~$x^1 \to x^2$, the right hand side tends to zero, which means the continuity of~$W_\alpha$ on~$C$.
               Therefore, we can also say that~$w_\alpha$ is continuous on~$C$ immediately from the definition.

               Now, assume that each~$\nabla f_i, i = 1, \dots, m$ is locally Lipschitz continuous.
               Since~$g_i$ is also locally Lipschitz continuous from~\zcref{thm:local_Lipschitz}, we can prove the local H\"older continuity of~$W_\alpha$ from~\zcref{eq:cont_w_alpha:cont}.
               On the other hand, the definitions~\zcref{eq:reg_lin_gap_MO,eq:reg_lin_gap_MO_sol} of~$w_\alpha$ and~$W_\alpha$ give
               \begin{align} 
                    &w_\alpha(x^1) - w_\alpha(x^2) \\
                   ={}& \min_{i = 1, \dots, m} \left[ \innerp{\nabla f_i(x^1)}{x^1 - W_\alpha(x^1)} + g_i(x^1) - g_i(W_\alpha(x^1)) \right] - \frac{\alpha}{2} \norm*{x^1 - W_\alpha(x^1)}_2^2 \\
                      &- \max_{y \in C} \min_{i = 1, \dots, m} \left[ \innerp*{\nabla f_i(x^2)}{x^2 - y} + g_i(x^2) - g_i(y) - \frac{\alpha}{2} \norm*{x^2 - y}_2^2 \right] \\
                   \le{}& \min_{i = 1, \dots, m} \left[ \innerp{\nabla f_i(x^1)}{x^1 - W_\alpha(x^1)} + g_i(x^1) - g_i(W_\alpha(x^1)) \right] - \frac{\alpha}{2} \norm*{x^1 - W_\alpha(x^1)}_2^2 \\
                        &- \min_{i = 1, \dots, m} \left[ \innerp*{\nabla f_i(x^2)}{x^2 - W_\alpha(x^1)} + g_i(x^2) - g_i(W_\alpha(x^1)) \right] + \frac{\alpha}{2} \norm*{x^2 - W_\alpha(x^1)}_2^2 \\
                   \le{}& \max_{i = 1, \dots, m} \left[ \innerp*{\nabla f_i(x^1) - \nabla f_i(x^2)}{x^1 - W_\alpha(x^1)} + \innerp*{\nabla f_i(x^2)}{x^1 - x^2} + g_i(x^1) - g_i(x^2) \right] \\
                        &- \frac{\alpha}{2} \innerp*{x^1 - x^2}{x^1 + x^2 - 2 W_\alpha(x^1)} \\
                   \le{}& \norm*{x^1 - W_\alpha(x^1)}_2 \max_{i = 1, \dots, m} \norm{\nabla f_i(x^1) - \nabla f_i(x^2)}_2 \\
                        &+ \max_{i = 1, \dots, m} \norm*{\nabla f_i(x^2)}_2 \norm{x^1 - x^2}_2 + \max_{i = 1, \dots, m} \abs{g_i(x^1) - g_i(x^2)} \\
                        &+ \frac{\alpha}{2} \norm{x^1 + x^2 - 2 W_\alpha(x^1)}_2 \norm{x^1 - x^2}_2
               ,\end{align} 
               where the first inequality comes from the relation~$\min_{i = 1, \dots, m} v^1_i - \min_{i = 1, \dots, m} v^2_i \le \max_{i = 1, \dots, m} (v^1_i - v^2_i)$ for any~$v^1, v^2 \in \setR^m$, and the third inequality follows from the Cauchy-Schwarz inequality.
               The above inequality holds even if we interchange~$x^1$ and~$x^2$.
               Furthermore,~$W_\alpha(x)$ and~$\nabla f_i(x)$ are bounded for any~$x \in \Omega$ due to their continuity.
               Therefore, local Lipschitz continuity of~$\nabla f_i$ and~$g_i$ implies the local Lipschitz continuity of~$w_\alpha$.
            \end{proof}

            On the other hand, in the same way as the derivation of~\zcref{eq:u dual}, Sion's minimax theorem~\cite{Sion1958} gives another representation of~$w_\alpha$ for~$\alpha > 0$ as follows:
            \begin{equation} \label{eq:reg_lin_gap_MO_Sion}
                w_\alpha(x) = \min_{\pi \in \Delta^m} \max_{y \in C} \sum_{i = 1}^{m} \pi_i \left[ \innerp{\nabla f_i(x)}{x - y} + g_i(x) - g_i(y) - \frac{\alpha}{2} \norm{x - y}_2^2 \right] 
            ,\end{equation} 
            where~$\Delta^m$ denotes the unit $m$-simplex~\zcref{eq:simplex}.
            Moreover, simple calculations show that
            \begin{equation}
                \begin{alignedat}{2}
                    w_\alpha(x) &= \min_{\pi \in \Delta^m} &&\left\{ \sum_{i = 1}^{m} \pi_i g_i(x) + \frac{1}{2 \alpha} \norm*{\sum_{i = 1}^{m} \pi_i \nabla f_i(x)}_2^2 \right. \\
                                &&& \left. {}- \min_{y \in C} \left[ \sum_{i = 1}^{m} \pi_i g_i(y) + \frac{\alpha}{2} \norm*{x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) - y}_2^2 \right] \right\} \\
                                &= \min_{\pi \in \Delta^m} &&\left[ \sum_{i = 1}^{m} \pi_i g_i(x) + \frac{1}{2 \alpha} \norm*{\sum_{i = 1}^{m} \pi_i \nabla f_i(x)}_2^2 \right. \\
                                &&& \left. {}- \alpha \envelope_{\frac{1}{\alpha} \sum\limits_{i = 1}^{m} \pi_i g_i + \indicator_C} \left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right) \right]
                            ,\end{alignedat}
                        \end{equation} 
                        where~$\envelope$ and~$\indicator_C$ are given by~\zcref{eq:Moreau_env,eq:indicator}, respectively.
                        In other words, we can compute~$w_\alpha$ via the following~$m$-dimensional, simplex-constrained, and convex optimization problem:
                        \begin{equation} \label{eq:dual_reg_lin_gap}
                            \begin{aligned}
                                \min_{\pi \in \setR^m} &&& \sum_{i = 1}^{m} \pi_i g_i(x) + \frac{1}{2 \alpha} \norm*{\sum_{i = 1}^{m} \pi_i \nabla f_i(x)}_2^2 \\
                                                       &&&\quad - \alpha \envelope_{\frac{1}{\alpha} \sum\limits_{i = 1}^{m} \pi_i g_i + \indicator_C}\left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right)  \\
                                \st &&& \pi \ge 0 \eqand \sum_{i = 1}^{m} \pi_i = 1 
                            .\end{aligned}
                        \end{equation} 
                        Moreover, the following theorem proves that~\zcref{eq:dual_reg_lin_gap} is differentiable.
                        \begin{theorem} \zlabel{thm:differentiability of the objective function of w_alpha}
                            The objective function of~\zcref{eq:dual_reg_lin_gap} is continuously differentiable at every~$\pi \in \setR^m$ and
                            \begin{multline}
                                \nabla_\pi \left[ \sum_{i = 1}^{m} \pi_i g_i(x) + \frac{1}{2 \alpha} \norm*{\sum_{i = 1}^{m} \pi_i \nabla f_i(x)}_2^2 - \alpha \envelope_{\frac{1}{\alpha} \sum\limits_{i = 1}^{m} g_i + \indicator_C}\left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right)  \right] \\
                                = g(x) - g\left( \prox_{\frac{1}{\alpha} \sum\limits_{i = 1}^{m} \pi_i g_i + \indicator_C}\left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right) \right) \\
                                - \jdv{f}{x} \left( \prox_{\frac{1}{\alpha} \sum\limits_{i = 1}^{m} \pi_i g_i + \indicator_C}\left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right) - x \right) 
                            ,\end{multline}
                            where~$\prox$ is the proximal operator~\zcref{eq:prox}, and~$\jdv{f}{x}$ is the Jacobian matrix at~$x$ given by~\zcref{eq:jac}.
                        \end{theorem}
                        \begin{proof}
                            Let
                            \begin{equation}
                                \theta(y, \pi) \coloneqq \sum_{i = 1}^{m} \pi_i g_i(y) + \frac{\alpha}{2} \norm*{x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) - y}_2^2
                            .\end{equation} 
                            Then,~$\theta$ is continuous,~$\theta_y(\cdot) \coloneqq \theta(y, \cdot)$ is continuously differentiable, and
                            \begin{equation}
                                \nabla_\pi \theta_y(\pi) = g(y) + \jdv{f}{x} \left( y - x + \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right) 
                            .\end{equation} 
                            Moreover,~$\prox_{\frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i g_i + \indicator_C}(x) = \argmin_{y \in C} \theta(y, \pi)$ is also continuous at every~$\pi \in \setR^m$ (cf.~\cite[Excercise 7.38]{Rockafellar1998}).
                            The above discussion implies that every assumption in \zcref{thm:sensitivity} is satisfied.
                            Combined with the uniqueness of~$\prox_{\frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i g_i + \indicator_C}(x)$, we get
                            \begin{multline}
                                \nabla_\pi \left[ \alpha \envelope_{\frac{1}{\alpha} \sum\limits_{i = 1}^{m} \pi_i g_i + \indicator_C}\left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right) \right] \\
                                = g\left( \prox_{\frac{1}{\alpha} \sum\limits_{i = 1}^{m} \pi_i g_i + \indicator_C}\left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right) \right) \\
                                + \jdv{f}{x} \left( \prox_{\frac{1}{\alpha} \sum\limits_{i = 1}^{m} \pi_i g_i + \indicator_C}\left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right) - x + \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right) 
                            .\end{multline}
                            On the other hand, we have
                            \begin{equation}
                                \nabla_\pi \left[ \sum_{i = 1}^{m} \pi_i g_i(x) + \frac{1}{2 \alpha} \norm*{\sum_{i = 1}^{m} \pi_i \nabla f_i(x)}_2^2 \right] = g(x) + \frac{1}{\alpha} \jdv{f}{x} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) 
                            .\end{equation} 
                            Adding the above two equalities, we obtain the desired result.
                        \end{proof}
                        Thus, like~\zcref{eq:dual_reg_gap}, \zcref{eq:dual_reg_lin_gap} is solvable with convex optimization techniques such as the interior point method~\cite{Bertsekas1999} when we can quickly evaluate~$\prox_{\frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i g_i + \indicator_C}(\cdot)$.
                        When~$n \gg m$, this usually gives a faster way to compute~$w_\alpha$.
                        \begin{example}
                            \begin{enumerate}
                                \item If~$g_i(x) = 0$ for all~$i = 1, \dots, m$, then~$\prox_{\frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i g_i + \indicator_C}$ reduces to the projection onto~$C$ from~\zcref{eq:prox_ind}.
                                \item If~$g_i(x) = g_1(x)$ for any~$i = 1, \dots, m$, or if~$g_i(x) = g_1(x_{I_i})$ and the index sets~$I_i \subseteq \set*{1, \dots, n}$ do not overlap each other, then~$\prox_{\frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i g_i}$ is computable with each~$\prox_{g_i}$ when~$C = \setR^n$.
                                \item Even if there is an overlap, we can compute~$\prox_{\frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i g_i}$ for special functions.
                                    For example, when~$m = 2, g_1(x) = \norm{x}_1, g_2(x) = \norm{x}_2^2$, and~$C = \setR^n$,~$\pi_1 g_1(x) + \pi_2 g_2(x)$ is the elastic net~\cite{Zou2005}. 
                                    It has a proximal operator in closed-form~\cite[Section 6.5.3]{Parikh2014}.
                            \end{enumerate}
                        \end{example}

                        Now, define the optimal solution set of~\zcref{eq:dual_reg_lin_gap} by
                        \begin{multline} \label{eq:Pi}
                            \Pi(x) = \argmin_{\pi \in \Delta^m} \left[ \sum_{i = 1}^{m} \pi_i g_i(x) + \frac{1}{2 \alpha} \norm*{\sum_{i = 1}^{m} \pi_i \nabla f_i(x)}_2^2 \right. \\
                            \left. {}- \alpha \envelope_{\frac{1}{\alpha} \sum\limits_{i = 1}^{m} \pi_i g_i + \indicator_C}\left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right) \right] 
                        .\end{multline} 
                        Then, in the same manner as \zcref{thm:smooth_reg_gap}, we obtain the following theorem.
                        \begin{theorem} \zlabel{thm:smooth_reg_lin_gap}
                            Let~$x \in C$.
                            Assume that~$f_i$ is twice continuously differentiable at~$x$.
                            Then, for all~$\alpha > 0$, the merit function~$w_\alpha$ defined by~\zcref{eq:reg_lin_gap_MO} has a directional derivative
                            \begin{multline}
                                w_\alpha'(x; z - x) = \inf_{\pi \in \Pi(x)} \left[ \sum_{i = 1}^{m} \pi_i g_i'(x; z - x) \right. \\
                                {}- \alpha \left\langle \left[ I_n - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla^2 f_i(x) \right] \left[x - \prox_{\frac{1}{\alpha} \sum\limits_{i = 1}^{m} \pi_i g_i + \indicator_C} \left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right) \right] \right.\\
                                \left. \left.- \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) , z - x \right\rangle \right] 
                            \end{multline}
                            for all~$z \in C$, where~$\prox$ and~$\Pi$ is given by~\zcref{eq:prox,eq:Pi}, respectively, and~$I_n \in \setR^{n \times n}$ is the~$n$-dimensional identity matrix.
                            In particular, if~$\Pi(x)$ is a singleton, i.e.,~$\Pi(x) = \set*{\pi(x)}$, and~$g_i$ is continuously differentiable at~$x$, then~$w_\alpha$ is continuously differentiable at~$x$, and we have
                            \begin{multline}
                                \nabla w_\alpha(x) = \sum_{i = 1}^{m} \pi_i(x) \nabla F_i(x) \\
                                {}- \alpha \left[ I_n - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i(x) \nabla^2 f_i(x) \right] \left[x - \prox_{\frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i(x) g_i + \indicator_C} \left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i(x) \nabla f_i(x) \right) \right]
                            .\end{multline}
                        \end{theorem}
                        If the convex part~$g_i$ is the same regardless of~$i$, we get the following corollary without assuming the differentiability of~$g_i$.
                        \begin{corollary} \zlabel{thm:composite_reg_lin_gap}
                            Let~$x \in C$ and~$\alpha > 0$.
                            Assume that~$f_i$ is twice continuously differentiable at~$x$ and~$g_i = g_1$ for all~$i = 1, \dots, m$, and recall that~$w_\alpha$ and~$\prox$ be defined by~\zcref{eq:reg_lin_gap_MO,eq:prox}, respectively.
                            If~$\Pi(x)$ given by~\zcref{eq:Pi} is a singleton, i.e.,~$\Pi(x) = \set*{\pi(x)}$, then the function~$w_\alpha - g_1$ is continuously differentiable at~$x$, and we have
                            \begin{multline}
                                \nabla_x \left( w_\alpha(x) - g_1(x) \right) \\
                                = - \alpha \left[ I_n - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i(x) \nabla^2 f_i(x) \right] \left[x - \prox_{\frac{1}{\alpha} g_1 + \indicator_C} \left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i(x) \nabla f_i(x) \right) \right] \\
                                + \sum_{i = 1}^{m} \pi_i(x) \nabla f_i(x)
                            .\end{multline}
                        \end{corollary}
                        \zcref{thm:composite_reg_lin_gap} implies that, under certain conditions, the merit function~$w_\alpha = (w_\alpha - g_1) + g_1$ is composite, i.e., the sum of a continuously differentiable function and a convex one.

                        \zcref{thm:reg_lin_gap,thm:smooth_reg_lin_gap} show that the Pareto stationary points for~\zcref{eq:composite_MO} are global optimal for the following directionally differentiable single-objective optimization problem:
                        \begin{equation} \label{eq:min_reg_lin_gap}
                            \min_{x \in C} \quad w_\alpha(x)
                        .\end{equation} 
                        Moreover, when the assumptions of \zcref{thm:composite_reg_lin_gap} hold, we can apply first-order methods such as the proximal gradient method~\cite{Fukushima1981} to~\zcref{eq:min_reg_lin_gap}.
                        On the other hand, if we consider~\zcref{ex:min_reg_gap} with~$f_i = 0$, we can see that the stationary point for~\zcref{eq:min_reg_lin_gap} is not necessarily Pareto stationary for~\zcref{eq:composite_MO}.
                        However, if~$f_i$ is convex and twice continuously differentiable, and~$F_i$ is strictly convex, then we can prove that every stationary point of~\zcref{eq:min_reg_lin_gap} is Pareto optimal for~\zcref{eq:composite_MO}, i.e., global optimal for~\zcref{eq:min_reg_gap}.
                        Note that this assumption does not assert the convexity of~$w_\alpha$.
                        \begin{theorem}
                            Let~$x \in C$ and~$\alpha > 0$.
                            Suppose that~$f_i$ is convex and twice continuously differentiable at~$x$, and~$F_i$ is strictly convex for any~$i = 1, \dots, m$.
                            If~$x$ is stationary for~\zcref{eq:min_reg_lin_gap}, i.e.,
                            \begin{equation}
                                w_\alpha'(x; z - x) \ge 0 \forallcondition{z \in C}
                            ,\end{equation} 
                            then~$x$ is Pareto optimal for~\zcref{eq:composite_MO}.
                        \end{theorem}
                        \begin{proof}
                            Let~$z \in C$ and $\pi \in \Pi(x)$, where~$\Pi(x)$ is defined by~\zcref{eq:Pi}.
                            Then, it follows from~\zcref{thm:smooth_reg_lin_gap} that
                            \begin{multline}
                                \sum_{i = 1}^{m} \pi_i g_i'(x; z - x) \\
                                - \alpha \left\langle \left[ I_n - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla^2 f_i(x) \right] \left[x - \prox_{\frac{1}{\alpha} \sum\limits_{i = 1}^{m} \pi_i g_i + \indicator_C} \left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right) \right] \right.\\
                                \left. - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) , z - x \right\rangle \ge 0
                            .\end{multline}
                            Substituting~$z = \prox_{\frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i F_i + \indicator_C}(x)$, we have
                            \begin{multline}
                                \sum_{i = 1}^{m} \pi_i F_i' \left(x; \prox_{\frac{1}{\alpha} \sum\limits_{i = 1}^{m} \pi_i g_i + \indicator_C}\left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right) - x \right) \\
                                + \alpha \left\langle \left[ I_n - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla^2 f_i(x) \right] \left[ x - \prox_{\frac{1}{\alpha} \sum\limits_{i = 1}^{m} \pi_i g_i + \indicator_C}\left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right) \right]  \right. , \\
                                \left. x - \prox_{\frac{1}{\alpha} \sum\limits_{i = 1}^{m} \pi_i g_i + \indicator_C}\left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right) \right\rangle \ge 0
                            .\end{multline}
                            Since the convexity of~$f_i$ implies that~$\nabla^2 f_i(x)$ is positive semidefinite, we get
                            \begin{multline}
                                \sum_{i = 1}^{m} \pi_i F_i' \left(x; \prox_{\frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i g_i + \indicator_C}\left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right) - x \right) \\
                                + \alpha \norm*{\prox_{\frac{1}{\alpha} \sum\limits_{i = 1}^{m} \pi_i g_i + \indicator_C}\left( x - \frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right)}_2^2 \ge 0
                            .\end{multline}
                            Therefore, with similar arguments used in the proof of~\zcref{thm:stationary_reg_gap}, we obtain~$x = \prox_{\frac{1}{\alpha} \sum_{i = 1}^{m} \pi_i g_i + \indicator_C}\left( x - (1 / \alpha) \sum_{i = 1}^{m} \pi_i \nabla f_i(x) \right)$, and thus~$w_\alpha(x) = 0$.
                            Since~$F_i$ is strictly convex,~$x$ is Pareto optimal for~\zcref{eq:composite_MO} from \zcref{thm:Pareto:strict_convex,thm:reg_lin_gap}.
                        \end{proof}
                        \end{document}
